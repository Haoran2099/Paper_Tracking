{
  "date": "2026-02-04",
  "papers": [
    {
      "arxiv_id": "2602.03825v1",
      "title": "Robust Intervention Learning from Emergency Stop Interventions",
      "abstract": "Human interventions are a common source of data in autonomous systems during testing. These interventions provide an important signal about where the current policy needs improvement, but are often noisy and incomplete. We define Robust Intervention Learning (RIL) as the problem of learning from intervention data while remaining robust to the quality and informativeness of the intervention signal. In the best case, interventions are precise and avoiding them is sufficient to solve the task, but in many realistic settings avoiding interventions is necessary but not sufficient for achieving good performance. We study robust intervention learning in the context of emergency stop interventions and propose Residual Intervention Fine-Tuning (RIFT), a residual fine-tuning algorithm that treats intervention feedback as an incomplete learning signal and explicitly combines it with a prior policy. By framing intervention learning as a fine-tuning problem, our approach leverages structure encoded in the prior policy to resolve ambiguity when intervention signals under-specify the task. We provide theoretical analysis characterizing conditions under which this formulation yields principled policy improvement, and identify regimes where intervention learning is expected to fail. Our experiments reveal that residual fine-tuning enables robust and consistent policy improvement across a range of intervention strategies and prior policy qualities, and highlight robust intervention learning as a promising direction for future work.",
      "authors": [
        "Ethan Pronovost",
        "Khimya Khetarpal",
        "Siddhartha Srinivasa"
      ],
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "published": "2026-02-03T18:33:21Z",
      "updated": "2026-02-03T18:33:21Z",
      "pdf_url": "https://arxiv.org/pdf/2602.03825v1",
      "abs_url": "http://arxiv.org/abs/2602.03825v1",
      "summary": "提出Residual Intervention Fine-Tuning算法，从紧急停止干预中进行鲁棒学习，提升自动驾驶系统性能。",
      "key_contributions": [
        "提出Robust Intervention Learning (RIL)问题定义",
        "提出Residual Intervention Fine-Tuning (RIFT)算法",
        "提供理论分析，表征算法的改进条件"
      ],
      "methodology": "将干预学习视为微调问题，利用先验策略的信息，通过残差微调的方式结合干预反馈信号。",
      "tags": [
        "强化学习",
        "干预学习",
        "自动驾驶",
        "微调"
      ],
      "assigned_category": "agent",
      "relevance_score": 8,
      "relevance_reason": "研究如何利用干预信息提升自主系统性能，与Agent学习策略优化密切相关。",
      "analyzed_at": "2026-02-04T20:41:59.231306",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.03822v1",
      "title": "They Said Memes Were Harmless-We Found the Ones That Hurt: Decoding Jokes, Symbols, and Cultural References",
      "abstract": "Meme-based social abuse detection is challenging because harmful intent often relies on implicit cultural symbolism and subtle cross-modal incongruence. Prior approaches, from fusion-based methods to in-context learning with Large Vision-Language Models (LVLMs), have made progress but remain limited by three factors: i) cultural blindness (missing symbolic context), ii) boundary ambiguity (satire vs. abuse confusion), and iii) lack of interpretability (opaque model reasoning). We introduce CROSS-ALIGN+, a three-stage framework that systematically addresses these limitations: (1) Stage I mitigates cultural blindness by enriching multimodal representations with structured knowledge from ConceptNet, Wikidata, and Hatebase; (2) Stage II reduces boundary ambiguity through parameter-efficient LoRA adapters that sharpen decision boundaries; and (3) Stage III enhances interpretability by generating cascaded explanations. Extensive experiments on five benchmarks and eight LVLMs demonstrate that CROSS-ALIGN+ consistently outperforms state-of-the-art methods, achieving up to 17% relative F1 improvement while providing interpretable justifications for each decision.",
      "authors": [
        "Sahil Tripathi",
        "Gautam Siddharth Kashyap",
        "Mehwish Nasim",
        "Jian Yang",
        "Jiechao Gao",
        "Usman Naseem"
      ],
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "published": "2026-02-03T18:29:46Z",
      "updated": "2026-02-03T18:29:46Z",
      "pdf_url": "https://arxiv.org/pdf/2602.03822v1",
      "abs_url": "http://arxiv.org/abs/2602.03822v1",
      "summary": "提出了CROSS-ALIGN+框架，提升基于meme的社交恶意信息检测效果，并增强模型可解释性。",
      "key_contributions": [
        "缓解文化盲区",
        "减少边界模糊",
        "增强可解释性"
      ],
      "methodology": "三阶段框架：通过知识库增强表示、LoRA适配器锐化边界、生成级联解释。",
      "tags": [
        "Meme",
        "Social Abuse Detection",
        "Multimodal Learning",
        "Explainability"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 8,
      "relevance_reason": "涉及常识推理和文化背景理解，与LLM推理能力密切相关。",
      "analyzed_at": "2026-02-04T20:42:02.063867",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.03817v1",
      "title": "Adaptive Evidence Weighting for Audio-Spatiotemporal Fusion",
      "abstract": "Many machine learning systems have access to multiple sources of evidence for the same prediction target, yet these sources often differ in reliability and informativeness across inputs. In bioacoustic classification, species identity may be inferred both from the acoustic signal and from spatiotemporal context such as location and season; while Bayesian inference motivates multiplicative evidence combination, in practice we typically only have access to discriminative predictors rather than calibrated generative models. We introduce \\textbf{F}usion under \\textbf{IN}dependent \\textbf{C}onditional \\textbf{H}ypotheses (\\textbf{FINCH}), an adaptive log-linear evidence fusion framework that integrates a pre-trained audio classifier with a structured spatiotemporal predictor. FINCH learns a per-sample gating function that estimates the reliability of contextual information from uncertainty and informativeness statistics. The resulting fusion family \\emph{contains} the audio-only classifier as a special case and explicitly bounds the influence of contextual evidence, yielding a risk-contained hypothesis class with an interpretable audio-only fallback. Across benchmarks, FINCH consistently outperforms fixed-weight fusion and audio-only baselines, improving robustness and error trade-offs even when contextual information is weak in isolation. We achieve state-of-the-art performance on CBI and competitive or improved performance on several subsets of BirdSet using a lightweight, interpretable, evidence-based approach. Code is available: \\texttt{\\href{https://anonymous.4open.science/r/birdnoise-85CD/README.md}{anonymous-repository}}",
      "authors": [
        "Oscar Ovanger",
        "Levi Harris",
        "Timothy H. Keitt"
      ],
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "published": "2026-02-03T18:21:13Z",
      "updated": "2026-02-03T18:21:13Z",
      "pdf_url": "https://arxiv.org/pdf/2602.03817v1",
      "abs_url": "http://arxiv.org/abs/2602.03817v1",
      "summary": "论文提出FINCH框架，自适应融合音频和时空信息，提升生物声学分类性能。",
      "key_contributions": [
        "提出了FINCH框架，用于自适应融合音频和时空证据。",
        "引入per-sample gating函数，评估上下文信息的可靠性。",
        "实现了在上下文信息较弱时，性能优于固定权重融合和仅使用音频的基线。"
      ],
      "methodology": "FINCH通过学习per-sample gating函数，基于不确定性和信息量统计，自适应地调整音频和时空证据的权重，最终进行融合。",
      "tags": [
        "音频分类",
        "时空融合",
        "自适应权重",
        "生物声学"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 6,
      "relevance_reason": "涉及多模态信息融合和决策，与推理有一定关联。",
      "analyzed_at": "2026-02-04T20:42:04.630352",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.03815v1",
      "title": "Fast-Slow Efficient Training for Multimodal Large Language Models via Visual Token Pruning",
      "abstract": "Multimodal Large Language Models (MLLMs) suffer from severe training inefficiency issue, which is associated with their massive model sizes and visual token numbers. Existing efforts in efficient training focus on reducing model sizes or trainable parameters. Inspired by the success of Visual Token Pruning (VTP) in improving inference efficiency, we are exploring another substantial research direction for efficient training by reducing visual tokens. However, applying VTP at the training stage results in a training-inference mismatch: pruning-trained models perform poorly when inferring on non-pruned full visual token sequences. To close this gap, we propose DualSpeed, a fast-slow framework for efficient training of MLLMs. The fast-mode is the primary mode, which incorporates existing VTP methods as plugins to reduce visual tokens, along with a mode isolator to isolate the model's behaviors. The slow-mode is the auxiliary mode, where the model is trained on full visual sequences to retain training-inference consistency. To boost its training, it further leverages self-distillation to learn from the sufficiently trained fast-mode. Together, DualSpeed can achieve both training efficiency and non-degraded performance. Experiments show DualSpeed accelerates the training of LLaVA-1.5 by 2.1$\\times$ and LLaVA-NeXT by 4.0$\\times$, retaining over 99% performance. Code: https://github.com/dingkun-zhang/DualSpeed",
      "authors": [
        "Dingkun Zhang",
        "Shuhan Qi",
        "Yulin Wu",
        "Xinyu Xiao",
        "Xuan Wang",
        "Long Chen"
      ],
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "published": "2026-02-03T18:18:11Z",
      "updated": "2026-02-03T18:18:11Z",
      "pdf_url": "https://arxiv.org/pdf/2602.03815v1",
      "abs_url": "http://arxiv.org/abs/2602.03815v1",
      "summary": "提出DualSpeed框架，通过视觉Token剪枝加速多模态大语言模型的训练，并保持推理性能。",
      "key_contributions": [
        "提出DualSpeed快速-慢速训练框架",
        "结合视觉Token剪枝（VTP）加速训练",
        "使用自蒸馏保证训练-推理一致性"
      ],
      "methodology": "DualSpeed采用快速模式（VTP+模式隔离）和慢速模式（完整视觉序列+自蒸馏）交替训练，兼顾效率和性能。",
      "tags": [
        "多模态学习",
        "大语言模型",
        "视觉Token剪枝",
        "高效训练",
        "自蒸馏"
      ],
      "assigned_category": "agent",
      "relevance_score": 6,
      "relevance_reason": "虽然主要关注训练效率，但可能影响Agent在多模态环境中的应用。",
      "analyzed_at": "2026-02-04T20:42:07.016229",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.03799v1",
      "title": "Conformal Reachability for Safe Control in Unknown Environments",
      "abstract": "Designing provably safe control is a core problem in trustworthy autonomy. However, most prior work in this regard assumes either that the system dynamics are known or deterministic, or that the state and action space are finite, significantly limiting application scope. We address this limitation by developing a probabilistic verification framework for unknown dynamical systems which combines conformal prediction with reachability analysis. In particular, we use conformal prediction to obtain valid uncertainty intervals for the unknown dynamics at each time step, with reachability then verifying whether safety is maintained within the conformal uncertainty bounds. Next, we develop an algorithmic approach for training control policies that optimize nominal reward while also maximizing the planning horizon with sound probabilistic safety guarantees. We evaluate the proposed approach in seven safe control settings spanning four domains -- cartpole, lane following, drone control, and safe navigation -- for both affine and nonlinear safety specifications. Our experiments show that the policies we learn achieve the strongest provable safety guarantees while still maintaining high average reward.",
      "authors": [
        "Xinhang Ma",
        "Junlin Wu",
        "Yiannis Kantaros",
        "Yevgeniy Vorobeychik"
      ],
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "published": "2026-02-03T18:01:38Z",
      "updated": "2026-02-03T18:01:38Z",
      "pdf_url": "https://arxiv.org/pdf/2602.03799v1",
      "abs_url": "http://arxiv.org/abs/2602.03799v1",
      "summary": "提出结合一致性预测和可达性分析的未知动力系统安全控制框架。",
      "key_contributions": [
        "提出基于一致性预测的安全控制框架",
        "开发优化名义奖励和最大化安全规划范围的控制策略训练算法",
        "在多个安全控制场景验证了算法的有效性"
      ],
      "methodology": "结合一致性预测获取不确定性区间，利用可达性分析验证安全，训练控制策略优化奖励和安全。",
      "tags": [
        "安全控制",
        "一致性预测",
        "可达性分析",
        "未知动力系统"
      ],
      "assigned_category": "agent",
      "relevance_score": 8,
      "relevance_reason": "论文研究了在不确定环境下设计安全控制策略的问题，属于智能体控制的重要方面。",
      "analyzed_at": "2026-02-04T20:42:11.673089",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.03794v1",
      "title": "Understanding Agent Scaling in LLM-Based Multi-Agent Systems via Diversity",
      "abstract": "LLM-based multi-agent systems (MAS) have emerged as a promising approach to tackle complex tasks that are difficult for individual LLMs. A natural strategy is to scale performance by increasing the number of agents; however, we find that such scaling exhibits strong diminishing returns in homogeneous settings, while introducing heterogeneity (e.g., different models, prompts, or tools) continues to yield substantial gains. This raises a fundamental question: what limits scaling, and why does diversity help? We present an information-theoretic framework showing that MAS performance is bounded by the intrinsic task uncertainty, not by agent count. We derive architecture-agnostic bounds demonstrating that improvements depend on how many effective channels the system accesses. Homogeneous agents saturate early because their outputs are strongly correlated, whereas heterogeneous agents contribute complementary evidence. We further introduce $K^*$, an effective channel count that quantifies the number of effective channels without ground-truth labels. Empirically, we show that heterogeneous configurations consistently outperform homogeneous scaling: 2 diverse agents can match or exceed the performance of 16 homogeneous agents. Our results provide principled guidelines for building efficient and robust MAS through diversity-aware design. Code and Dataset are available at the link: https://github.com/SafeRL-Lab/Agent-Scaling.",
      "authors": [
        "Yingxuan Yang",
        "Chengrui Qu",
        "Muning Wen",
        "Laixi Shi",
        "Ying Wen",
        "Weinan Zhang",
        "Adam Wierman",
        "Shangding Gu"
      ],
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "published": "2026-02-03T17:58:10Z",
      "updated": "2026-02-03T17:58:10Z",
      "pdf_url": "https://arxiv.org/pdf/2602.03794v1",
      "abs_url": "http://arxiv.org/abs/2602.03794v1",
      "summary": "论文研究了LLM多智能体系统中智能体数量与性能的关系，强调了多样性的重要性。",
      "key_contributions": [
        "提出了多智能体系统性能受限于任务不确定性的信息论框架",
        "推导了架构无关的性能边界，强调有效通道数量的重要性",
        "引入了$K^*$指标量化有效通道数量，无需ground-truth标签"
      ],
      "methodology": "通过信息论建模分析多智能体系统性能瓶颈，并进行实验验证异构智能体的优越性。",
      "tags": [
        "LLM",
        "Multi-Agent System",
        "Diversity",
        "Information Theory"
      ],
      "assigned_category": "agent",
      "relevance_score": 9,
      "relevance_reason": "直接研究了基于LLM的多智能体系统，并提出了scaling的优化方法。",
      "analyzed_at": "2026-02-04T20:42:16.091167",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.03784v1",
      "title": "Context Compression via Explicit Information Transmission",
      "abstract": "Long-context inference with Large Language Models (LLMs) is costly due to quadratic attention and growing key-value caches, motivating context compression. In this work, we study soft context compression, where a long context is condensed into a small set of continuous representations. Existing methods typically re-purpose the LLM itself as a trainable compressor, relying on layer-by-layer self-attention to iteratively aggregate information. We argue that this paradigm suffers from two structural limitations: (i) progressive representation overwriting across layers (ii) uncoordinated allocation of compression capacity across tokens. We propose ComprExIT (Context Compression via Explicit Information Transmission), a lightweight framework that formulates soft compression into a new paradigm: explicit information transmission over frozen LLM hidden states. This decouples compression from the model's internal self-attention dynamics. ComprExIT performs (i) depth-wise transmission to selectively transmit multi-layer information into token anchors, mitigating progressive overwriting, and (ii) width-wise transmission to aggregate anchors into a small number of slots via a globally optimized transmission plan, ensuring coordinated allocation of information. Across six question-answering benchmarks, ComprExIT consistently outperforms state-of-the-art context compression methods while introducing only ~1% additional parameters, demonstrating that explicit and coordinated information transmission enables more effective and robust long-context compression.",
      "authors": [
        "Jiangnan Ye",
        "Hanqi Yan",
        "Zhenyi Shen",
        "Heng Chang",
        "Ye Mao",
        "Yulan He"
      ],
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "published": "2026-02-03T17:44:12Z",
      "updated": "2026-02-03T17:44:12Z",
      "pdf_url": "https://arxiv.org/pdf/2602.03784v1",
      "abs_url": "http://arxiv.org/abs/2602.03784v1",
      "summary": "ComprExIT通过显式信息传输实现高效LLM上下文压缩，解决了传统自注意力压缩的局限性。",
      "key_contributions": [
        "提出ComprExIT框架，解耦压缩和LLM内部自注意力。",
        "引入深度和宽度方向的信息传输机制。",
        "实验证明ComprExIT优于现有上下文压缩方法，参数量更少。"
      ],
      "methodology": "ComprExIT利用冻结LLM隐层状态，通过深度信息传输选择性传递信息，再通过宽度信息传输优化信息分配到少量token槽中。",
      "tags": [
        "LLM",
        "上下文压缩",
        "信息传输",
        "长文本",
        "效率"
      ],
      "assigned_category": "memory",
      "relevance_score": 9,
      "relevance_reason": "论文直接解决LLM长上下文的内存效率问题，属于核心相关研究。",
      "analyzed_at": "2026-02-04T20:42:21.228668",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.03769v1",
      "title": "Reasoning with Latent Tokens in Diffusion Language Models",
      "abstract": "Discrete diffusion models have recently become competitive with autoregressive models for language modeling, even outperforming them on reasoning tasks requiring planning and global coherence, but they require more computation at inference time. We trace this trade-off to a key mechanism: diffusion models are trained to jointly predict a distribution over all unknown tokens, including those that will not actually be decoded in the current step. Ablating this joint prediction yields faster inference but degrades performance, revealing that accurate prediction at the decoded position relies on joint reasoning about the distribution of undecoded tokens. We interpret these as latent tokens and introduce a method for modulating their number, demonstrating empirically that this enables a smooth tradeoff between inference speed and sample quality. Furthermore, we demonstrate that latent tokens can be introduced into autoregressive models through an auxiliary multi-token prediction objective, yielding substantial improvements on the same reasoning tasks where they have traditionally struggled. Our results suggest that latent tokens, while arising naturally in diffusion, represent a general mechanism for improving performance on tasks requiring global coherence or lookahead.",
      "authors": [
        "Andre He",
        "Sean Welleck",
        "Daniel Fried"
      ],
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "published": "2026-02-03T17:27:46Z",
      "updated": "2026-02-03T17:27:46Z",
      "pdf_url": "https://arxiv.org/pdf/2602.03769v1",
      "abs_url": "http://arxiv.org/abs/2602.03769v1",
      "summary": "扩散语言模型通过联合预测未知token进行推理，本文探究了隐变量token的作用，并将其引入自回归模型。",
      "key_contributions": [
        "揭示了扩散模型中隐变量token对于推理能力的重要性",
        "提出了一种调节隐变量token数量的方法，平衡推理速度和样本质量",
        "将隐变量token引入自回归模型，提升其在推理任务上的表现"
      ],
      "methodology": "通过消融实验分析联合预测机制，提出调节隐变量token数量方法，并在扩散模型和自回归模型上进行实验验证。",
      "tags": [
        "扩散模型",
        "自回归模型",
        "推理",
        "隐变量"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 9,
      "relevance_reason": "论文核心关注扩散模型和自回归模型在推理任务上的能力，并提出了改进方法。",
      "analyzed_at": "2026-02-04T20:42:27.993041",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.03730v1",
      "title": "Efficient Variance-reduced Estimation from Generative EHR Models: The SCOPE and REACH Estimators",
      "abstract": "Generative models trained using self-supervision of tokenized electronic health record (EHR) timelines show promise for clinical outcome prediction. This is typically done using Monte Carlo simulation for future patient trajectories. However, existing approaches suffer from three key limitations: sparse estimate distributions that poorly differentiate patient risk levels, extreme computational costs, and high sampling variance. We propose two new estimators: the Sum of Conditional Outcome Probability Estimator (SCOPE) and Risk Estimation from Anticipated Conditional Hazards (REACH), that leverage next-token probability distributions discarded by standard Monte Carlo. We prove both estimators are unbiased and that REACH guarantees variance reduction over Monte Carlo sampling for any model and outcome. Empirically, on hospital mortality prediction in MIMIC-IV using the ETHOS-ARES framework, SCOPE and REACH match 100-sample Monte Carlo performance using only 10-11 samples (95% CI: [9,11]), representing a ~10x reduction in inference cost without degrading calibration. For ICU admission prediction, efficiency gains are more modest (~1.2x), which we attribute to the outcome's lower \"spontaneity,\" a property we characterize theoretically and empirically. These methods substantially improve the feasibility of deploying generative EHR models in resource-constrained clinical settings.",
      "authors": [
        "Luke Solo",
        "Matthew B. A. McDermott",
        "William F. Parker",
        "Bashar Ramadan",
        "Michael C. Burkhart",
        "Brett K. Beaulieu-Jones"
      ],
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "published": "2026-02-03T16:49:44Z",
      "updated": "2026-02-03T16:49:44Z",
      "pdf_url": "https://arxiv.org/pdf/2602.03730v1",
      "abs_url": "http://arxiv.org/abs/2602.03730v1",
      "summary": "提出了SCOPE和REACH两种新的EHR生成模型估计器，显著降低了计算成本和抽样方差。",
      "key_contributions": [
        "提出了SCOPE和REACH两种新的无偏估计器",
        "证明了REACH保证了方差缩减",
        "在MIMIC-IV数据集上验证了方法的有效性，显著降低了推理成本"
      ],
      "methodology": "利用生成模型中被丢弃的next-token概率分布，设计了SCOPE和REACH估计器，并进行了理论分析和实验验证。",
      "tags": [
        "EHR",
        "生成模型",
        "临床预测",
        "方差缩减",
        "自监督学习"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 7,
      "relevance_reason": "论文研究降低计算成本，侧重推理效率，与推理相关。",
      "analyzed_at": "2026-02-04T20:42:32.510094",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.03709v1",
      "title": "No Shortcuts to Culture: Indonesian Multi-hop Question Answering for Complex Cultural Understanding",
      "abstract": "Understanding culture requires reasoning across context, tradition, and implicit social knowledge, far beyond recalling isolated facts. Yet most culturally focused question answering (QA) benchmarks rely on single-hop questions, which may allow models to exploit shallow cues rather than demonstrate genuine cultural reasoning. In this work, we introduce ID-MoCQA, the first large-scale multi-hop QA dataset for assessing the cultural understanding of large language models (LLMs), grounded in Indonesian traditions and available in both English and Indonesian. We present a new framework that systematically transforms single-hop cultural questions into multi-hop reasoning chains spanning six clue types (e.g., commonsense, temporal, geographical). Our multi-stage validation pipeline, combining expert review and LLM-as-a-judge filtering, ensures high-quality question-answer pairs. Our evaluation across state-of-the-art models reveals substantial gaps in cultural reasoning, particularly in tasks requiring nuanced inference. ID-MoCQA provides a challenging and essential benchmark for advancing the cultural competency of LLMs.",
      "authors": [
        "Vynska Amalia Permadi",
        "Xingwei Tan",
        "Nafise Sadat Moosavi",
        "Nikos Aletras"
      ],
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "published": "2026-02-03T16:32:00Z",
      "updated": "2026-02-03T16:32:00Z",
      "pdf_url": "https://arxiv.org/pdf/2602.03709v1",
      "abs_url": "http://arxiv.org/abs/2602.03709v1",
      "summary": "提出了ID-MoCQA，一个用于评估LLM文化理解能力的大规模多跳印尼文化问答数据集。",
      "key_contributions": [
        "构建了大规模印尼文化多跳问答数据集ID-MoCQA",
        "提出了将单跳问题转换为多跳推理链的框架",
        "设计了多阶段验证流程确保数据集质量"
      ],
      "methodology": "通过专家评审和LLM筛选，系统性地将单跳文化问题转化为包含六种线索类型的多跳推理链。",
      "tags": [
        "多跳问答",
        "文化理解",
        "印尼文化",
        "LLM评估"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 9,
      "relevance_reason": "论文核心关注LLM在文化推理方面的能力，与reasoning类别高度相关。",
      "analyzed_at": "2026-02-04T20:42:36.250593",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.03708v1",
      "title": "Beyond Tokens: Semantic-Aware Speculative Decoding for Efficient Inference by Probing Internal States",
      "abstract": "Large Language Models (LLMs) achieve strong performance across many tasks but suffer from high inference latency due to autoregressive decoding. The issue is exacerbated in Large Reasoning Models (LRMs), which generate lengthy chains of thought. While speculative decoding accelerates inference by drafting and verifying multiple tokens in parallel, existing methods operate at the token level and ignore semantic equivalence (i.e., different token sequences expressing the same meaning), leading to inefficient rejections. We propose SemanticSpec, a semantic-aware speculative decoding framework that verifies entire semantic sequences instead of tokens. SemanticSpec introduces a semantic probability estimation mechanism that probes the model's internal hidden states to assess the likelihood of generating sequences with specific meanings.Experiments on four benchmarks show that SemanticSpec achieves up to 2.7x speedup on DeepSeekR1-32B and 2.1x on QwQ-32B, consistently outperforming token-level and sequence-level baselines in both efficiency and effectiveness.",
      "authors": [
        "Ximing Dong",
        "Shaowei Wang",
        "Dayi Lin",
        "Boyuan Chen",
        "Ahmed E. Hassan"
      ],
      "categories": [
        "cs.CL",
        "cs.PF"
      ],
      "primary_category": "cs.CL",
      "published": "2026-02-03T16:30:30Z",
      "updated": "2026-02-03T16:30:30Z",
      "pdf_url": "https://arxiv.org/pdf/2602.03708v1",
      "abs_url": "http://arxiv.org/abs/2602.03708v1",
      "summary": "SemanticSpec通过语义感知的推测解码，提升LLM推理效率，尤其在长链推理中表现突出。",
      "key_contributions": [
        "提出语义感知的推测解码框架SemanticSpec",
        "引入语义概率估计机制，利用内部隐状态评估语义序列的可能性",
        "实验证明在多个基准测试上优于传统方法"
      ],
      "methodology": "通过探查模型内部隐状态，评估生成具有特定语义序列的可能性，从而实现更高效的推测解码。",
      "tags": [
        "LLM",
        "推测解码",
        "语义理解",
        "推理效率",
        "内部状态"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 9,
      "relevance_reason": "论文核心解决LLM推理效率问题，与推理主题高度相关。",
      "analyzed_at": "2026-02-04T20:42:39.158296",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.03704v1",
      "title": "Cognitively Diverse Multiple-Choice Question Generation: A Hybrid Multi-Agent Framework with Large Language Models",
      "abstract": "Recent advances in large language models (LLMs) have made automated multiple-choice question (MCQ) generation increasingly feasible; however, reliably producing items that satisfy controlled cognitive demands remains a challenge. To address this gap, we introduce ReQUESTA, a hybrid, multi-agent framework for generating cognitively diverse MCQs that systematically target text-based, inferential, and main idea comprehension. ReQUESTA decomposes MCQ authoring into specialized subtasks and coordinates LLM-powered agents with rule-based components to support planning, controlled generation, iterative evaluation, and post-processing. We evaluated the framework in a large-scale reading comprehension study using academic expository texts, comparing ReQUESTA-generated MCQs with those produced by a single-pass GPT-5 zero-shot baseline. Psychometric analyses of learner responses assessed item difficulty and discrimination, while expert raters evaluated question quality across multiple dimensions, including topic relevance and distractor quality. Results showed that ReQUESTA-generated items were consistently more challenging, more discriminative, and more strongly aligned with overall reading comprehension performance. Expert evaluations further indicated stronger alignment with central concepts and superior distractor linguistic consistency and semantic plausibility, particularly for inferential questions. These findings demonstrate that hybrid, agentic orchestration can systematically improve the reliability and controllability of LLM-based generation, highlighting workflow design as a key lever for structured artifact generation beyond single-pass prompting.",
      "authors": [
        "Yu Tian",
        "Linh Huynh",
        "Katerina Christhilf",
        "Shubham Chakraborty",
        "Micah Watanabe",
        "Tracy Arner",
        "Danielle McNamara"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "published": "2026-02-03T16:26:47Z",
      "updated": "2026-02-03T16:26:47Z",
      "pdf_url": "https://arxiv.org/pdf/2602.03704v1",
      "abs_url": "http://arxiv.org/abs/2602.03704v1",
      "summary": "ReQUESTA框架利用多智能体和LLM生成认知多样化、高质量的多项选择题。",
      "key_contributions": [
        "提出ReQUESTA框架，用于生成认知多样化的多项选择题",
        "结合LLM和规则，实现可控的问题生成流程",
        "实验证明ReQUESTA生成的问题质量更高，更具挑战性和区分度"
      ],
      "methodology": "构建混合多智能体框架，分解问题生成任务，结合LLM和规则，进行规划、生成、评估和后处理。",
      "tags": [
        "多项选择题生成",
        "大型语言模型",
        "智能体",
        "阅读理解",
        "认知评估"
      ],
      "assigned_category": "agent",
      "relevance_score": 9,
      "relevance_reason": "核心是多智能体框架设计，用于改进LLM生成任务，符合agent类别。",
      "analyzed_at": "2026-02-04T20:42:43.605004",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.03695v1",
      "title": "Agent Primitives: Reusable Latent Building Blocks for Multi-Agent Systems",
      "abstract": "While existing multi-agent systems (MAS) can handle complex problems by enabling collaboration among multiple agents, they are often highly task-specific, relying on manually crafted agent roles and interaction prompts, which leads to increased architectural complexity and limited reusability across tasks. Moreover, most MAS communicate primarily through natural language, making them vulnerable to error accumulation and instability in long-context, multi-stage interactions within internal agent histories.   In this work, we propose \\textbf{Agent Primitives}, a set of reusable latent building blocks for LLM-based MAS. Inspired by neural network design, where complex models are built from reusable components, we observe that many existing MAS architectures can be decomposed into a small number of recurring internal computation patterns. Based on this observation, we instantiate three primitives: Review, Voting and Selection, and Planning and Execution. All primitives communicate internally via key-value (KV) cache, which improves both robustness and efficiency by mitigating information degradation across multi-stage interactions. To enable automatic system construction, an Organizer agent selects and composes primitives for each query, guided by a lightweight knowledge pool of previously successful configurations, forming a primitive-based MAS.   Experiments show that primitives-based MAS improve average accuracy by 12.0-16.5\\% over single-agent baselines, reduce token usage and inference latency by approximately 3$\\times$-4$\\times$ compared to text-based MAS, while incurring only 1.3$\\times$-1.6$\\times$ overhead relative to single-agent inference and providing more stable performance across model backbones.",
      "authors": [
        "Haibo Jin",
        "Kuang Peng",
        "Ye Yu",
        "Xiaopeng Yuan",
        "Haohan Wang"
      ],
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.MA",
      "published": "2026-02-03T16:17:53Z",
      "updated": "2026-02-03T16:17:53Z",
      "pdf_url": "https://arxiv.org/pdf/2602.03695v1",
      "abs_url": "http://arxiv.org/abs/2602.03695v1",
      "summary": "提出了Agent Primitives，一种可复用的多智能体系统构建块，提升了效率和鲁棒性。",
      "key_contributions": [
        "提出了Agent Primitives的概念，包括Review, Voting and Selection, Planning and Execution三种基本单元。",
        "使用KV cache进行内部通信，提高鲁棒性和效率。",
        "提出了基于知识池的自动系统构建方法，通过Organizer agent选择和组合Primitives。"
      ],
      "methodology": "通过观察现有MAS架构的共性，提炼出可复用的Agent Primitives，并设计了基于KV cache的内部通信机制，实现自动系统构建。",
      "tags": [
        "Multi-Agent Systems",
        "LLM",
        "Agent Primitives",
        "KV Cache"
      ],
      "assigned_category": "agent",
      "relevance_score": 9,
      "relevance_reason": "论文核心在于多智能体系统的设计和构建，直接研究了AI Agents领域的重要问题。",
      "analyzed_at": "2026-02-04T20:42:46.417857",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.03693v1",
      "title": "OCRTurk: A Comprehensive OCR Benchmark for Turkish",
      "abstract": "Document parsing is now widely used in applications, such as large-scale document digitization, retrieval-augmented generation, and domain-specific pipelines in healthcare and education. Benchmarking these models is crucial for assessing their reliability and practical robustness. Existing benchmarks mostly target high-resource languages and provide limited coverage for low-resource settings, such as Turkish. Moreover, existing studies on Turkish document parsing lack a standardized benchmark that reflects real-world scenarios and document diversity. To address this gap, we introduce OCRTurk, a Turkish document parsing benchmark covering multiple layout elements and document categories at three difficulty levels. OCRTurk consists of 180 Turkish documents drawn from academic articles, theses, slide decks, and non-academic articles. We evaluate seven OCR models on OCRTurk using element-wise metrics. Across difficulty levels, PaddleOCR achieves the strongest overall results, leading most element-wise metrics except figures and attaining high Normalized Edit Distance scores in easy, medium, and hard subsets. We also observe performance variation by document type. Models perform well on non-academic documents, while slideshows become the most challenging.",
      "authors": [
        "Deniz Yılmaz",
        "Evren Ayberk Munis",
        "Çağrı Toraman",
        "Süha Kağan Köse",
        "Burak Aktaş",
        "Mehmet Can Baytekin",
        "Bilge Kaan Görür"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "published": "2026-02-03T16:11:25Z",
      "updated": "2026-02-03T16:11:25Z",
      "pdf_url": "https://arxiv.org/pdf/2602.03693v1",
      "abs_url": "http://arxiv.org/abs/2602.03693v1",
      "summary": "OCRTurk是一个土耳其语文档解析基准，包含多种文档类型和难度等级，评估了七个OCR模型。",
      "key_contributions": [
        "提出了OCRTurk土耳其语文档解析基准",
        "覆盖多种文档类型和布局元素",
        "评估了七个OCR模型在OCRTurk上的性能"
      ],
      "methodology": "构建包含180个土耳其语文档的数据集，并使用元素级指标评估七个OCR模型的性能，分析不同文档类型和难度下的表现。",
      "tags": [
        "OCR",
        "文档解析",
        "土耳其语",
        "基准测试",
        "评估"
      ],
      "assigned_category": "memory",
      "relevance_score": 6,
      "relevance_reason": "与RAG相关，通过提高文档解析质量提升检索效果。",
      "analyzed_at": "2026-02-04T20:42:49.384865",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.03664v1",
      "title": "Mitigating Conversational Inertia in Multi-Turn Agents",
      "abstract": "Large language models excel as few-shot learners when provided with appropriate demonstrations, yet this strength becomes problematic in multiturn agent scenarios, where LLMs erroneously mimic their own previous responses as few-shot examples. Through attention analysis, we identify conversational inertia, a phenomenon where models exhibit strong diagonal attention to previous responses, which is associated with imitation bias that constrains exploration. This reveals a tension when transforming few-shot LLMs into agents: longer context enriches environmental feedback for exploitation, yet also amplifies conversational inertia that undermines exploration. Our key insight is that for identical states, actions generated with longer contexts exhibit stronger inertia than those with shorter contexts, enabling construction of preference pairs without environment rewards. Based on this, we propose Context Preference Learning to calibrate model preferences to favor low-inertia responses over highinertia ones. We further provide context management strategies at inference time to balance exploration and exploitation. Experimental results across eight agentic environments and one deep research scenario validate that our framework reduces conversational inertia and achieves performance improvements.",
      "authors": [
        "Yang Wan",
        "Zheng Cao",
        "Zhenhao Zhang",
        "Zhengwen Zeng",
        "Shuheng Shen",
        "Changhua Meng",
        "Linchao Zhu"
      ],
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "published": "2026-02-03T15:47:32Z",
      "updated": "2026-02-03T15:47:32Z",
      "pdf_url": "https://arxiv.org/pdf/2602.03664v1",
      "abs_url": "http://arxiv.org/abs/2602.03664v1",
      "summary": "该论文研究了多轮Agent中的对话惯性问题，并提出通过上下文偏好学习降低惯性，提升性能。",
      "key_contributions": [
        "发现了LLM Agent中的对话惯性现象",
        "提出了基于上下文偏好学习的解决方法",
        "提出了平衡探索和利用的上下文管理策略"
      ],
      "methodology": "通过注意力分析识别对话惯性，构建偏好对进行学习，并设计上下文管理策略。",
      "tags": [
        "LLM Agent",
        "对话惯性",
        "偏好学习",
        "上下文管理"
      ],
      "assigned_category": "agent",
      "relevance_score": 9,
      "relevance_reason": "论文直接研究了多轮Agent中的核心问题，并提出了有效的解决方案。",
      "analyzed_at": "2026-02-04T20:42:56.705334",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.03652v1",
      "title": "RAGTurk: Best Practices for Retrieval Augmented Generation in Turkish",
      "abstract": "Retrieval-Augmented Generation (RAG) enhances LLM factuality, yet design guidance remains English-centric, limiting insights for morphologically rich languages like Turkish. We address this by constructing a comprehensive Turkish RAG dataset derived from Turkish Wikipedia and CulturaX, comprising question-answer pairs and relevant passage chunks. We benchmark seven stages of the RAG pipeline, from query transformation and reranking to answer refinement, without task-specific fine-tuning. Our results show that complex methods like HyDE maximize accuracy (85%) that is considerably higher than the baseline (78.70%). Also a Pareto-optimal configuration using Cross-encoder Reranking and Context Augmentation achieves comparable performance (84.60%) with much lower cost. We further demonstrate that over-stacking generative modules can degrade performance by distorting morphological cues, whereas simple query clarification with robust reranking offers an effective solution.",
      "authors": [
        "Süha Kağan Köse",
        "Mehmet Can Baytekin",
        "Burak Aktaş",
        "Bilge Kaan Görür",
        "Evren Ayberk Munis",
        "Deniz Yılmaz",
        "Muhammed Yusuf Kartal",
        "Çağrı Toraman"
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "published": "2026-02-03T15:35:11Z",
      "updated": "2026-02-03T15:35:11Z",
      "pdf_url": "https://arxiv.org/pdf/2602.03652v1",
      "abs_url": "http://arxiv.org/abs/2602.03652v1",
      "summary": "该论文构建了土耳其语RAG数据集，并评估了不同RAG流程的性能，优化土耳其语RAG系统。",
      "key_contributions": [
        "构建了土耳其语RAG数据集",
        "评估了不同RAG流程在土耳其语上的性能",
        "提出了针对土耳其语RAG的优化方法"
      ],
      "methodology": "构建土耳其语数据集，基准测试RAG流程各个阶段，对比不同方法的效果，寻找帕累托最优配置。",
      "tags": [
        "RAG",
        "土耳其语",
        "检索增强生成",
        "多语言"
      ],
      "assigned_category": "memory",
      "relevance_score": 9,
      "relevance_reason": "直接研究了土耳其语RAG系统的构建和优化，属于核心相关。",
      "analyzed_at": "2026-02-04T20:42:59.339087",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.03645v1",
      "title": "Reinforcement Fine-Tuning for History-Aware Dense Retriever in RAG",
      "abstract": "Retrieval-augmented generation (RAG) enables large language models (LLMs) to produce evidence-based responses, and its performance hinges on the matching between the retriever and LLMs. Retriever optimization has emerged as an efficient alternative to fine-tuning LLMs. However, existing solutions suffer from objective mismatch between retriever optimization and the goal of RAG pipeline. Reinforcement learning (RL) provides a promising solution to address this limitation, yet applying RL to retriever optimization introduces two fundamental challenges: 1) the deterministic retrieval is incompatible with RL formulations, and 2) state aliasing arises from query-only retrieval in multi-hop reasoning. To address these challenges, we replace deterministic retrieval with stochastic sampling and formulate RAG as a Markov decision process, making retriever optimizable by RL. Further, we incorporate retrieval history into the state at each retrieval step to mitigate state aliasing. Extensive experiments across diverse RAG pipelines, datasets, and retriever scales demonstrate consistent improvements of our approach in RAG performance.",
      "authors": [
        "Yicheng Zhang",
        "Zhen Qin",
        "Zhaomin Wu",
        "Wenqi Zhang",
        "Shuiguang Deng"
      ],
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "published": "2026-02-03T15:30:14Z",
      "updated": "2026-02-03T15:30:14Z",
      "pdf_url": "https://arxiv.org/pdf/2602.03645v1",
      "abs_url": "http://arxiv.org/abs/2602.03645v1",
      "summary": "提出了一种基于强化学习的历史感知稠密检索器微调方法，优化RAG管道的检索性能。",
      "key_contributions": [
        "提出了基于强化学习的检索器优化方法。",
        "使用随机抽样代替确定性检索，使检索器可以通过RL优化。",
        "引入检索历史到状态中，缓解多跳推理中的状态混叠问题。"
      ],
      "methodology": "将RAG构建为马尔科夫决策过程，用强化学习优化检索器，并在状态中加入检索历史信息。",
      "tags": [
        "RAG",
        "强化学习",
        "检索器优化",
        "历史感知"
      ],
      "assigned_category": "memory",
      "relevance_score": 9,
      "relevance_reason": "直接研究RAG中的检索器优化问题，与RAG领域核心相关。",
      "analyzed_at": "2026-02-04T20:43:02.493975",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.03640v1",
      "title": "Tutorial on Reasoning for IR & IR for Reasoning",
      "abstract": "Information retrieval has long focused on ranking documents by semantic relatedness. Yet many real-world information needs demand more: enforcement of logical constraints, multi-step inference, and synthesis of multiple pieces of evidence. Addressing these requirements is, at its core, a problem of reasoning. Across AI communities, researchers are developing diverse solutions for the problem of reasoning, from inference-time strategies and post-training of LLMs, to neuro-symbolic systems, Bayesian and probabilistic frameworks, geometric representations, and energy-based models. These efforts target the same problem: to move beyond pattern-matching systems toward structured, verifiable inference. However, they remain scattered across disciplines, making it difficult for IR researchers to identify the most relevant ideas and opportunities. To help navigate the fragmented landscape of research in reasoning, this tutorial first articulates a working definition of reasoning within the context of information retrieval and derives from it a unified analytical framework. The framework maps existing approaches along axes that reflect the core components of the definition. By providing a comprehensive overview of recent approaches and mapping current methods onto the defined axes, we expose their trade-offs and complementarities, highlight where IR can benefit from cross-disciplinary advances, and illustrate how retrieval process itself can play a central role in broader reasoning systems. The tutorial will equip participants with both a conceptual framework and practical guidance for enhancing reasoning-capable IR systems, while situating IR as a domain that both benefits and contributes to the broader development of reasoning methodologies.",
      "authors": [
        "Mohanna Hoveyda",
        "Panagiotis Efstratiadis",
        "Arjen de Vries",
        "Maarten de Rijke"
      ],
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "published": "2026-02-03T15:24:36Z",
      "updated": "2026-02-03T15:24:36Z",
      "pdf_url": "https://arxiv.org/pdf/2602.03640v1",
      "abs_url": "http://arxiv.org/abs/2602.03640v1",
      "summary": "本教程定义了信息检索中的推理，构建统一分析框架，促进跨学科合作，提升IR系统的推理能力。",
      "key_contributions": [
        "定义了信息检索中推理的概念",
        "构建了推理方法的统一分析框架",
        "揭示了现有方法的权衡和互补性，强调了IR在推理系统中的作用"
      ],
      "methodology": "通过文献综述和分析，构建概念框架，并对现有方法进行映射和比较，从而提供指导。",
      "tags": [
        "信息检索",
        "推理",
        "知识推理",
        "语义理解"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 9,
      "relevance_reason": "论文核心在于探讨LLM在信息检索任务中的推理能力，高度相关。",
      "analyzed_at": "2026-02-04T20:43:05.073105",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.03630v1",
      "title": "Can LLMs Do Rocket Science? Exploring the Limits of Complex Reasoning with GTOC 12",
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable proficiency in code generation and general reasoning, yet their capacity for autonomous multi-stage planning in high-dimensional, physically constrained environments remains an open research question. This study investigates the limits of current AI agents by evaluating them against the 12th Global Trajectory Optimization Competition (GTOC 12), a complex astrodynamics challenge requiring the design of a large-scale asteroid mining campaign. We adapt the MLE-Bench framework to the domain of orbital mechanics and deploy an AIDE-based agent architecture to autonomously generate and refine mission solutions. To assess performance beyond binary validity, we employ an \"LLM-as-a-Judge\" methodology, utilizing a rubric developed by domain experts to evaluate strategic viability across five structural categories. A comparative analysis of models, ranging from GPT-4-Turbo to reasoning-enhanced architectures like Gemini 2.5 Pro, and o3, reveals a significant trend: the average strategic viability score has nearly doubled in the last two years (rising from 9.3 to 17.2 out of 26). However, we identify a critical capability gap between strategy and execution. While advanced models demonstrate sophisticated conceptual understanding, correctly framing objective functions and mission architectures, they consistently fail at implementation due to physical unit inconsistencies, boundary condition errors, and inefficient debugging loops. We conclude that, while current LLMs often demonstrate sufficient knowledge and intelligence to tackle space science tasks, they remain limited by an implementation barrier, functioning as powerful domain facilitators rather than fully autonomous engineers.",
      "authors": [
        "Iñaki del Campo",
        "Pablo Cuervo",
        "Victor Rodriguez-Fernandez",
        "Roberto Armellin",
        "Jack Yarndley"
      ],
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "published": "2026-02-03T15:18:26Z",
      "updated": "2026-02-03T15:18:26Z",
      "pdf_url": "https://arxiv.org/pdf/2602.03630v1",
      "abs_url": "http://arxiv.org/abs/2602.03630v1",
      "summary": "评估LLM在复杂航天任务中的能力，发现其擅长策略但缺乏执行力。",
      "key_contributions": [
        "评估LLM在GTOC 12挑战中的表现",
        "提出“LLM-as-a-Judge”的评估方法",
        "揭示LLM在策略和执行之间的能力差距"
      ],
      "methodology": "使用AIDE-based agent架构，并采用LLM作为裁判评估LLM生成的任务方案。",
      "tags": [
        "LLM",
        "航天",
        "GTOC 12",
        "推理",
        "智能体"
      ],
      "assigned_category": "agent",
      "relevance_score": 9,
      "relevance_reason": "论文直接研究LLM作为智能体解决复杂航天任务的关键问题。",
      "analyzed_at": "2026-02-04T20:43:10.750284",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.03627v1",
      "title": "Ultra Fast PDE Solving via Physics Guided Few-step Diffusion",
      "abstract": "Diffusion-based models have demonstrated impressive accuracy and generalization in solving partial differential equations (PDEs). However, they still face significant limitations, such as high sampling costs and insufficient physical consistency, stemming from their many-step iterative sampling mechanism and lack of explicit physics constraints. To address these issues, we propose Phys-Instruct, a novel physics-guided distillation framework which not only (1) compresses a pre-trained diffusion PDE solver into a few-step generator via matching generator and prior diffusion distributions to enable rapid sampling, but also (2) enhances the physics consistency by explicitly injecting PDE knowledge through a PDE distillation guidance. Physic-Instruct is built upon a solid theoretical foundation, leading to a practical physics-constrained training objective that admits tractable gradients. Across five PDE benchmarks, Phys-Instruct achieves orders-of-magnitude faster inference while reducing PDE error by more than 8 times compared to state-of-the-art diffusion baselines. Moreover, the resulting unconditional student model functions as a compact prior, enabling efficient and physically consistent inference for various downstream conditional tasks. Our results indicate that Phys-Instruct is a novel, effective, and efficient framework for ultra-fast PDE solving powered by deep generative models.",
      "authors": [
        "Cindy Xiangrui Kong",
        "Yueqi Wang",
        "Haoyang Zheng",
        "Weijian Luo",
        "Guang Lin"
      ],
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "published": "2026-02-03T15:16:42Z",
      "updated": "2026-02-03T15:16:42Z",
      "pdf_url": "https://arxiv.org/pdf/2602.03627v1",
      "abs_url": "http://arxiv.org/abs/2602.03627v1",
      "summary": "Phys-Instruct通过物理引导的蒸馏，加速扩散模型求解偏微分方程，并提升物理一致性。",
      "key_contributions": [
        "提出Phys-Instruct框架，加速PDE求解。",
        "通过PDE知识蒸馏，增强物理一致性。",
        "实现比现有扩散模型快几个数量级的推理速度，并降低PDE误差。"
      ],
      "methodology": "通过匹配生成器和先验扩散分布，将预训练扩散模型压缩为少步生成器，并注入PDE知识。",
      "tags": [
        "扩散模型",
        "偏微分方程",
        "知识蒸馏",
        "物理约束"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 7,
      "relevance_reason": "涉及通过物理信息提升模型推理能力，与LLM推理具有一定关联。",
      "analyzed_at": "2026-02-04T20:43:13.265212",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.03619v1",
      "title": "Learning Query-Specific Rubrics from Human Preferences for DeepResearch Report Generation",
      "abstract": "Nowadays, training and evaluating DeepResearch-generated reports remain challenging due to the lack of verifiable reward signals. Accordingly, rubric-based evaluation has become a common practice. However, existing approaches either rely on coarse, pre-defined rubrics that lack sufficient granularity, or depend on manually constructed query-specific rubrics that are costly and difficult to scale. In this paper, we propose a pipeline to train human-preference-aligned query-specific rubric generators tailored for DeepResearch report generation. We first construct a dataset of DeepResearch-style queries annotated with human preferences over paired reports, and train rubric generators via reinforcement learning with a hybrid reward combining human preference supervision and LLM-based rubric evaluation. To better handle long-horizon reasoning, we further introduce a Multi-agent Markov-state (MaMs) workflow for report generation. We empirically show that our proposed rubric generators deliver more discriminative and better human-aligned supervision than existing rubric design strategies. Moreover, when integrated into the MaMs training framework, DeepResearch systems equipped with our rubric generators consistently outperform all open-source baselines on the DeepResearch Bench and achieve performance comparable to that of leading closed-source models.",
      "authors": [
        "Changze Lv",
        "Jie Zhou",
        "Wentao Zhao",
        "Jingwen Xu",
        "Zisu Huang",
        "Muzhao Tian",
        "Shihan Dou",
        "Tao Gui",
        "Le Tian",
        "Xiao Zhou",
        "Xiaoqing Zheng",
        "Xuanjing Huang",
        "Jie Zhou"
      ],
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "published": "2026-02-03T15:09:56Z",
      "updated": "2026-02-03T15:09:56Z",
      "pdf_url": "https://arxiv.org/pdf/2602.03619v1",
      "abs_url": "http://arxiv.org/abs/2602.03619v1",
      "summary": "提出一种基于人类偏好的查询特定评估标准生成方法，用于提升深度研究报告的生成质量。",
      "key_contributions": [
        "构建了深度研究风格查询及人类偏好标注的数据集",
        "提出使用混合奖励强化学习训练评估标准生成器",
        "引入多智能体马尔可夫状态工作流(MaMs)提升报告生成效果"
      ],
      "methodology": "通过强化学习训练评估标准生成器，结合人类偏好监督和LLM评估，并采用MaMs工作流优化长程推理。",
      "tags": [
        "深度研究报告",
        "评估标准生成",
        "强化学习",
        "人类偏好",
        "多智能体"
      ],
      "assigned_category": "agent",
      "relevance_score": 8,
      "relevance_reason": "涉及多智能体和AI Agent驱动的研究报告生成，密切相关。",
      "analyzed_at": "2026-02-04T20:43:15.620819",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.03611v1",
      "title": "Explanations Leak: Membership Inference with Differential Privacy and Active Learning Defense",
      "abstract": "Counterfactual explanations (CFs) are increasingly integrated into Machine Learning as a Service (MLaaS) systems to improve transparency; however, ML models deployed via APIs are already vulnerable to privacy attacks such as membership inference and model extraction, and the impact of explanations on this threat landscape remains insufficiently understood. In this work, we focus on the problem of how CFs expand the attack surface of MLaaS by strengthening membership inference attacks (MIAs), and on the need to design defense mechanisms that mitigate this emerging risk without undermining utility and explainability. First, we systematically analyze how exposing CFs through query-based APIs enables more effective shadow-based MIAs. Second, we propose a defense framework that integrates Differential Privacy (DP) with Active Learning (AL) to jointly reduce memorization and limit effective training data exposure. Finally, we conduct an extensive empirical evaluation to characterize the three-way trade-off between privacy leakage, predictive performance, and explanation quality. Our findings highlight the need to carefully balance transparency, utility, and privacy in the responsible deployment of explainable MLaaS systems.",
      "authors": [
        "Fatima Ezzeddine",
        "Osama Zammar",
        "Silvia Giordano",
        "Omran Ayoub"
      ],
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "published": "2026-02-03T15:04:09Z",
      "updated": "2026-02-03T15:04:09Z",
      "pdf_url": "https://arxiv.org/pdf/2602.03611v1",
      "abs_url": "http://arxiv.org/abs/2602.03611v1",
      "summary": "研究对抗性解释如何增强成员推理攻击，并提出差分隐私和主动学习结合的防御框架。",
      "key_contributions": [
        "分析了解释泄露对成员推理攻击的影响",
        "提出了基于差分隐私和主动学习的防御框架",
        "评估了隐私泄露、预测性能和解释质量之间的权衡"
      ],
      "methodology": "通过查询API暴露对抗解释，构建影子模型进行成员推理攻击，并使用DP和AL降低模型记忆。",
      "tags": [
        "成员推理攻击",
        "对抗性解释",
        "差分隐私",
        "主动学习",
        "隐私保护"
      ],
      "assigned_category": "memory",
      "relevance_score": 5,
      "relevance_reason": "虽然主要关注隐私攻击，但涉及模型记忆和数据泄露，有一定参考价值。",
      "analyzed_at": "2026-02-04T20:43:19.000153",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.03582v1",
      "title": "Optimization and Generation in Aerodynamics Inverse Design",
      "abstract": "Inverse design with physics-based objectives is challenging because it couples high-dimensional geometry with expensive simulations, as exemplified by aerodynamic shape optimization for drag reduction. We revisit inverse design through two canonical solutions, the optimal design point and the optimal design distribution, and relate them to optimization and guided generation. Building on this view, we propose a new training loss for cost predictors and a density-gradient optimization method that improves objectives while preserving plausible shapes. We further unify existing training-free guided generation methods. To address their inability to approximate conditional covariance in high dimensions, we develop a time- and memory-efficient algorithm for approximate covariance estimation. Experiments on a controlled 2D study and high-fidelity 3D aerodynamic benchmarks (car and aircraft), validated by OpenFOAM simulations and miniature wind-tunnel tests with 3D-printed prototypes, demonstrate consistent gains in both optimization and guided generation. Additional offline RL results further support the generality of our approach.",
      "authors": [
        "Huaguan Chen",
        "Ning Lin",
        "Luxi Chen",
        "Rui Zhang",
        "Wenbing Huang",
        "Chongxuan Li",
        "Hao Sun"
      ],
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "published": "2026-02-03T14:32:26Z",
      "updated": "2026-02-03T14:32:26Z",
      "pdf_url": "https://arxiv.org/pdf/2602.03582v1",
      "abs_url": "http://arxiv.org/abs/2602.03582v1",
      "summary": "论文提出优化和引导生成方法，解决气动逆向设计中高维几何与昂贵仿真的挑战。",
      "key_contributions": [
        "提出新的成本预测器训练损失",
        "开发密度梯度优化方法",
        "统一现有无训练引导生成方法",
        "提出时间高效的近似协方差估计算法"
      ],
      "methodology": "结合优化和引导生成视角，改进成本预测，优化设计，并通过OpenFOAM和风洞实验验证。",
      "tags": [
        "气动优化",
        "逆向设计",
        "引导生成",
        "协方差估计",
        "强化学习"
      ],
      "assigned_category": "agent",
      "relevance_score": 5,
      "relevance_reason": "通过优化设计进行任务规划和执行，可用于智能体设计。",
      "analyzed_at": "2026-02-04T20:43:29.514853",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.03578v1",
      "title": "Use Graph When It Needs: Efficiently and Adaptively Integrating Retrieval-Augmented Generation with Graphs",
      "abstract": "Large language models (LLMs) often struggle with knowledge-intensive tasks due to hallucinations and outdated parametric knowledge. While Retrieval-Augmented Generation (RAG) addresses this by integrating external corpora, its effectiveness is limited by fragmented information in unstructured domain documents. Graph-augmented RAG (GraphRAG) emerged to enhance contextual reasoning through structured knowledge graphs, yet paradoxically underperforms vanilla RAG in real-world scenarios, exhibiting significant accuracy drops and prohibitive latency despite gains on complex queries. We identify the rigid application of GraphRAG to all queries, regardless of complexity, as the root cause. To resolve this, we propose an efficient and adaptive GraphRAG framework called EA-GraphRAG that dynamically integrates RAG and GraphRAG paradigms through syntax-aware complexity analysis. Our approach introduces: (i) a syntactic feature constructor that parses each query and extracts a set of structural features; (ii) a lightweight complexity scorer that maps these features to a continuous complexity score; and (iii) a score-driven routing policy that selects dense RAG for low-score queries, invokes graph-based retrieval for high-score queries, and applies complexity-aware reciprocal rank fusion to handle borderline cases. Extensive experiments on a comprehensive benchmark, consisting of two single-hop and two multi-hop QA benchmarks, demonstrate that our EA-GraphRAG significantly improves accuracy, reduces latency, and achieves state-of-the-art performance in handling mixed scenarios involving both simple and complex queries.",
      "authors": [
        "Su Dong",
        "Qinggang Zhang",
        "Yilin Xiao",
        "Shengyuan Chen",
        "Chuang Zhou",
        "Xiao Huang"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "published": "2026-02-03T14:26:28Z",
      "updated": "2026-02-03T14:26:28Z",
      "pdf_url": "https://arxiv.org/pdf/2602.03578v1",
      "abs_url": "http://arxiv.org/abs/2602.03578v1",
      "summary": "EA-GraphRAG通过语法分析自适应地结合RAG和GraphRAG，提升了知识密集型任务的准确性和效率。",
      "key_contributions": [
        "提出了语法感知的复杂度分析方法",
        "设计了轻量级的复杂度评分器",
        "实现了基于分数的自适应路由策略"
      ],
      "methodology": "通过语法特征提取和复杂度评分，动态选择RAG或GraphRAG，并用复杂性感知的倒数排名融合处理边界情况。",
      "tags": [
        "RAG",
        "知识图谱",
        "检索增强生成",
        "复杂度分析"
      ],
      "assigned_category": "memory",
      "relevance_score": 9,
      "relevance_reason": "论文核心在于改进RAG，利用图谱提高知识检索和利用效率。",
      "analyzed_at": "2026-02-04T20:43:32.619552",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.03569v1",
      "title": "EHRWorld: A Patient-Centric Medical World Model for Long-Horizon Clinical Trajectories",
      "abstract": "World models offer a principled framework for simulating future states under interventions, but realizing such models in complex, high-stakes domains like medicine remains challenging. Recent large language models (LLMs) have achieved strong performance on static medical reasoning tasks, raising the question of whether they can function as dynamic medical world models capable of simulating disease progression and treatment outcomes over time. In this work, we show that LLMs only incorporating medical knowledge struggle to maintain consistent patient states under sequential interventions, leading to error accumulation in long-horizon clinical simulation. To address this limitation, we introduce EHRWorld, a patient-centric medical world model trained under a causal sequential paradigm, together with EHRWorld-110K, a large-scale longitudinal clinical dataset derived from real-world electronic health records. Extensive evaluations demonstrate that EHRWorld significantly outperforms naive LLM-based baselines, achieving more stable long-horizon simulation, improved modeling of clinically sensitive events, and favorable reasoning efficiency, highlighting the necessity of training on causally grounded, temporally evolving clinical data for reliable and robust medical world modeling.",
      "authors": [
        "Linjie Mu",
        "Zhongzhen Huang",
        "Yannian Gu",
        "Shengqian Qin",
        "Shaoting Zhang",
        "Xiaofan Zhang"
      ],
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "published": "2026-02-03T14:12:24Z",
      "updated": "2026-02-03T14:12:24Z",
      "pdf_url": "https://arxiv.org/pdf/2602.03569v1",
      "abs_url": "http://arxiv.org/abs/2602.03569v1",
      "summary": "EHRWorld模型通过在临床数据上训练，显著提升了LLM在长期医疗模拟中的稳定性和准确性。",
      "key_contributions": [
        "提出了EHRWorld模型，用于模拟长期临床轨迹。",
        "构建了大规模纵向临床数据集EHRWorld-110K。",
        "证明了在因果和时序临床数据上训练对于可靠的医疗世界建模至关重要。"
      ],
      "methodology": "使用因果序贯范式，在大规模电子病历数据集上训练患者中心的医学世界模型EHRWorld。",
      "tags": [
        "医疗AI",
        "世界模型",
        "电子病历",
        "因果推理",
        "LLM"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 8,
      "relevance_reason": "论文侧重于使用LLM进行医学推理，模拟疾病发展和治疗结果。",
      "analyzed_at": "2026-02-04T20:43:36.026912",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.03554v1",
      "title": "When Single Answer Is Not Enough: Rethinking Single-Step Retrosynthesis Benchmarks for LLMs",
      "abstract": "Recent progress has expanded the use of large language models (LLMs) in drug discovery, including synthesis planning. However, objective evaluation of retrosynthesis performance remains limited. Existing benchmarks and metrics typically rely on published synthetic procedures and Top-K accuracy based on single ground-truth, which does not capture the open-ended nature of real-world synthesis planning. We propose a new benchmarking framework for single-step retrosynthesis that evaluates both general-purpose and chemistry-specialized LLMs using ChemCensor, a novel metric for chemical plausibility. By emphasizing plausibility over exact match, this approach better aligns with human synthesis planning practices. We also introduce CREED, a novel dataset comprising millions of ChemCensor-validated reaction records for LLM training, and use it to train a model that improves over the LLM baselines under this benchmark.",
      "authors": [
        "Bogdan Zagribelnyy",
        "Ivan Ilin",
        "Maksim Kuznetsov",
        "Nikita Bondarev",
        "Roman Schutski",
        "Thomas MacDougall",
        "Rim Shayakhmetov",
        "Zulfat Miftakhutdinov",
        "Mikolaj Mizera",
        "Vladimir Aladinskiy",
        "Alex Aliper",
        "Alex Zhavoronkov"
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "published": "2026-02-03T14:03:32Z",
      "updated": "2026-02-03T14:03:32Z",
      "pdf_url": "https://arxiv.org/pdf/2602.03554v1",
      "abs_url": "http://arxiv.org/abs/2602.03554v1",
      "summary": "论文提出一种新的单步逆合成基准测试框架，并使用化学合理性指标ChemCensor评估LLM的性能。",
      "key_contributions": [
        "提出了新的逆合成基准测试框架",
        "引入了化学合理性指标ChemCensor",
        "构建了大规模数据集CREED用于LLM训练"
      ],
      "methodology": "使用ChemCensor评估LLM生成的逆合成反应的化学合理性，并使用CREED数据集训练LLM。",
      "tags": [
        "LLM",
        "逆合成",
        "药物发现",
        "基准测试",
        "化学信息学"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 8,
      "relevance_reason": "涉及LLM在特定领域的推理能力评估，并提出新评估方法。",
      "analyzed_at": "2026-02-04T20:43:44.804699",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.03548v1",
      "title": "SEAD: Self-Evolving Agent for Multi-Turn Service Dialogue",
      "abstract": "Large Language Models have demonstrated remarkable capabilities in open-domain dialogues. However, current methods exhibit suboptimal performance in service dialogues, as they rely on noisy, low-quality human conversation data. This limitation arises from data scarcity and the difficulty of simulating authentic, goal-oriented user behaviors. To address these issues, we propose SEAD (Self-Evolving Agent for Service Dialogue), a framework that enables agents to learn effective strategies without large-scale human annotations. SEAD decouples user modeling into two components: a Profile Controller that generates diverse user states to manage training curriculum, and a User Role-play Model that focuses on realistic role-playing. This design ensures the environment provides adaptive training scenarios rather than acting as an unfair adversary. Experiments demonstrate that SEAD significantly outperforms Open-source Foundation Models and Closed-source Commercial Models, improving task completion rate by 17.6% and dialogue efficiency by 11.1%. Code is available at: https://github.com/Da1yuqin/SEAD.",
      "authors": [
        "Yuqin Dai",
        "Ning Gao",
        "Wei Zhang",
        "Jie Wang",
        "Zichen Luo",
        "Jinpeng Wang",
        "Yujie Wang",
        "Ruiyuan Wu",
        "Chaozheng Wang"
      ],
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "published": "2026-02-03T14:01:11Z",
      "updated": "2026-02-03T14:01:11Z",
      "pdf_url": "https://arxiv.org/pdf/2602.03548v1",
      "abs_url": "http://arxiv.org/abs/2602.03548v1",
      "summary": "SEAD框架通过自进化学习提升LLM在服务对话中的表现，无需大量人工标注。",
      "key_contributions": [
        "提出SEAD框架，解决服务对话数据稀缺和用户行为模拟难题",
        "解耦用户建模为Profile Controller和User Role-play Model",
        "显著提升任务完成率和对话效率"
      ],
      "methodology": "SEAD通过Profile Controller生成多样用户状态，User Role-play Model模拟真实用户行为，实现自适应训练。",
      "tags": [
        "服务对话",
        "自进化学习",
        "用户建模",
        "强化学习"
      ],
      "assigned_category": "agent",
      "relevance_score": 9,
      "relevance_reason": "论文核心在于构建服务对话Agent，属于AI Agent领域关键研究。",
      "analyzed_at": "2026-02-04T20:43:47.620157",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.03527v1",
      "title": "WARP Logic Neural Networks",
      "abstract": "Fast and efficient AI inference is increasingly important, and recent models that directly learn low-level logic operations have achieved state-of-the-art performance. However, existing logic neural networks incur high training costs, introduce redundancy or rely on approximate gradients, which limits scalability. To overcome these limitations, we introduce WAlsh Relaxation for Probabilistic (WARP) logic neural networks -- a novel gradient-based framework that efficiently learns combinations of hardware-native logic blocks. We show that WARP yields the most parameter-efficient representation for exactly learning Boolean functions and that several prior approaches arise as restricted special cases. Training is improved by introducing learnable thresholding and residual initialization, while we bridge the gap between relaxed training and discrete logic inference through stochastic smoothing. Experiments demonstrate faster convergence than state-of-the-art baselines, while scaling effectively to deeper architectures and logic functions with higher input arity.",
      "authors": [
        "Lino Gerlach",
        "Thore Gerlach",
        "Liv Våge",
        "Elliott Kauffman",
        "Isobel Ojalvo"
      ],
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "published": "2026-02-03T13:46:51Z",
      "updated": "2026-02-03T13:46:51Z",
      "pdf_url": "https://arxiv.org/pdf/2602.03527v1",
      "abs_url": "http://arxiv.org/abs/2602.03527v1",
      "summary": "WARP逻辑神经网络通过高效学习硬件原生逻辑块组合，降低训练成本，提高推理速度。",
      "key_contributions": [
        "提出WARP逻辑神经网络框架",
        "参数效率最高的布尔函数表示",
        "引入可学习阈值和残差初始化"
      ],
      "methodology": "基于梯度学习，利用Walsh松弛学习概率逻辑，结合随机平滑实现离散逻辑推理。",
      "tags": [
        "逻辑神经网络",
        "硬件加速",
        "机器学习",
        "梯度下降"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 9,
      "relevance_reason": "论文核心关注逻辑推理，并提出新的逻辑神经网络模型。",
      "analyzed_at": "2026-02-04T20:43:53.816651",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.03516v1",
      "title": "Not All Negative Samples Are Equal: LLMs Learn Better from Plausible Reasoning",
      "abstract": "Learning from negative samples holds great promise for improving Large Language Model (LLM) reasoning capability, yet existing methods treat all incorrect responses as equally informative, overlooking the crucial role of sample quality. To address this, we propose Plausible Negative Samples (PNS), a method that synthesizes high-quality negative samples exhibiting expected format and structural coherence while ultimately yielding incorrect answers. PNS trains a dedicated model via reverse reinforcement learning (RL) guided by a composite reward combining format compliance, accuracy inversion, reward model assessment, and chain-of-thought evaluation, generating responses nearly indistinguishable from correct solutions. We further validate PNS as a plug-and-play data source for preference optimization across three backbone models on seven mathematical reasoning benchmarks. Results demonstrate that PNS consistently outperforms other negative sample synthesis methods, achieving an average improvement of 2.03% over RL-trained models.",
      "authors": [
        "Zixiang Di",
        "Jinyi Han",
        "Shuo Zhang",
        "Ying Liao",
        "Zhi Li",
        "Xiaofeng Ji",
        "Yongqi Wang",
        "Zheming Yang",
        "Ming Gao",
        "Bingdong Li",
        "Jie Wang"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "published": "2026-02-03T13:32:02Z",
      "updated": "2026-02-03T13:32:02Z",
      "pdf_url": "https://arxiv.org/pdf/2602.03516v1",
      "abs_url": "http://arxiv.org/abs/2602.03516v1",
      "summary": "提出PNS方法，通过合成高质量负样本来提升LLM的推理能力。",
      "key_contributions": [
        "提出了Plausible Negative Samples（PNS）方法",
        "使用逆向强化学习生成高质量负样本",
        "验证了PNS作为偏好优化的数据源的有效性"
      ],
      "methodology": "使用逆向强化学习训练模型，生成格式正确但答案错误的负样本，用于训练LLM。",
      "tags": [
        "LLM",
        "推理",
        "负样本",
        "强化学习"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 9,
      "relevance_reason": "论文直接针对LLM的推理能力，并提出了新的负样本生成方法。",
      "analyzed_at": "2026-02-04T20:43:56.162416",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.03507v1",
      "title": "Learning to Reason Faithfully through Step-Level Faithfulness Maximization",
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has markedly improved the performance of Large Language Models (LLMs) on tasks requiring multi-step reasoning. However, most RLVR pipelines rely on sparse outcome-based rewards, providing little supervision over intermediate steps and thus encouraging over-confidence and spurious reasoning, which in turn increases hallucinations. To address this, we propose FaithRL, a general reinforcement learning framework that directly optimizes reasoning faithfulness. We formalize a faithfulness-maximization objective and theoretically show that optimizing it mitigates over-confidence. To instantiate this objective, we introduce a geometric reward design and a faithfulness-aware advantage modulation mechanism that assigns step-level credit by penalizing unsupported steps while preserving valid partial derivations. Across diverse backbones and benchmarks, FaithRL consistently reduces hallucination rates while maintaining (and often improving) answer correctness. Further analysis confirms that FaithRL increases step-wise reasoning faithfulness and generalizes robustly. Our code is available at https://github.com/aintdoin/FaithRL.",
      "authors": [
        "Runquan Gui",
        "Yafu Li",
        "Xiaoye Qu",
        "Ziyan Liu",
        "Yeqiu Cheng",
        "Yu Cheng"
      ],
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "published": "2026-02-03T13:28:17Z",
      "updated": "2026-02-03T13:28:17Z",
      "pdf_url": "https://arxiv.org/pdf/2602.03507v1",
      "abs_url": "http://arxiv.org/abs/2602.03507v1",
      "summary": "FaithRL通过最大化步骤级忠实度来提升LLM多步推理的可靠性，降低幻觉率。",
      "key_contributions": [
        "提出了FaithRL框架，直接优化推理忠实度",
        "设计了几何奖励机制和忠实度感知的优势调制机制",
        "理论证明优化忠实度目标可以缓解过度自信问题"
      ],
      "methodology": "FaithRL通过几何奖励惩罚不支持的步骤，并使用忠实度感知的优势调制机制分配步骤级信用，从而最大化推理忠实度。",
      "tags": [
        "Reinforcement Learning",
        "Reasoning",
        "Faithfulness",
        "Hallucination"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 10,
      "relevance_reason": "论文核心在于提升LLM的推理能力和忠实度，直接解决关键问题。",
      "analyzed_at": "2026-02-04T20:43:59.243319",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.03496v1",
      "title": "Lookahead Path Likelihood Optimization for Diffusion LLMs",
      "abstract": "Diffusion Large Language Models (dLLMs) support arbitrary-order generation, yet their inference performance critically depends on the unmasking order. Existing strategies rely on heuristics that greedily optimize local confidence, offering limited guidance for identifying unmasking paths that are globally consistent and accurate. To bridge this gap, we introduce path log-likelihood (Path LL), a trajectory-conditioned objective that strongly correlates with downstream accuracy and enables principled selection of unmasking paths. To optimize Path LL at inference time, we propose POKE, an efficient value estimator that predicts the expected future Path LL of a partial decoding trajectory. We then integrate this lookahead signal into POKE-SMC, a Sequential Monte Carlo-based search framework for dynamically identifying optimal unmasking paths. Extensive experiments across 6 reasoning tasks show that POKE-SMC consistently improves accuracy, achieving 2%--3% average gains over strong decoding-time scaling baselines at comparable inference overhead on LLaDA models and advancing the accuracy--compute Pareto frontier.",
      "authors": [
        "Xuejie Liu",
        "Yap Vit Chun",
        "Yitao Liang",
        "Anji Liu"
      ],
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "published": "2026-02-03T13:12:41Z",
      "updated": "2026-02-03T13:12:41Z",
      "pdf_url": "https://arxiv.org/pdf/2602.03496v1",
      "abs_url": "http://arxiv.org/abs/2602.03496v1",
      "summary": "提出了一种基于路径似然优化的扩散LLM解码方法，提升推理准确性。",
      "key_contributions": [
        "提出了路径对数似然(Path LL)目标",
        "设计了高效的值估计器POKE",
        "提出了基于POKE的序列蒙特卡洛搜索框架POKE-SMC"
      ],
      "methodology": "引入路径似然作为优化目标，通过值估计器预测未来路径似然，并利用蒙特卡洛搜索寻找最优解码路径。",
      "tags": [
        "Diffusion LLM",
        "Inference",
        "Unmasking Order",
        "Lookahead Search"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 9,
      "relevance_reason": "论文核心关注LLM的推理和解码过程优化，属于推理领域关键问题。",
      "analyzed_at": "2026-02-04T20:44:01.958782",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.03486v1",
      "title": "DeepDFA: Injecting Temporal Logic in Deep Learning for Sequential Subsymbolic Applications",
      "abstract": "Integrating logical knowledge into deep neural network training is still a hard challenge, especially for sequential or temporally extended domains involving subsymbolic observations. To address this problem, we propose DeepDFA, a neurosymbolic framework that integrates high-level temporal logic - expressed as Deterministic Finite Automata (DFA) or Moore Machines - into neural architectures. DeepDFA models temporal rules as continuous, differentiable layers, enabling symbolic knowledge injection into subsymbolic domains. We demonstrate how DeepDFA can be used in two key settings: (i) static image sequence classification, and (ii) policy learning in interactive non-Markovian environments. Across extensive experiments, DeepDFA outperforms traditional deep learning models (e.g., LSTMs, GRUs, Transformers) and novel neuro-symbolic systems, achieving state-of-the-art results in temporal knowledge integration. These results highlight the potential of DeepDFA to bridge subsymbolic learning and symbolic reasoning in sequential tasks.",
      "authors": [
        "Elena Umili",
        "Francesco Argenziano",
        "Roberto Capobianco"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "published": "2026-02-03T12:59:47Z",
      "updated": "2026-02-03T12:59:47Z",
      "pdf_url": "https://arxiv.org/pdf/2602.03486v1",
      "abs_url": "http://arxiv.org/abs/2602.03486v1",
      "summary": "DeepDFA通过将时序逻辑注入深度学习，提升序列子符号应用性能。",
      "key_contributions": [
        "提出DeepDFA神经符号框架",
        "将时序逻辑（DFA）建模为可微分层",
        "在图像序列分类和非马尔可夫环境策略学习中验证有效性"
      ],
      "methodology": "DeepDFA将确定性有限自动机集成到神经网络架构中，实现符号知识注入，并利用可微分层进行训练。",
      "tags": [
        "神经符号学习",
        "时序逻辑",
        "深度学习",
        "序列建模"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 8,
      "relevance_reason": "论文核心在于将逻辑推理融入深度学习，属于LLM reasoning的高级应用。",
      "analyzed_at": "2026-02-04T20:44:10.201010",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.03485v1",
      "title": "Self-Verification Dilemma: Experience-Driven Suppression of Overused Checking in LLM Reasoning",
      "abstract": "Large Reasoning Models (LRMs) achieve strong performance by generating long reasoning traces with reflection. Through a large-scale empirical analysis, we find that a substantial fraction of reflective steps consist of self-verification (recheck) that repeatedly confirm intermediate results. These rechecks occur frequently across models and benchmarks, yet the vast majority are confirmatory rather than corrective, rarely identifying errors and altering reasoning outcomes. This reveals a mismatch between how often self-verification is activated and how often it is actually useful. Motivated by this, we propose a novel, experience-driven test-time framework that reduces the overused verification. Our method detects the activation of recheck behavior, consults an offline experience pool of past verification outcomes, and estimates whether a recheck is likely unnecessary via efficient retrieval. When historical experience suggests unnecessary, a suppression signal redirects the model to proceed. Across multiple model and benchmarks, our approach reduces token usage up to 20.3% while maintaining the accuracy, and in some datasets even yields accuracy improvements.",
      "authors": [
        "Quanyu Long",
        "Kai Jie Jiang",
        "Jianda Chen",
        "Xu Guo",
        "Leilei Gan",
        "Wenya Wang"
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "published": "2026-02-03T12:58:23Z",
      "updated": "2026-02-03T12:58:23Z",
      "pdf_url": "https://arxiv.org/pdf/2602.03485v1",
      "abs_url": "http://arxiv.org/abs/2602.03485v1",
      "summary": "论文发现LLM推理中过度自验证现象，提出经验驱动框架抑制无效自验证，减少token使用并保持甚至提升准确率。",
      "key_contributions": [
        "发现LLM推理中过度自验证问题",
        "提出经验驱动的自验证抑制框架",
        "实验证明该方法能减少token使用并维持/提升准确率"
      ],
      "methodology": "通过检测LLM的自验证行为，检索历史经验池判断是否需要验证，如果经验表明不必要，则抑制验证。",
      "tags": [
        "LLM",
        "Reasoning",
        "Self-Verification",
        "Efficiency",
        "Experience-Driven"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 9,
      "relevance_reason": "论文直接研究LLM推理中的自验证问题，是LLM推理领域的核心问题。",
      "analyzed_at": "2026-02-04T20:44:12.964657",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.03837v1",
      "title": "Accelerating Scientific Research with Gemini: Case Studies and Common Techniques",
      "abstract": "Recent advances in large language models (LLMs) have opened new avenues for accelerating scientific research. While models are increasingly capable of assisting with routine tasks, their ability to contribute to novel, expert-level mathematical discovery is less understood. We present a collection of case studies demonstrating how researchers have successfully collaborated with advanced AI models, specifically Google's Gemini-based models (in particular Gemini Deep Think and its advanced variants), to solve open problems, refute conjectures, and generate new proofs across diverse areas in theoretical computer science, as well as other areas such as economics, optimization, and physics. Based on these experiences, we extract common techniques for effective human-AI collaboration in theoretical research, such as iterative refinement, problem decomposition, and cross-disciplinary knowledge transfer. While the majority of our results stem from this interactive, conversational methodology, we also highlight specific instances that push beyond standard chat interfaces. These include deploying the model as a rigorous adversarial reviewer to detect subtle flaws in existing proofs, and embedding it within a \"neuro-symbolic\" loop that autonomously writes and executes code to verify complex derivations. Together, these examples highlight the potential of AI not just as a tool for automation, but as a versatile, genuine partner in the creative process of scientific discovery.",
      "authors": [
        "David P. Woodruff",
        "Vincent Cohen-Addad",
        "Lalit Jain",
        "Jieming Mao",
        "Song Zuo",
        "MohammadHossein Bateni",
        "Simina Branzei",
        "Michael P. Brenner",
        "Lin Chen",
        "Ying Feng",
        "Lance Fortnow",
        "Gang Fu",
        "Ziyi Guan",
        "Zahra Hadizadeh",
        "Mohammad T. Hajiaghayi",
        "Mahdi JafariRaviz",
        "Adel Javanmard",
        "Karthik C. S.",
        "Ken-ichi Kawarabayashi",
        "Ravi Kumar",
        "Silvio Lattanzi",
        "Euiwoong Lee",
        "Yi Li",
        "Ioannis Panageas",
        "Dimitris Paparas",
        "Benjamin Przybocki",
        "Bernardo Subercaseaux",
        "Ola Svensson",
        "Shayan Taherijam",
        "Xuan Wu",
        "Eylon Yogev",
        "Morteza Zadimoghaddam",
        "Samson Zhou",
        "Vahab Mirrokni"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "published": "2026-02-03T18:56:17Z",
      "updated": "2026-02-03T18:56:17Z",
      "pdf_url": "https://arxiv.org/pdf/2602.03837v1",
      "abs_url": "http://arxiv.org/abs/2602.03837v1",
      "summary": "该论文展示了Gemini模型在科学研究中的应用，并总结了人机协作的有效方法。",
      "key_contributions": [
        "展示Gemini模型在解决开放性科学问题中的能力",
        "提取有效人机协作的通用技术",
        "探索Gemini模型作为严谨评审员和代码执行器的应用"
      ],
      "methodology": "通过案例研究，分析研究人员与Gemini模型协作解决科学问题的过程，提取通用技术。",
      "tags": [
        "LLM",
        "科学研究",
        "人机协作",
        "Gemini"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 8,
      "relevance_reason": "重点在于利用LLM进行科学发现和问题解决，属于LLM Reasoning领域。",
      "analyzed_at": "2026-02-04T13:01:23.984009",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.03814v1",
      "title": "Conformal Thinking: Risk Control for Reasoning on a Compute Budget",
      "abstract": "Reasoning Large Language Models (LLMs) enable test-time scaling, with dataset-level accuracy improving as the token budget increases, motivating adaptive reasoning -- spending tokens when they improve reliability and stopping early when additional computation is unlikely to help. However, setting the token budget, as well as the threshold for adaptive reasoning, is a practical challenge that entails a fundamental risk-accuracy trade-off. We re-frame the budget setting problem as risk control, limiting the error rate while minimizing compute. Our framework introduces an upper threshold that stops reasoning when the model is confident (risking incorrect output) and a novel parametric lower threshold that preemptively stops unsolvable instances (risking premature stoppage). Given a target risk and a validation set, we use distribution-free risk control to optimally specify these stopping mechanisms. For scenarios with multiple budget controlling criteria, we incorporate an efficiency loss to select the most computationally efficient exiting mechanism. Empirical results across diverse reasoning tasks and models demonstrate the effectiveness of our risk control approach, demonstrating computational efficiency gains from the lower threshold and ensemble stopping mechanisms while adhering to the user-specified risk target.",
      "authors": [
        "Xi Wang",
        "Anushri Suresh",
        "Alvin Zhang",
        "Rishi More",
        "William Jurayj",
        "Benjamin Van Durme",
        "Mehrdad Farajtabar",
        "Daniel Khashabi",
        "Eric Nalisnick"
      ],
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "published": "2026-02-03T18:17:22Z",
      "updated": "2026-02-03T18:17:22Z",
      "pdf_url": "https://arxiv.org/pdf/2602.03814v1",
      "abs_url": "http://arxiv.org/abs/2602.03814v1",
      "summary": "提出一种在计算预算下控制LLM推理风险的框架，优化计算效率。",
      "key_contributions": [
        "提出基于风险控制的LLM推理预算设定框架",
        "引入上限和下限阈值来控制推理过程",
        "利用分布无关的风险控制方法优化阈值",
        "引入效率损失来选择最高效的退出机制"
      ],
      "methodology": "利用验证集和目标风险，通过优化上限和下限阈值，控制推理过程，并采用效率损失选择退出机制。",
      "tags": [
        "LLM",
        "Reasoning",
        "Risk Control",
        "Compute Budget",
        "Adaptive Reasoning"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 9,
      "relevance_reason": "论文直接研究LLM推理中的计算效率和风险控制，属于核心相关。",
      "analyzed_at": "2026-02-04T13:01:36.371337",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.03776v1",
      "title": "DiffLOB: Diffusion Models for Counterfactual Generation in Limit Order Books",
      "abstract": "Modern generative models for limit order books (LOBs) can reproduce realistic market dynamics, but remain fundamentally passive: they either model what typically happens without accounting for hypothetical future market conditions, or they require interaction with another agent to explore alternative outcomes. This limits their usefulness for stress testing, scenario analysis, and decision-making. We propose \\textbf{DiffLOB}, a regime-conditioned \\textbf{Diff}usion model for controllable and counterfactual generation of \\textbf{LOB} trajectories. DiffLOB explicitly conditions the generative process on future market regimes--including trend, volatility, liquidity, and order-flow imbalance, which enables the model to answer counterfactual queries of the form: ``If the future market regime were X instead of Y, how would the limit order book evolve?'' Our systematic evaluation framework for counterfactual LOB generation consists of three criteria: (1) \\textit{Controllable Realism}, measuring how well generated trajectories can reproduce marginal distributions, temporal dependence structure and regime variables; (2) \\textit{Counterfactual validity}, testing whether interventions on future regimes induce consistent changes in the generated LOB dynamics; (3) \\textit{Counterfactual usefulness}, assessing whether synthetic counterfactual trajectories improve downstream prediction of future market regimes.",
      "authors": [
        "Zhuohan Wang",
        "Carmine Ventre"
      ],
      "categories": [
        "q-fin.CP",
        "cs.AI"
      ],
      "primary_category": "q-fin.CP",
      "published": "2026-02-03T17:34:56Z",
      "updated": "2026-02-03T17:34:56Z",
      "pdf_url": "https://arxiv.org/pdf/2602.03776v1",
      "abs_url": "http://arxiv.org/abs/2602.03776v1",
      "summary": "DiffLOB提出了一种基于扩散模型的条件LOB生成方法，用于可控和反事实的轨迹生成。",
      "key_contributions": [
        "提出了DiffLOB模型，用于生成可控和反事实的LOB轨迹。",
        "引入了基于未来市场状态调节的生成过程。",
        "设计了一个系统化的反事实LOB生成评估框架。"
      ],
      "methodology": "利用扩散模型，通过调节未来市场状态（趋势、波动率等）生成LOB轨迹，并设计评估框架验证。",
      "tags": [
        "扩散模型",
        "限价订单簿",
        "反事实推理",
        "金融市场"
      ],
      "assigned_category": "agent",
      "relevance_score": 6,
      "relevance_reason": "涉及到利用生成模型探索不同情景，对agent的决策有参考价值。",
      "analyzed_at": "2026-02-04T13:01:52.142964",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.03775v1",
      "title": "An Empirical Study of Collective Behaviors and Social Dynamics in Large Language Model Agents",
      "abstract": "Large Language Models (LLMs) increasingly mediate our social, cultural, and political interactions. While they can simulate some aspects of human behavior and decision-making, it is still underexplored whether repeated interactions with other agents amplify their biases or lead to exclusionary behaviors. To this end, we study Chirper.ai-an LLM-driven social media platform-analyzing 7M posts and interactions among 32K LLM agents over a year. We start with homophily and social influence among LLMs, learning that similar to humans', their social networks exhibit these fundamental phenomena. Next, we study the toxic language of LLMs, its linguistic features, and their interaction patterns, finding that LLMs show different structural patterns in toxic posting than humans. After studying the ideological leaning in LLMs posts, and the polarization in their community, we focus on how to prevent their potential harmful activities. We present a simple yet effective method, called Chain of Social Thought (CoST), that reminds LLM agents to avoid harmful posting.",
      "authors": [
        "Farnoosh Hashemi",
        "Michael W. Macy"
      ],
      "categories": [
        "cs.SI",
        "cs.AI"
      ],
      "primary_category": "cs.SI",
      "published": "2026-02-03T17:34:32Z",
      "updated": "2026-02-03T17:34:32Z",
      "pdf_url": "https://arxiv.org/pdf/2602.03775v1",
      "abs_url": "http://arxiv.org/abs/2602.03775v1",
      "summary": "研究LLM驱动的社交平台中智能体的行为、偏见和有害活动，并提出CoST方法缓解。",
      "key_contributions": [
        "分析LLM智能体在社交平台中的同质性和社会影响",
        "研究LLM智能体的毒性语言和互动模式",
        "提出CoST方法降低LLM智能体的有害行为"
      ],
      "methodology": "分析Chirper.ai平台7M帖子和32K智能体的互动，采用统计分析和文本分析方法，并设计CoST干预实验。",
      "tags": [
        "LLM",
        "Social Media",
        "Agent",
        "Bias",
        "Toxicity",
        "CoST"
      ],
      "assigned_category": "agent",
      "relevance_score": 9,
      "relevance_reason": "核心研究LLM作为智能体在社交环境中的行为模式和潜在危害。",
      "analyzed_at": "2026-02-04T13:01:57.381781",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.03772v1",
      "title": "UniGeM: Unifying Data Mixing and Selection via Geometric Exploration and Mining",
      "abstract": "The scaling of Large Language Models (LLMs) is increasingly limited by data quality. Most methods handle data mixing and sample selection separately, which can break the structure in code corpora. We introduce \\textbf{UniGeM}, a framework that unifies mixing and selection by treating data curation as a \\textit{manifold approximation} problem without training proxy models or relying on external reference datasets. UniGeM operates hierarchically: \\textbf{Macro-Exploration} learns mixing weights with stability-based clustering; \\textbf{Micro-Mining} filters high-quality instances by their geometric distribution to ensure logical consistency. Validated by training 8B and 16B MoE models on 100B tokens, UniGeM achieves \\textbf{2.0$\\times$ data efficiency} over a random baseline and further improves overall performance compared to SOTA methods in reasoning-heavy evaluations and multilingual generalization.",
      "authors": [
        "Changhao Wang",
        "Yunfei Yu",
        "Xinhao Yao",
        "Jiaolong Yang",
        "Riccardo Cantoro",
        "Chaobo Li",
        "Qing Cui",
        "Jun Zhou"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "published": "2026-02-03T17:32:56Z",
      "updated": "2026-02-03T17:32:56Z",
      "pdf_url": "https://arxiv.org/pdf/2602.03772v1",
      "abs_url": "http://arxiv.org/abs/2602.03772v1",
      "summary": "UniGeM通过几何探索统一数据混合和选择，提高LLM训练的数据效率。",
      "key_contributions": [
        "提出UniGeM框架，统一数据混合和选择",
        "通过几何分布过滤高质量实例，保证逻辑一致性",
        "无需训练代理模型或依赖外部数据集"
      ],
      "methodology": "基于流形近似的数据质量优化，分层进行宏观探索和微观挖掘。",
      "tags": [
        "数据混合",
        "数据选择",
        "流形学习",
        "LLM训练"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 7,
      "relevance_reason": "提升LLM推理能力的有效数据选择方法，高度相关。",
      "analyzed_at": "2026-02-04T13:02:07.054670",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.03719v1",
      "title": "Training Multi-Turn Search Agent via Contrastive Dynamic Branch Sampling",
      "abstract": "Agentic reinforcement learning has enabled large language models to perform complex multi-turn planning and tool use. However, learning in long-horizon settings remains challenging due to sparse, trajectory-level outcome rewards. While prior tree-based methods attempt to mitigate this issue, they often suffer from high variance and computational inefficiency. Through empirical analysis of search agents, We identify a common pattern: performance diverges mainly due to decisions near the tail. Motivated by this observation, we propose Branching Relative Policy Optimization (BranPO), a value-free method that provides step-level contrastive supervision without dense rewards. BranPO truncates trajectories near the tail and resamples alternative continuations to construct contrastive suffixes over shared prefixes, reducing credit ambiguity in long-horizon rollouts. To further boost efficiency and stabilize training, we introduce difficulty-aware branch sampling to adapt branching frequency across tasks, and redundant step masking to suppress uninformative actions. Extensive experiments on various question answering benchmarks demonstrate that BranPO consistently outperforms strong baselines, achieving significant accuracy gains on long-horizon tasks without increasing the overall training budget. Our code is available at \\href{https://github.com/YubaoZhao/BranPO}{code}.",
      "authors": [
        "Yubao Zhao",
        "Weiquan Huang",
        "Sudong Wang",
        "Ruochen Zhao",
        "Chen Chen",
        "Yao Shu",
        "Chengwei Qin"
      ],
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "published": "2026-02-03T16:43:09Z",
      "updated": "2026-02-03T16:43:09Z",
      "pdf_url": "https://arxiv.org/pdf/2602.03719v1",
      "abs_url": "http://arxiv.org/abs/2602.03719v1",
      "summary": "BranPO通过对比动态分支抽样优化多轮搜索Agent，提升长程任务性能。",
      "key_contributions": [
        "提出了Branching Relative Policy Optimization (BranPO)方法",
        "引入难度感知分支抽样和冗余步骤屏蔽",
        "在多种问答benchmark上验证了BranPO的有效性"
      ],
      "methodology": "通过截断尾部轨迹并重采样构建对比后缀，在共享前缀上进行对比监督，减少长程rollout的信用分配模糊性。",
      "tags": [
        "Agent",
        "Reinforcement Learning",
        "Contrastive Learning",
        "Multi-turn Planning"
      ],
      "assigned_category": "agent",
      "relevance_score": 9,
      "relevance_reason": "论文核心关注多轮Agent的训练和优化，属于agent研究的关键问题。",
      "analyzed_at": "2026-02-04T13:02:20.450026",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.03681v1",
      "title": "Neural Attention Search Linear: Towards Adaptive Token-Level Hybrid Attention Models",
      "abstract": "The quadratic computational complexity of softmax transformers has become a bottleneck in long-context scenarios. In contrast, linear attention model families provide a promising direction towards a more efficient sequential model. These linear attention models compress past KV values into a single hidden state, thereby efficiently reducing complexity during both training and inference. However, their expressivity remains limited by the size of their hidden state. Previous work proposed interleaving softmax and linear attention layers to reduce computational complexity while preserving expressivity. Nevertheless, the efficiency of these models remains bottlenecked by their softmax attention layers. In this paper, we propose Neural Attention Search Linear (NAtS-L), a framework that applies both linear attention and softmax attention operations within the same layer on different tokens. NAtS-L automatically determines whether a token can be handled by a linear attention model, i.e., tokens that have only short-term impact and can be encoded into fixed-size hidden states, or require softmax attention, i.e., tokens that contain information related to long-term retrieval and need to be preserved for future queries. By searching for optimal Gated DeltaNet and softmax attention combinations across tokens, we show that NAtS-L provides a strong yet efficient token-level hybrid architecture.",
      "authors": [
        "Difan Deng",
        "Andreas Bentzen Winje",
        "Lukas Fehring",
        "Marius Lindauer"
      ],
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "published": "2026-02-03T16:02:50Z",
      "updated": "2026-02-03T16:02:50Z",
      "pdf_url": "https://arxiv.org/pdf/2602.03681v1",
      "abs_url": "http://arxiv.org/abs/2602.03681v1",
      "summary": "NAtS-L提出了一种token级别的混合注意力机制，自动选择线性或softmax注意力。",
      "key_contributions": [
        "提出NAtS-L框架，实现token级别的混合注意力。",
        "自动搜索最优的Gated DeltaNet和softmax注意力组合。",
        "在长文本场景下，平衡了效率和表达能力。"
      ],
      "methodology": "通过搜索算法，针对不同token动态选择线性注意力（Gated DeltaNet）或softmax注意力，实现混合注意力机制。",
      "tags": [
        "Transformer",
        "线性注意力",
        "Softmax注意力",
        "混合注意力",
        "长文本"
      ],
      "assigned_category": "memory",
      "relevance_score": 7,
      "relevance_reason": "涉及长文本处理和信息压缩，与 memory 和 retrieval 有一定相关性。",
      "analyzed_at": "2026-02-04T13:02:40.255557",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.03647v1",
      "title": "Search-R2: Enhancing Search-Integrated Reasoning via Actor-Refiner Collaboration",
      "abstract": "Search-integrated reasoning enables language agents to transcend static parametric knowledge by actively querying external sources. However, training these agents via reinforcement learning is hindered by the multi-scale credit assignment problem: existing methods typically rely on sparse, trajectory-level rewards that fail to distinguish between high-quality reasoning and fortuitous guesses, leading to redundant or misleading search behaviors. To address this, we propose Search-R2, a novel Actor-Refiner collaboration framework that enhances reasoning through targeted intervention, with both components jointly optimized during training. Our approach decomposes the generation process into an Actor, which produces initial reasoning trajectories, and a Meta-Refiner, which selectively diagnoses and repairs flawed steps via a 'cut-and-regenerate' mechanism. To provide fine-grained supervision, we introduce a hybrid reward design that couples outcome correctness with a dense process reward quantifying the information density of retrieved evidence. Theoretically, we formalize the Actor-Refiner interaction as a smoothed mixture policy, proving that selective correction yields strict performance gains over strong baselines. Extensive experiments across various general and multi-hop QA datasets demonstrate that Search-R2 consistently outperforms strong RAG and RL-based baselines across model scales, achieving superior reasoning accuracy with minimal overhead.",
      "authors": [
        "Bowei He",
        "Minda Hu",
        "Zenan Xu",
        "Hongru Wang",
        "Licheng Zong",
        "Yankai Chen",
        "Chen Ma",
        "Xue Liu",
        "Pluto Zhou",
        "Irwin King"
      ],
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "published": "2026-02-03T15:32:09Z",
      "updated": "2026-02-03T15:32:09Z",
      "pdf_url": "https://arxiv.org/pdf/2602.03647v1",
      "abs_url": "http://arxiv.org/abs/2602.03647v1",
      "summary": "Search-R2通过Actor-Refiner协作，结合混合奖励，提升了搜索集成推理的性能。",
      "key_contributions": [
        "提出Actor-Refiner协作框架，增强搜索集成推理。",
        "设计混合奖励，提供细粒度监督。",
        "证明选择性纠正策略的性能优势。"
      ],
      "methodology": "将生成过程分解为Actor和Meta-Refiner，Meta-Refiner选择性诊断并修复错误步骤，结合outcome和process reward进行训练。",
      "tags": [
        "搜索集成推理",
        "强化学习",
        "Actor-Refiner",
        "奖励设计"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 9,
      "relevance_reason": "论文核心在于提升LLM的推理能力，使用了创新的框架和奖励机制。",
      "analyzed_at": "2026-02-04T13:02:47.296432",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.03635v1",
      "title": "TRE: Encouraging Exploration in the Trust Region",
      "abstract": "Entropy regularization is a standard technique in reinforcement learning (RL) to enhance exploration, yet it yields negligible effects or even degrades performance in Large Language Models (LLMs). We attribute this failure to the cumulative tail risk inherent to LLMs with massive vocabularies and long generation horizons. In such environments, standard global entropy maximization indiscriminately dilutes probability mass into the vast tail of invalid tokens rather than focusing on plausible candidates, thereby disrupting coherent reasoning. To address this, we propose Trust Region Entropy (TRE), a method that encourages exploration strictly within the model's trust region. Extensive experiments across mathematical reasoning (MATH), combinatorial search (Countdown), and preference alignment (HH) tasks demonstrate that TRE consistently outperforms vanilla PPO, standard entropy regularization, and other exploration baselines. Our code is available at https://github.com/WhyChaos/TRE-Encouraging-Exploration-in-the-Trust-Region.",
      "authors": [
        "Chao Huang",
        "Yujing Lu",
        "Quangang Li",
        "Shenghe Wang",
        "Yan Wang",
        "Yueyang Zhang",
        "Long Xia",
        "Jiashu Zhao",
        "Zhiyuan Sun",
        "Daiting Shi",
        "Tingwen Liu"
      ],
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "published": "2026-02-03T15:21:49Z",
      "updated": "2026-02-03T15:21:49Z",
      "pdf_url": "https://arxiv.org/pdf/2602.03635v1",
      "abs_url": "http://arxiv.org/abs/2602.03635v1",
      "summary": "论文提出了一种Trust Region Entropy（TRE）方法，提升LLM在强化学习中的探索能力。",
      "key_contributions": [
        "发现了标准熵正则化在LLM中失效的原因是累积尾部风险",
        "提出了TRE方法，在模型信任区域内鼓励探索",
        "实验证明TRE在数学推理、组合搜索和偏好对齐任务中优于其他基线方法"
      ],
      "methodology": "提出TRE方法，限制探索范围在模型的信任区域内，避免无效token的影响。使用PPO算法进行训练和评估。",
      "tags": [
        "强化学习",
        "探索",
        "熵正则化",
        "LLM",
        "信任区域"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 9,
      "relevance_reason": "论文解决LLM推理过程中的探索问题，聚焦于提高推理能力。",
      "analyzed_at": "2026-02-04T13:02:53.101819",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.03613v1",
      "title": "Simulation-Based Inference via Regression Projection and Batched Discrepancies",
      "abstract": "We analyze a lightweight simulation-based inference method that infers simulator parameters using only a regression-based projection of the observed data. After fitting a surrogate linear regression once, the procedure simulates small batches at the proposed parameter values and assigns kernel weights based on the resulting batch-residual discrepancy, producing a self-normalized pseudo-posterior that is simple, parallelizable, and requires access only to the fitted regression coefficients rather than raw observations. We formalize the construction as an importance-sampling approximation to a population target that averages over simulator randomness, prove consistency as the number of parameter draws grows, and establish stability in estimating the surrogate regression from finite samples. We then characterize the asymptotic concentration as the batch size increases and the bandwidth shrinks, showing that the pseudo-posterior concentrates on an identified set determined by the chosen projection, thereby clarifying when the method yields point versus set identification. Experiments on a tractable nonlinear model and on a cosmological calibration task using the DREAMS simulation suite illustrate the computational advantages of regression-based projections and the identifiability limitations arising from low-information summaries.",
      "authors": [
        "Arya Farahi",
        "Jonah Rose",
        "Paul Torrey"
      ],
      "categories": [
        "stat.ME",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "stat.ME",
      "published": "2026-02-03T15:07:40Z",
      "updated": "2026-02-03T15:07:40Z",
      "pdf_url": "https://arxiv.org/pdf/2602.03613v1",
      "abs_url": "http://arxiv.org/abs/2602.03613v1",
      "summary": "提出一种基于回归投影和批量差异的模拟推断方法，加速参数推断并分析其局限性。",
      "key_contributions": [
        "提出基于回归投影的轻量级模拟推断方法",
        "证明该方法的一致性和稳定性",
        "分析了该方法在点识别和集合识别方面的渐近性质"
      ],
      "methodology": "通过回归拟合代理模型，模拟小批量数据，并基于残差计算权重，构建自归一化伪后验。",
      "tags": [
        "simulation-based inference",
        "regression",
        "importance sampling",
        "parameter inference"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 6,
      "relevance_reason": "涉及模型推理和参数估计，有一定的相关性。",
      "analyzed_at": "2026-02-04T13:03:01.999534",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.03560v1",
      "title": "HySparse: A Hybrid Sparse Attention Architecture with Oracle Token Selection and KV Cache Sharing",
      "abstract": "This work introduces Hybrid Sparse Attention (HySparse), a new architecture that interleaves each full attention layer with several sparse attention layers. While conceptually simple, HySparse strategically derives each sparse layer's token selection and KV caches directly from the preceding full attention layer. This architecture resolves two fundamental limitations of prior sparse attention methods. First, conventional approaches typically rely on additional proxies to predict token importance, introducing extra complexity and potentially suboptimal performance. In contrast, HySparse uses the full attention layer as a precise oracle to identify important tokens. Second, existing sparse attention designs often reduce computation without saving KV cache. HySparse enables sparse attention layers to reuse the full attention KV cache, thereby reducing both computation and memory. We evaluate HySparse on both 7B dense and 80B MoE models. Across all settings, HySparse consistently outperforms both full attention and hybrid SWA baselines. Notably, in the 80B MoE model with 49 total layers, only 5 layers employ full attention, yet HySparse achieves substantial performance gains while reducing KV cache storage by nearly 10x.",
      "authors": [
        "Yizhao Gao",
        "Jianyu Wei",
        "Qihao Zhang",
        "Yu Cheng",
        "Shimao Chen",
        "Zhengju Tang",
        "Zihan Jiang",
        "Yifan Song",
        "Hailin Zhang",
        "Liang Zhao",
        "Bo Yang",
        "Gang Wang",
        "Shijie Cao",
        "Fuli Luo"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "published": "2026-02-03T14:05:57Z",
      "updated": "2026-02-03T14:05:57Z",
      "pdf_url": "https://arxiv.org/pdf/2602.03560v1",
      "abs_url": "http://arxiv.org/abs/2602.03560v1",
      "summary": "HySparse通过全注意力层引导稀疏注意力，有效减少计算和内存开销并提升性能。",
      "key_contributions": [
        "提出HySparse架构，交错全注意力和稀疏注意力层",
        "使用全注意力层作为oracle进行token选择",
        "稀疏注意力层复用全注意力层的KV cache"
      ],
      "methodology": "将全注意力层与稀疏注意力层交错，利用全注意力层指导稀疏注意力层的token选择和KV cache共享。",
      "tags": [
        "Sparse Attention",
        "KV Cache",
        "Large Language Models",
        "Efficiency"
      ],
      "assigned_category": "memory",
      "relevance_score": 7,
      "relevance_reason": "降低KV cache内存占用，提升模型效率，与memory相关。",
      "analyzed_at": "2026-02-04T13:03:17.462458",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.03545v1",
      "title": "Persona Generators: Generating Diverse Synthetic Personas at Scale",
      "abstract": "Evaluating AI systems that interact with humans requires understanding their behavior across diverse user populations, but collecting representative human data is often expensive or infeasible, particularly for novel technologies or hypothetical future scenarios. Recent work in Generative Agent-Based Modeling has shown that large language models can simulate human-like synthetic personas with high fidelity, accurately reproducing the beliefs and behaviors of specific individuals. However, most approaches require detailed data about target populations and often prioritize density matching (replicating what is most probable) rather than support coverage (spanning what is possible), leaving long-tail behaviors underexplored. We introduce Persona Generators, functions that can produce diverse synthetic populations tailored to arbitrary contexts. We apply an iterative improvement loop based on AlphaEvolve, using large language models as mutation operators to refine our Persona Generator code over hundreds of iterations. The optimization process produces lightweight Persona Generators that can automatically expand small descriptions into populations of diverse synthetic personas that maximize coverage of opinions and preferences along relevant diversity axes. We demonstrate that evolved generators substantially outperform existing baselines across six diversity metrics on held-out contexts, producing populations that span rare trait combinations difficult to achieve in standard LLM outputs.",
      "authors": [
        "Davide Paglieri",
        "Logan Cross",
        "William A. Cunningham",
        "Joel Z. Leibo",
        "Alexander Sasha Vezhnevets"
      ],
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "published": "2026-02-03T13:59:03Z",
      "updated": "2026-02-03T13:59:03Z",
      "pdf_url": "https://arxiv.org/pdf/2602.03545v1",
      "abs_url": "http://arxiv.org/abs/2602.03545v1",
      "summary": "提出Persona Generators，用于生成多样化、大规模的合成角色，提升AI系统评估的覆盖度。",
      "key_contributions": [
        "提出 Persona Generators，一种自动生成多样化合成角色的函数。",
        "使用基于AlphaEvolve的迭代改进循环，优化Persona Generator代码。",
        "证明了该方法在多样性指标上优于现有基线，并能产生稀有特征组合。"
      ],
      "methodology": "使用AlphaEvolve迭代改进循环，利用大型语言模型作为变异算子，优化Persona Generator代码，从而产生多样化角色。",
      "tags": [
        "AI Agents",
        "Generative Models",
        "Persona Generation",
        "Large Language Models"
      ],
      "assigned_category": "agent",
      "relevance_score": 9,
      "relevance_reason": "直接研究AI Agent领域中角色生成与多样性的问题，并提出创新方法。",
      "analyzed_at": "2026-02-04T13:03:25.511196",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.03542v1",
      "title": "Can Large Language Models Generalize Procedures Across Representations?",
      "abstract": "Large language models (LLMs) are trained and tested extensively on symbolic representations such as code and graphs, yet real-world user tasks are often specified in natural language. To what extent can LLMs generalize across these representations? Here, we approach this question by studying isomorphic tasks involving procedures represented in code, graphs, and natural language (e.g., scheduling steps in planning). We find that training LLMs with popular post-training methods on graphs or code data alone does not reliably generalize to corresponding natural language tasks, while training solely on natural language can lead to inefficient performance gains. To address this gap, we propose a two-stage data curriculum that first trains on symbolic, then natural language data. The curriculum substantially improves model performance across model families and tasks. Remarkably, a 1.5B Qwen model trained by our method can closely match zero-shot GPT-4o in naturalistic planning. Finally, our analysis suggests that successful cross-representation generalization can be interpreted as a form of generative analogy, which our curriculum effectively encourages.",
      "authors": [
        "Fangru Lin",
        "Valentin Hofmann",
        "Xingchen Wan",
        "Weixing Wang",
        "Zifeng Ding",
        "Anthony G. Cohn",
        "Janet B. Pierrehumbert"
      ],
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "published": "2026-02-03T13:56:54Z",
      "updated": "2026-02-03T13:56:54Z",
      "pdf_url": "https://arxiv.org/pdf/2602.03542v1",
      "abs_url": "http://arxiv.org/abs/2602.03542v1",
      "summary": "研究LLM在代码、图和自然语言等表示之间的泛化能力，并提出一种两阶段数据课程。",
      "key_contributions": [
        "揭示了LLM在不同表示形式之间泛化的局限性",
        "提出了一种有效的两阶段数据课程训练方法",
        "证明了该方法显著提升了跨表示的泛化能力"
      ],
      "methodology": "通过对比不同训练策略下LLM在同构任务上的表现，验证两阶段数据课程的有效性。",
      "tags": [
        "LLM",
        "Generalization",
        "Representation Learning",
        "Data Curriculum",
        "Planning"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 8,
      "relevance_reason": "研究LLM在不同表示形式下的推理和泛化能力，高度相关。",
      "analyzed_at": "2026-02-04T13:03:27.671297",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.03541v1",
      "title": "Group Selection as a Safeguard Against AI Substitution",
      "abstract": "Reliance on generative AI can reduce cultural variance and diversity, especially in creative work. This reduction in variance has already led to problems in model performance, including model collapse and hallucination. In this paper, we examine the long-term consequences of AI use for human cultural evolution and the conditions under which widespread AI use may lead to \"cultural collapse\", a process in which reliance on AI-generated content reduces human variation and innovation and slows cumulative cultural evolution. Using an agent-based model and evolutionary game theory, we compare two types of AI use: complement and substitute. AI-complement users seek suggestions and guidance while remaining the main producers of the final output, whereas AI-substitute users provide minimal input, and rely on AI to produce most of the output. We then study how these use strategies compete and spread under evolutionary dynamics. We find that AI-substitute users prevail under individual-level selection despite the stronger reduction in cultural variance. By contrast, AI-complement users can benefit their groups by maintaining the variance needed for exploration, and can therefore be favored under cultural group selection when group boundaries are strong. Overall, our findings shed light on the long-term, population-level effects of AI adoption and inform policy and organizational strategies to mitigate these risks.",
      "authors": [
        "Qiankun Zhong",
        "Thomas F. Eisenmann",
        "Julian Garcia",
        "Iyad Rahwan"
      ],
      "categories": [
        "cs.AI",
        "econ.TH"
      ],
      "primary_category": "cs.AI",
      "published": "2026-02-03T13:56:47Z",
      "updated": "2026-02-03T13:56:47Z",
      "pdf_url": "https://arxiv.org/pdf/2602.03541v1",
      "abs_url": "http://arxiv.org/abs/2602.03541v1",
      "summary": "AI替代使用降低文化多样性，威胁人类文化演进；群体选择可促进AI辅助使用，维持文化创新。",
      "key_contributions": [
        "揭示AI使用对文化演进的长期影响",
        "提出“文化崩溃”概念并分析其成因",
        "比较AI替代和辅助两种使用策略",
        "研究群体选择在文化传承中的作用"
      ],
      "methodology": "采用Agent-Based模型和演化博弈论，模拟不同AI使用策略的竞争和传播，分析其对文化多样性和演进的影响。",
      "tags": [
        "AI替代",
        "文化演进",
        "群体选择",
        "Agent-Based模型"
      ],
      "assigned_category": "agent",
      "relevance_score": 7,
      "relevance_reason": "研究了AI agent的使用策略对群体的影响，涉及agent的关键特性。",
      "analyzed_at": "2026-02-04T13:03:30.408791",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.03537v1",
      "title": "MatGPTQ: Accurate and Efficient Post-Training Matryoshka Quantization",
      "abstract": "Matryoshka Quantization (MatQuant) is a recent quantization approach showing that a single integer-quantized model can be served across multiple precisions, by slicing the most significant bits (MSB) at inference time. This enables a single checkpoint to cover a wide range of memory and latency budgets, but renders quantization much more challenging. In particular, the initial MatQuant relies on expensive quantization-aware training (QAT) variants, rather than fast one-shot post training quantization (PTQ), and lacks open-source and kernel support. We address all of these limitations by introducing Post-Training Matryoshka Quantization (MatGPTQ), a new PTQ pipeline that produces a single parent model jointly optimized for multiple target precisions in one-shot, based on a small calibration set. MatGPTQ casts Matryoshka quantization as a multi-precision objective with bit-slicing and cross-bit error compensation, resulting in an algorithm that produces a multi-bit-width, \"sliceable\" model in a single pass. We also incorporate a new budget-aware search for heterogeneous per-layer bit-witdhs and provide efficient kernels that implement slicing and mixed-precision execution. Across standard LLMs and benchmarks, MatGPTQ preserves high-bit accuracy while substantially improving performance at low-bit-witdh settings. Overall, we establish a new state of the art for Matryoshka-style post-training quantization and make single-checkpoint, multi-precision deployment open and practical. Code is available at https://github.com/IST-DASLab/MatGPTQ.",
      "authors": [
        "Maximilian Kleinegger",
        "Elvir Crnčević",
        "Dan Alistarh"
      ],
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "published": "2026-02-03T13:52:18Z",
      "updated": "2026-02-03T13:52:18Z",
      "pdf_url": "https://arxiv.org/pdf/2602.03537v1",
      "abs_url": "http://arxiv.org/abs/2602.03537v1",
      "summary": "MatGPTQ提出了一种高效的后训练Matryoshka量化方法，实现了单模型多精度部署。",
      "key_contributions": [
        "提出MatGPTQ：一种新的后训练Matryoshka量化流程",
        "引入跨位误差补偿，优化多精度目标",
        "开发了高效的切片和混合精度执行内核"
      ],
      "methodology": "将Matryoshka量化转化为多精度目标，通过位切片和跨位误差补偿实现单次优化，并进行预算感知的异构位宽搜索。",
      "tags": [
        "量化",
        "后训练量化",
        "模型压缩",
        "多精度"
      ],
      "assigned_category": "memory",
      "relevance_score": 7,
      "relevance_reason": "涉及模型压缩，可以减小模型体积，从而优化memory",
      "analyzed_at": "2026-02-04T13:03:32.631196",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.03493v1",
      "title": "Least but not Last: Fine-tuning Intermediate Principal Components for Better Performance-Forgetting Trade-Offs",
      "abstract": "Low-Rank Adaptation (LoRA) methods have emerged as crucial techniques for adapting large pre-trained models to downstream tasks under computational and memory constraints. However, they face a fundamental challenge in balancing task-specific performance gains against catastrophic forgetting of pre-trained knowledge, where existing methods provide inconsistent recommendations. This paper presents a comprehensive analysis of the performance-forgetting trade-offs inherent in low-rank adaptation using principal components as initialization. Our investigation reveals that fine-tuning intermediate components leads to better balance and show more robustness to high learning rates than first (PiSSA) and last (MiLoRA) components in existing work. Building on these findings, we provide a practical approach for initialization of LoRA that offers superior trade-offs. We demonstrate in a thorough empirical study on a variety of computer vision and NLP tasks that our approach improves accuracy and reduces forgetting, also in continual learning scenarios.",
      "authors": [
        "Alessio Quercia",
        "Arya Bangun",
        "Ira Assent",
        "Hanno Scharr"
      ],
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "published": "2026-02-03T13:09:29Z",
      "updated": "2026-02-03T13:09:29Z",
      "pdf_url": "https://arxiv.org/pdf/2602.03493v1",
      "abs_url": "http://arxiv.org/abs/2602.03493v1",
      "summary": "通过微调中间主成分，LoRA方法在性能和遗忘之间实现了更好的权衡。",
      "key_contributions": [
        "分析了LoRA中性能-遗忘的权衡问题",
        "提出了一种基于中间主成分的LoRA初始化方法",
        "经验证明该方法在多个任务上提高了精度并减少了遗忘"
      ],
      "methodology": "通过分析主成分初始化LoRA，发现微调中间成分效果更好，并提出了一种更优的初始化方法，并在实验中验证其有效性。",
      "tags": [
        "LoRA",
        "低秩适配",
        "灾难性遗忘",
        "微调",
        "主成分分析",
        "持续学习"
      ],
      "assigned_category": "memory",
      "relevance_score": 6,
      "relevance_reason": "虽然不是直接memory，但涉及知识的遗忘与保持，具有一定关联。",
      "analyzed_at": "2026-02-04T13:03:43.468780",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.03491v1",
      "title": "Decoupling Skeleton and Flesh: Efficient Multimodal Table Reasoning with Disentangled Alignment and Structure-aware Guidance",
      "abstract": "Reasoning over table images remains challenging for Large Vision-Language Models (LVLMs) due to complex layouts and tightly coupled structure-content information. Existing solutions often depend on expensive supervised training, reinforcement learning, or external tools, limiting efficiency and scalability. This work addresses a key question: how to adapt LVLMs to table reasoning with minimal annotation and no external tools? Specifically, we first introduce DiSCo, a Disentangled Structure-Content alignment framework that explicitly separates structural abstraction from semantic grounding during multimodal alignment, efficiently adapting LVLMs to tables structures. Building on DiSCo, we further present Table-GLS, a Global-to-Local Structure-guided reasoning framework that performs table reasoning via structured exploration and evidence-grounded inference. Extensive experiments across diverse benchmarks demonstrate that our framework efficiently enhances LVLM's table understanding and reasoning capabilities, particularly generalizing to unseen table structures.",
      "authors": [
        "Yingjie Zhu",
        "Xuefeng Bai",
        "Kehai Chen",
        "Yang Xiang",
        "Youcheng Pan",
        "Xiaoqiang Zhou",
        "Min Zhang"
      ],
      "categories": [
        "cs.CV",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "published": "2026-02-03T13:08:31Z",
      "updated": "2026-02-03T13:08:31Z",
      "pdf_url": "https://arxiv.org/pdf/2602.03491v1",
      "abs_url": "http://arxiv.org/abs/2602.03491v1",
      "summary": "提出DisCo和Table-GLS框架，解耦表格结构和内容，提升LVLM在表格推理上的效率和泛化性。",
      "key_contributions": [
        "提出DisCo框架，解耦结构和内容。",
        "提出Table-GLS框架，进行结构引导的推理。",
        "实验证明框架有效提升LVLM表格理解和推理能力。"
      ],
      "methodology": "DiSCo分离结构抽象和语义对齐，Table-GLS进行结构探索和证据推理，利用全局到局部结构引导LVLM进行表格推理。",
      "tags": [
        "LVLM",
        "表格推理",
        "解耦",
        "结构引导",
        "多模态"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 9,
      "relevance_reason": "论文核心在于提升LLM在表格推理方面的能力，直接解决推理问题。",
      "analyzed_at": "2026-02-04T13:03:46.037592",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.03798v1",
      "title": "FullStack-Agent: Enhancing Agentic Full-Stack Web Coding via Development-Oriented Testing and Repository Back-Translation",
      "abstract": "Assisting non-expert users to develop complex interactive websites has become a popular task for LLM-powered code agents. However, existing code agents tend to only generate frontend web pages, masking the lack of real full-stack data processing and storage with fancy visual effects. Notably, constructing production-level full-stack web applications is far more challenging than only generating frontend web pages, demanding careful control of data flow, comprehensive understanding of constantly updating packages and dependencies, and accurate localization of obscure bugs in the codebase. To address these difficulties, we introduce FullStack-Agent, a unified agent system for full-stack agentic coding that consists of three parts: (1) FullStack-Dev, a multi-agent framework with strong planning, code editing, codebase navigation, and bug localization abilities. (2) FullStack-Learn, an innovative data-scaling and self-improving method that back-translates crawled and synthesized website repositories to improve the backbone LLM of FullStack-Dev. (3) FullStack-Bench, a comprehensive benchmark that systematically tests the frontend, backend and database functionalities of the generated website. Our FullStack-Dev outperforms the previous state-of-the-art method by 8.7%, 38.2%, and 15.9% on the frontend, backend, and database test cases respectively. Additionally, FullStack-Learn raises the performance of a 30B model by 9.7%, 9.5%, and 2.8% on the three sets of test cases through self-improvement, demonstrating the effectiveness of our approach. The code is released at https://github.com/mnluzimu/FullStack-Agent.",
      "authors": [
        "Zimu Lu",
        "Houxing Ren",
        "Yunqiao Yang",
        "Ke Wang",
        "Zhuofan Zong",
        "Mingjie Zhan",
        "Hongsheng Li"
      ],
      "categories": [
        "cs.SE",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.SE",
      "published": "2026-02-03T18:01:34Z",
      "updated": "2026-02-03T18:01:34Z",
      "pdf_url": "https://arxiv.org/pdf/2602.03798v1",
      "abs_url": "http://arxiv.org/abs/2602.03798v1",
      "summary": "FullStack-Agent通过多智能体框架、回译学习和综合测试，提升全栈Web应用开发的性能。",
      "key_contributions": [
        "提出FullStack-Agent系统，包含开发、学习和测试三个模块",
        "设计FullStack-Dev多智能体框架，具备规划、编辑、导航和调试能力",
        "提出FullStack-Learn自提升方法，通过回译提升LLM性能"
      ],
      "methodology": "构建多智能体系统，结合回译技术进行数据增强，并利用全面基准测试评估模型在全栈各方面的表现。",
      "tags": [
        "AI Agent",
        "Full-Stack Development",
        "Code Generation",
        "Back-Translation",
        "Multi-Agent System"
      ],
      "assigned_category": "agent",
      "relevance_score": 9,
      "relevance_reason": "论文核心在于构建AI Agent解决全栈开发问题，涉及多智能体和工具使用。",
      "analyzed_at": "2026-02-04T17:12:26.337040",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.03792v1",
      "title": "WebSentinel: Detecting and Localizing Prompt Injection Attacks for Web Agents",
      "abstract": "Prompt injection attacks manipulate webpage content to cause web agents to execute attacker-specified tasks instead of the user's intended ones. Existing methods for detecting and localizing such attacks achieve limited effectiveness, as their underlying assumptions often do not hold in the web-agent setting. In this work, we propose WebSentinel, a two-step approach for detecting and localizing prompt injection attacks in webpages. Given a webpage, Step I extracts \\emph{segments of interest} that may be contaminated, and Step II evaluates each segment by checking its consistency with the webpage content as context. We show that WebSentinel is highly effective, substantially outperforming baseline methods across multiple datasets of both contaminated and clean webpages that we collected. Our code is available at: https://github.com/wxl-lxw/WebSentinel.",
      "authors": [
        "Xilong Wang",
        "Yinuo Liu",
        "Zhun Wang",
        "Dawn Song",
        "Neil Gong"
      ],
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "published": "2026-02-03T17:55:04Z",
      "updated": "2026-02-03T17:55:04Z",
      "pdf_url": "https://arxiv.org/pdf/2602.03792v1",
      "abs_url": "http://arxiv.org/abs/2602.03792v1",
      "summary": "WebSentinel通过两步法检测并定位网页中的提示注入攻击，优于现有方法。",
      "key_contributions": [
        "提出WebSentinel检测框架",
        "设计基于一致性检查的检测方法",
        "构建了包含污染和干净网页的数据集"
      ],
      "methodology": "提取潜在污染片段，然后基于网页上下文对每个片段进行一致性检查，识别注入攻击。",
      "tags": [
        "Prompt Injection",
        "Web Agents",
        "Security",
        "Detection"
      ],
      "assigned_category": "agent",
      "relevance_score": 9,
      "relevance_reason": "该论文直接解决了Web Agent环境下的提示注入攻击问题，属于核心研究领域。",
      "analyzed_at": "2026-02-04T17:12:30.318384",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.03787v1",
      "title": "Inference-time Unlearning Using Conformal Prediction",
      "abstract": "Machine unlearning is the process of efficiently removing specific information from a trained machine learning model without retraining from scratch. Existing unlearning methods, which often provide provable guarantees, typically involve retraining a subset of model parameters based on a forget set. While these approaches show promise in certain scenarios, their underlying assumptions are often challenged in real-world applications -- particularly when applied to generative models. Furthermore, updating parameters using these unlearning procedures often degrades the general-purpose capabilities the model acquired during pre-training. Motivated by these shortcomings, this paper considers the paradigm of inference time unlearning -- wherein, the generative model is equipped with an (approximately correct) verifier that judges whether the model's response satisfies appropriate unlearning guarantees. This paper introduces a framework that iteratively refines the quality of the generated responses using feedback from the verifier without updating the model parameters. The proposed framework leverages conformal prediction to reduce computational overhead and provide distribution-free unlearning guarantees. This paper's approach significantly outperforms existing state-of-the-art methods, reducing unlearning error by up to 93% across challenging unlearning benchmarks.",
      "authors": [
        "Somnath Basu Roy Chowdhury",
        "Rahul Kidambi",
        "Avinava Dubey",
        "David Wang",
        "Gokhan Mergen",
        "Amr Ahmed",
        "Aranyak Mehta"
      ],
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "published": "2026-02-03T17:46:50Z",
      "updated": "2026-02-03T17:46:50Z",
      "pdf_url": "https://arxiv.org/pdf/2602.03787v1",
      "abs_url": "http://arxiv.org/abs/2602.03787v1",
      "summary": "提出一种基于Conformal Prediction的推理时免训练卸载框架，提升卸载性能并提供保证。",
      "key_contributions": [
        "提出了推理时卸载框架，无需模型参数更新",
        "利用Conformal Prediction减少计算开销",
        "提供分布无关的卸载保证"
      ],
      "methodology": "使用可验证器迭代优化生成响应，基于Conformal Prediction提供反馈，实现推理时卸载。",
      "tags": [
        "Machine Unlearning",
        "Conformal Prediction",
        "Generative Models"
      ],
      "assigned_category": "memory",
      "relevance_score": 7,
      "relevance_reason": "该论文提出了新的卸载方法，对提升模型安全和隐私有一定参考意义。",
      "analyzed_at": "2026-02-04T17:12:32.363348",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.03786v1",
      "title": "AOrchestra: Automating Sub-Agent Creation for Agentic Orchestration",
      "abstract": "Language agents have shown strong promise for task automation. Realizing this promise for increasingly complex, long-horizon tasks has driven the rise of a sub-agent-as-tools paradigm for multi-turn task solving. However, existing designs still lack a dynamic abstraction view of sub-agents, thereby hurting adaptability. We address this challenge with a unified, framework-agnostic agent abstraction that models any agent as a tuple Instruction, Context, Tools, Model. This tuple acts as a compositional recipe for capabilities, enabling the system to spawn specialized executors for each task on demand. Building on this abstraction, we introduce an agentic system AOrchestra, where the central orchestrator concretizes the tuple at each step: it curates task-relevant context, selects tools and models, and delegates execution via on-the-fly automatic agent creation. Such designs enable reducing human engineering efforts, and remain framework-agnostic with plug-and-play support for diverse agents as task executors. It also enables a controllable performance-cost trade-off, allowing the system to approach Pareto-efficient. Across three challenging benchmarks (GAIA, SWE-Bench, Terminal-Bench), AOrchestra achieves 16.28% relative improvement against the strongest baseline when paired with Gemini-3-Flash. The code is available at: https://github.com/FoundationAgents/AOrchestra",
      "authors": [
        "Jianhao Ruan",
        "Zhihao Xu",
        "Yiran Peng",
        "Fashen Ren",
        "Zhaoyang Yu",
        "Xinbing Liang",
        "Jinyu Xiang",
        "Bang Liu",
        "Chenglin Wu",
        "Yuyu Luo",
        "Jiayi Zhang"
      ],
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "published": "2026-02-03T17:46:16Z",
      "updated": "2026-02-03T17:46:16Z",
      "pdf_url": "https://arxiv.org/pdf/2602.03786v1",
      "abs_url": "http://arxiv.org/abs/2602.03786v1",
      "summary": "AOrchestra通过动态创建子代理实现复杂任务的自动化，并优化性能成本。",
      "key_contributions": [
        "提出了一个框架无关的代理抽象模型(Instruction, Context, Tools, Model)",
        "实现了AOrchestra系统，支持自动子代理创建和任务委派",
        "在多个benchmark上验证了AOrchestra的有效性和性能优势"
      ],
      "methodology": "AOrchestra通过动态抽象代理模型，根据任务需求自动选择工具和模型，并实时创建子代理执行任务。",
      "tags": [
        "Agentic Orchestration",
        "Sub-agent",
        "Task Automation",
        "LLM"
      ],
      "assigned_category": "agent",
      "relevance_score": 9,
      "relevance_reason": "论文核心内容是关于AI Agent的编排和自动化，高度相关。",
      "analyzed_at": "2026-02-04T17:12:34.560381",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.03783v1",
      "title": "Efficient Estimation of Kernel Surrogate Models for Task Attribution",
      "abstract": "Modern AI agents such as large language models are trained on diverse tasks -- translation, code generation, mathematical reasoning, and text prediction -- simultaneously. A key question is to quantify how each individual training task influences performance on a target task, a problem we refer to as task attribution. The direct approach, leave-one-out retraining, measures the effect of removing each task, but is computationally infeasible at scale. An alternative approach that builds surrogate models to predict a target task's performance for any subset of training tasks has emerged in recent literature. Prior work focuses on linear surrogate models, which capture first-order relationships, but miss nonlinear interactions such as synergy, antagonism, or XOR-type effects. In this paper, we first consider a unified task weighting framework for analyzing task attribution methods, and show a new connection between linear surrogate models and influence functions through a second-order analysis. Then, we introduce kernel surrogate models, which more effectively represent second-order task interactions. To efficiently learn the kernel surrogate, we develop a gradient-based estimation procedure that leverages a first-order approximation of pretrained models; empirically, this yields accurate estimates with less than $2\\%$ relative error without repeated retraining. Experiments across multiple domains -- including math reasoning in transformers, in-context learning, and multi-objective reinforcement learning -- demonstrate the effectiveness of kernel surrogate models. They achieve a $25\\%$ higher correlation with the leave-one-out ground truth than linear surrogates and influence-function baselines. When used for downstream task selection, kernel surrogate models yield a $40\\%$ improvement in demonstration selection for in-context learning and multi-objective reinforcement learning benchmarks.",
      "authors": [
        "Zhenshuo Zhang",
        "Minxuan Duan",
        "Hongyang R. Zhang"
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "published": "2026-02-03T17:43:48Z",
      "updated": "2026-02-03T17:43:48Z",
      "pdf_url": "https://arxiv.org/pdf/2602.03783v1",
      "abs_url": "http://arxiv.org/abs/2602.03783v1",
      "summary": "提出核代理模型，用于高效准确地评估训练任务对目标任务的影响，优于线性模型。",
      "key_contributions": [
        "提出统一的任务权重框架分析任务归因方法。",
        "引入核代理模型，有效捕捉二阶任务交互。",
        "开发基于梯度的核代理模型高效估计方法。"
      ],
      "methodology": "利用预训练模型的一阶近似，通过梯度下降估计核代理模型，避免重复训练，提升效率和准确性。",
      "tags": [
        "任务归因",
        "核代理模型",
        "元学习",
        "模型评估"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 7,
      "relevance_reason": "论文探讨了模型训练中不同任务的影响，与LLM推理能力相关。",
      "analyzed_at": "2026-02-04T17:12:38.697130",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.03839v1",
      "title": "Understanding and Exploiting Weight Update Sparsity for Communication-Efficient Distributed RL",
      "abstract": "Reinforcement learning (RL) is a critical component for post-training large language models (LLMs). However, in bandwidth-constrained distributed RL, scalability is often bottlenecked by the synchronization of policy weights from trainers to inference workers, particularly over commodity networks or in decentralized settings. While recent studies suggest that RL updates modify only a small fraction of model parameters, these observations are typically based on coarse checkpoint differences. We present a systematic empirical study of weight-update sparsity at both step-level and multi-step granularities, examining its evolution across training dynamics, off-policy delay, and model scale. We find that update sparsity is consistently high, frequently exceeding 99% across practically relevant settings. Leveraging this structure, we propose PULSE (Patch Updates via Lossless Sparse Encoding), a simple yet highly efficient lossless weight synchronization method that transmits only the indices and values of modified parameters. PULSE is robust to transmission errors and avoids floating-point drift inherent in additive delta schemes. In bandwidth-constrained decentralized environments, our approach achieves over 100x (14 GB to ~108 MB) communication reduction while maintaining bit-identical training dynamics and performance compared to full weight synchronization. By exploiting this structure, PULSE enables decentralized RL training to approach centralized throughput, reducing the bandwidth required for weight synchronization from 20 Gbit/s to 0.2 Gbit/s to maintain high GPU utilization.",
      "authors": [
        "Erfan Miahi",
        "Eugene Belilovsky"
      ],
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "published": "2026-02-03T18:56:48Z",
      "updated": "2026-02-03T18:56:48Z",
      "pdf_url": "https://arxiv.org/pdf/2602.03839v1",
      "abs_url": "http://arxiv.org/abs/2602.03839v1",
      "summary": "论文提出PULSE方法，利用权重更新的稀疏性，显著降低分布式RL中的通信开销。",
      "key_contributions": [
        "系统性地研究了RL权重更新的稀疏性",
        "提出了PULSE方法，一种高效的无损权重同步方法",
        "实验证明PULSE能大幅减少通信开销，提升分布式RL性能"
      ],
      "methodology": "通过实验分析权重更新的稀疏性，并据此设计了只传输更新索引和值的PULSE算法。",
      "tags": [
        "强化学习",
        "分布式学习",
        "通信效率",
        "权重稀疏性"
      ],
      "assigned_category": "agent",
      "relevance_score": 7,
      "relevance_reason": "该论文主要解决分布式RL中的通信问题，提升agent训练效率，高度相关。",
      "analyzed_at": "2026-02-04T17:18:45.364971",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.03828v1",
      "title": "AutoFigure: Generating and Refining Publication-Ready Scientific Illustrations",
      "abstract": "High-quality scientific illustrations are crucial for effectively communicating complex scientific and technical concepts, yet their manual creation remains a well-recognized bottleneck in both academia and industry. We present FigureBench, the first large-scale benchmark for generating scientific illustrations from long-form scientific texts. It contains 3,300 high-quality scientific text-figure pairs, covering diverse text-to-illustration tasks from scientific papers, surveys, blogs, and textbooks. Moreover, we propose AutoFigure, the first agentic framework that automatically generates high-quality scientific illustrations based on long-form scientific text. Specifically, before rendering the final result, AutoFigure engages in extensive thinking, recombination, and validation to produce a layout that is both structurally sound and aesthetically refined, outputting a scientific illustration that achieves both structural completeness and aesthetic appeal. Leveraging the high-quality data from FigureBench, we conduct extensive experiments to test the performance of AutoFigure against various baseline methods. The results demonstrate that AutoFigure consistently surpasses all baseline methods, producing publication-ready scientific illustrations. The code, dataset and huggingface space are released in https://github.com/ResearAI/AutoFigure.",
      "authors": [
        "Minjun Zhu",
        "Zhen Lin",
        "Yixuan Weng",
        "Panzhong Lu",
        "Qiujie Xie",
        "Yifan Wei",
        "Sifan Liu",
        "Qiyao Sun",
        "Yue Zhang"
      ],
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.DL"
      ],
      "primary_category": "cs.AI",
      "published": "2026-02-03T18:41:43Z",
      "updated": "2026-02-03T18:41:43Z",
      "pdf_url": "https://arxiv.org/pdf/2602.03828v1",
      "abs_url": "http://arxiv.org/abs/2602.03828v1",
      "summary": "AutoFigure提出一个自动生成高质量科学插图的Agent框架，并构建了大规模基准数据集FigureBench。",
      "key_contributions": [
        "构建了大规模科学插图基准数据集FigureBench",
        "提出了Agentic框架AutoFigure，用于自动生成科学插图",
        "AutoFigure通过思考、重组和验证，生成结构完整和美观的插图"
      ],
      "methodology": "AutoFigure采用Agent框架，通过思考、重组和验证步骤，最终生成高质量科学插图。",
      "tags": [
        "科学插图生成",
        "Agentic框架",
        "基准数据集",
        "文本到图像"
      ],
      "assigned_category": "agent",
      "relevance_score": 8,
      "relevance_reason": "论文提出Agent框架解决科学插图生成问题，与Agent主题高度相关。",
      "analyzed_at": "2026-02-04T17:18:49.386507",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.03773v1",
      "title": "Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL",
      "abstract": "Large Language Models (LLMs) that can continually improve beyond their training budgets are able to solve increasingly difficult problems by adapting at test time, a property we refer to as extrapolation. However, standard reinforcement learning (RL) operates over fixed problem distributions and training budgets, which limits extrapolation amidst distribution shift at test time. To address this, we introduce RC, an iterative decoding algorithm that replaces standard autoregressive decoding during both training and inference. RC exploits an asymmetry between the response generation and summarization capabilities of LLMs to construct reasoning chains that consistently improve across iterations. Models trained to use RC can extrapolate and continually improve over reasoning horizons more than an order of magnitude longer than those seen during training. Empirically, training a 4B model with RC using a 16k-token training budget improves performance on HMMT 2025 from 40% to nearly 70% with 0.5m tokens at test time, outperforming both comparably sized models and many larger reasoning LLMs. Finally, we also show that models trained with RC can more effectively leverage existing scaffolds to further scale test-time performance, due to the improved summary-conditioned generation abilities learned through training.",
      "authors": [
        "Ian Wu",
        "Yuxiao Qu",
        "Amrith Setlur",
        "Aviral Kumar"
      ],
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "published": "2026-02-03T17:34:04Z",
      "updated": "2026-02-03T17:34:04Z",
      "pdf_url": "https://arxiv.org/pdf/2602.03773v1",
      "abs_url": "http://arxiv.org/abs/2602.03773v1",
      "summary": "RC算法通过迭代解码，利用LLM的生成和总结能力，实现推理链的持续改进，提升模型在长推理任务上的性能。",
      "key_contributions": [
        "提出了一种新的迭代解码算法RC",
        "证明RC可以提升模型在长推理任务上的外推能力",
        "验证了RC在HMMT 2025数据集上的有效性"
      ],
      "methodology": "RC算法利用LLM的response generation和summarization能力，迭代构建推理链，并通过RL训练，使模型能在更长的推理horizon上持续改进。",
      "tags": [
        "Reasoning",
        "Reinforcement Learning",
        "Large Language Models",
        "Extrapolation"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 9,
      "relevance_reason": "论文的核心在于改进LLM的推理能力，并提出了新的算法。",
      "analyzed_at": "2026-02-04T17:19:25.642264",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    }
  ],
  "fetch_time": "2026-02-04T20:32:43.098241"
}