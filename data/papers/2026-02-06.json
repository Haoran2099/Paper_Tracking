{
  "date": "2026-02-06",
  "papers": [
    {
      "arxiv_id": "2602.06043v1",
      "title": "Shared LoRA Subspaces for almost Strict Continual Learning",
      "abstract": "Adapting large pretrained models to new tasks efficiently and continually is crucial for real-world deployment but remains challenging due to catastrophic forgetting and the high cost of retraining. While parameter-efficient tuning methods like low rank adaptation (LoRA) reduce computational demands, they lack mechanisms for strict continual learning and knowledge integration, without relying on data replay, or multiple adapters. We propose Share, a novel approach to parameter efficient continual finetuning that learns and dynamically updates a single, shared low-rank subspace, enabling seamless adaptation across multiple tasks and modalities. Share constructs a foundational subspace that extracts core knowledge from past tasks and incrementally integrates new information by identifying essential subspace directions. Knowledge from each new task is incorporated into this evolving subspace, facilitating forward knowledge transfer, while minimizing catastrophic interference. This approach achieves up to 100x parameter reduction and 281x memory savings over traditional LoRA methods, maintaining performance comparable to jointly trained models. A single Share model can replace hundreds of task-specific LoRA adapters, supporting scalable, asynchronous continual learning. Experiments across image classification, natural language understanding, 3D pose estimation, and text-to-image generation validate its effectiveness, making Share a practical and scalable solution for lifelong learning in large-scale AI systems.",
      "authors": [
        "Prakhar Kaushik",
        "Ankit Vaidya",
        "Shravan Chaudhari",
        "Rama Chellappa",
        "Alan Yuille"
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "published": "2026-02-05T18:59:58Z",
      "updated": "2026-02-05T18:59:58Z",
      "pdf_url": "https://arxiv.org/pdf/2602.06043v1",
      "abs_url": "http://arxiv.org/abs/2602.06043v1",
      "summary": "Share提出一种共享LoRA子空间的方法，用于解决严格持续学习中的灾难性遗忘问题。",
      "key_contributions": [
        "提出Share方法，学习并动态更新共享低秩子空间",
        "实现了高达100倍的参数缩减和281倍的内存节省",
        "在多个任务和模态上验证了Share的有效性"
      ],
      "methodology": "Share通过构建基础子空间提取核心知识，并增量集成新信息，最小化灾难性干扰。",
      "tags": [
        "Continual Learning",
        "Parameter-Efficient Tuning",
        "Low-Rank Adaptation"
      ],
      "assigned_category": "agent_tuning",
      "relevance_score": 6,
      "relevance_reason": "LoRA和参数高效调优与Agent Tuning有一定的关联性，可用于优化Agent的表现。",
      "analyzed_at": "2026-02-06T06:48:59.823408",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.06038v1",
      "title": "CommCP: Efficient Multi-Agent Coordination via LLM-Based Communication with Conformal Prediction",
      "abstract": "To complete assignments provided by humans in natural language, robots must interpret commands, generate and answer relevant questions for scene understanding, and manipulate target objects. Real-world deployments often require multiple heterogeneous robots with different manipulation capabilities to handle different assignments cooperatively. Beyond the need for specialized manipulation skills, effective information gathering is important in completing these assignments. To address this component of the problem, we formalize the information-gathering process in a fully cooperative setting as an underexplored multi-agent multi-task Embodied Question Answering (MM-EQA) problem, which is a novel extension of canonical Embodied Question Answering (EQA), where effective communication is crucial for coordinating efforts without redundancy. To address this problem, we propose CommCP, a novel LLM-based decentralized communication framework designed for MM-EQA. Our framework employs conformal prediction to calibrate the generated messages, thereby minimizing receiver distractions and enhancing communication reliability. To evaluate our framework, we introduce an MM-EQA benchmark featuring diverse, photo-realistic household scenarios with embodied questions. Experimental results demonstrate that CommCP significantly enhances the task success rate and exploration efficiency over baselines. The experiment videos, code, and dataset are available on our project website: https://comm-cp.github.io.",
      "authors": [
        "Xiaopan Zhang",
        "Zejin Wang",
        "Zhixu Li",
        "Jianpeng Yao",
        "Jiachen Li"
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.RO",
      "published": "2026-02-05T18:59:45Z",
      "updated": "2026-02-05T18:59:45Z",
      "pdf_url": "https://arxiv.org/pdf/2602.06038v1",
      "abs_url": "http://arxiv.org/abs/2602.06038v1",
      "summary": "论文提出了CommCP框架，利用LLM和一致性预测解决多智能体多任务具身问答中的通信协作问题。",
      "key_contributions": [
        "提出了多智能体多任务具身问答 (MM-EQA) 问题",
        "设计了基于LLM和一致性预测的去中心化通信框架CommCP",
        "构建了包含真实家庭场景的MM-EQA基准数据集"
      ],
      "methodology": "使用LLM生成消息，通过一致性预测校准消息，减少接收者干扰，提高通信可靠性，提升任务成功率和探索效率。",
      "tags": [
        "多智能体",
        "LLM",
        "具身问答",
        "通信",
        "一致性预测"
      ],
      "assigned_category": "agent",
      "relevance_score": 9,
      "relevance_reason": "该论文的核心在于多智能体协作，通信机制是关键，完全符合AI Agents的定义。",
      "analyzed_at": "2026-02-06T06:49:15.955498",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.06036v1",
      "title": "DFlash: Block Diffusion for Flash Speculative Decoding",
      "abstract": "Autoregressive large language models (LLMs) deliver strong performance but require inherently sequential decoding, leading to high inference latency and poor GPU utilization. Speculative decoding mitigates this bottleneck by using a fast draft model whose outputs are verified in parallel by the target LLM; however, existing methods still rely on autoregressive drafting, which remains sequential and limits practical speedups. Diffusion LLMs offer a promising alternative by enabling parallel generation, but current diffusion models typically underperform compared with autoregressive models. In this paper, we introduce DFlash, a speculative decoding framework that employs a lightweight block diffusion model for parallel drafting. By generating draft tokens in a single forward pass and conditioning the draft model on context features extracted from the target model, DFlash enables efficient drafting with high-quality outputs and higher acceptance rates. Experiments show that DFlash achieves over 6x lossless acceleration across a range of models and tasks, delivering up to 2.5x higher speedup than the state-of-the-art speculative decoding method EAGLE-3.",
      "authors": [
        "Jian Chen",
        "Yesheng Liang",
        "Zhijian Liu"
      ],
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "published": "2026-02-05T18:59:30Z",
      "updated": "2026-02-05T18:59:30Z",
      "pdf_url": "https://arxiv.org/pdf/2602.06036v1",
      "abs_url": "http://arxiv.org/abs/2602.06036v1",
      "summary": "DFlash提出了一种基于扩散模型的推测解码框架，显著加速LLM的推理过程。",
      "key_contributions": [
        "提出DFlash框架，利用扩散模型并行生成草稿token",
        "将目标模型上下文特征融入草稿模型，提高草稿质量",
        "实验证明DFlash在多种任务上实现了显著的加速效果"
      ],
      "methodology": "利用轻量级块扩散模型并行生成草稿token，并使用目标模型的上下文特征指导生成，实现高效的推测解码。",
      "tags": [
        "LLM",
        "推测解码",
        "扩散模型",
        "模型加速",
        "并行计算"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 9,
      "relevance_reason": "论文直接解决了LLM推理速度慢的核心问题，提出创新性的加速方法。",
      "analyzed_at": "2026-02-06T06:49:26.404085",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.06035v1",
      "title": "InterPrior: Scaling Generative Control for Physics-Based Human-Object Interactions",
      "abstract": "Humans rarely plan whole-body interactions with objects at the level of explicit whole-body movements. High-level intentions, such as affordance, define the goal, while coordinated balance, contact, and manipulation can emerge naturally from underlying physical and motor priors. Scaling such priors is key to enabling humanoids to compose and generalize loco-manipulation skills across diverse contexts while maintaining physically coherent whole-body coordination. To this end, we introduce InterPrior, a scalable framework that learns a unified generative controller through large-scale imitation pretraining and post-training by reinforcement learning. InterPrior first distills a full-reference imitation expert into a versatile, goal-conditioned variational policy that reconstructs motion from multimodal observations and high-level intent. While the distilled policy reconstructs training behaviors, it does not generalize reliably due to the vast configuration space of large-scale human-object interactions. To address this, we apply data augmentation with physical perturbations, and then perform reinforcement learning finetuning to improve competence on unseen goals and initializations. Together, these steps consolidate the reconstructed latent skills into a valid manifold, yielding a motion prior that generalizes beyond the training data, e.g., it can incorporate new behaviors such as interactions with unseen objects. We further demonstrate its effectiveness for user-interactive control and its potential for real robot deployment.",
      "authors": [
        "Sirui Xu",
        "Samuel Schulter",
        "Morteza Ziyadi",
        "Xialin He",
        "Xiaohan Fei",
        "Yu-Xiong Wang",
        "Liangyan Gui"
      ],
      "categories": [
        "cs.CV",
        "cs.GR",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "published": "2026-02-05T18:59:27Z",
      "updated": "2026-02-05T18:59:27Z",
      "pdf_url": "https://arxiv.org/pdf/2602.06035v1",
      "abs_url": "http://arxiv.org/abs/2602.06035v1",
      "summary": "InterPrior提出了一种可扩展的生成控制器，用于学习基于物理的人机交互，通过模仿学习和强化学习相结合。",
      "key_contributions": [
        "提出了InterPrior框架，用于学习人机交互的生成控制器",
        "通过大规模模仿学习和强化学习相结合，提升了控制器的泛化能力",
        "验证了该框架在用户交互控制和真实机器人部署中的潜力"
      ],
      "methodology": "首先进行模仿学习预训练，然后通过数据增强和强化学习进行微调，学习一个通用的、目标导向的变分策略。",
      "tags": [
        "机器人",
        "人机交互",
        "强化学习",
        "模仿学习"
      ],
      "assigned_category": "agent",
      "relevance_score": 7,
      "relevance_reason": "论文构建能够与环境交互的智能体，实现特定目标，符合Agent的定义。",
      "analyzed_at": "2026-02-06T06:49:28.162341",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.06034v1",
      "title": "V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval",
      "abstract": "Multimodal Large Language Models (MLLMs) have recently been applied to universal multimodal retrieval, where Chain-of-Thought (CoT) reasoning improves candidate reranking. However, existing approaches remain largely language-driven, relying on static visual encodings and lacking the ability to actively verify fine-grained visual evidence, which often leads to speculative reasoning in visually ambiguous cases. We propose V-Retrver, an evidence-driven retrieval framework that reformulates multimodal retrieval as an agentic reasoning process grounded in visual inspection. V-Retrver enables an MLLM to selectively acquire visual evidence during reasoning via external visual tools, performing a multimodal interleaved reasoning process that alternates between hypothesis generation and targeted visual verification.To train such an evidence-gathering retrieval agent, we adopt a curriculum-based learning strategy combining supervised reasoning activation, rejection-based refinement, and reinforcement learning with an evidence-aligned objective. Experiments across multiple multimodal retrieval benchmarks demonstrate consistent improvements in retrieval accuracy (with 23.0% improvements on average), perception-driven reasoning reliability, and generalization.",
      "authors": [
        "Dongyang Chen",
        "Chaoyang Wang",
        "Dezhao SU",
        "Xi Xiao",
        "Zeyu Zhang",
        "Jing Xiong",
        "Qing Li",
        "Yuzhang Shang",
        "Shichao Ka"
      ],
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2026-02-05T18:59:21Z",
      "updated": "2026-02-05T18:59:21Z",
      "pdf_url": "https://arxiv.org/pdf/2602.06034v1",
      "abs_url": "http://arxiv.org/abs/2602.06034v1",
      "summary": "V-Retrver通过视觉证据驱动的Agent推理，提升通用多模态检索的准确性和可靠性。",
      "key_contributions": [
        "提出V-Retrver框架，利用Agent进行视觉证据驱动的推理",
        "引入课程学习策略，训练证据收集检索Agent",
        "实验证明V-Retrver在多模态检索任务上的性能提升"
      ],
      "methodology": "使用MLLM作为Agent，通过视觉工具选择性获取视觉证据，进行多模态交错推理，结合课程学习进行Agent训练。",
      "tags": [
        "多模态检索",
        "视觉推理",
        "Agent",
        "MLLM",
        "证据驱动"
      ],
      "assigned_category": "multimodal",
      "relevance_score": 9,
      "relevance_reason": "论文核心内容为多模态检索，且基于MLLM的Agent进行视觉推理，高度相关。",
      "analyzed_at": "2026-02-06T06:49:30.342415",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.06030v1",
      "title": "PhysicsAgentABM: Physics-Guided Generative Agent-Based Modeling",
      "abstract": "Large language model (LLM)-based multi-agent systems enable expressive agent reasoning but are expensive to scale and poorly calibrated for timestep-aligned state-transition simulation, while classical agent-based models (ABMs) offer interpretability but struggle to integrate rich individual-level signals and non-stationary behaviors. We propose PhysicsAgentABM, which shifts inference to behaviorally coherent agent clusters: state-specialized symbolic agents encode mechanistic transition priors, a multimodal neural transition model captures temporal and interaction dynamics, and uncertainty-aware epistemic fusion yields calibrated cluster-level transition distributions. Individual agents then stochastically realize transitions under local constraints, decoupling population inference from entity-level variability. We further introduce ANCHOR, an LLM agent-driven clustering strategy based on cross-contextual behavioral responses and a novel contrastive loss, reducing LLM calls by up to 6-8 times. Experiments across public health, finance, and social sciences show consistent gains in event-time accuracy and calibration over mechanistic, neural, and LLM baselines. By re-architecting generative ABM around population-level inference with uncertainty-aware neuro-symbolic fusion, PhysicsAgentABM establishes a new paradigm for scalable and calibrated simulation with LLMs.",
      "authors": [
        "Kavana Venkatesh",
        "Yinhan He",
        "Jundong Li",
        "Jiaming Cui"
      ],
      "categories": [
        "cs.MA",
        "cs.LG"
      ],
      "primary_category": "cs.MA",
      "published": "2026-02-05T18:59:01Z",
      "updated": "2026-02-05T18:59:01Z",
      "pdf_url": "https://arxiv.org/pdf/2602.06030v1",
      "abs_url": "http://arxiv.org/abs/2602.06030v1",
      "summary": "PhysicsAgentABM通过神经符号融合实现可扩展和校准的生成式Agent建模。",
      "key_contributions": [
        "提出PhysicsAgentABM框架，融合物理机制和LLM",
        "引入ANCHOR聚类策略，降低LLM调用次数",
        "在多个领域验证了模型的准确性和校准性"
      ],
      "methodology": "利用状态专业化符号代理编码先验，神经模型捕捉动态，不确定性融合生成过渡分布，实现个体行为的随机实现。",
      "tags": [
        "Agent-Based Modeling",
        "Large Language Models",
        "Neuro-Symbolic AI"
      ],
      "assigned_category": "agent",
      "relevance_score": 9,
      "relevance_reason": "论文核心在于利用LLM和物理机制构建更有效的Agent，直接研究该领域的关键问题。",
      "analyzed_at": "2026-02-06T06:49:32.138272",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.06029v1",
      "title": "Curiosity is Knowledge: Self-Consistent Learning and No-Regret Optimization with Active Inference",
      "abstract": "Active inference (AIF) unifies exploration and exploitation by minimizing the Expected Free Energy (EFE), balancing epistemic value (information gain) and pragmatic value (task performance) through a curiosity coefficient. Yet it has been unclear when this balance yields both coherent learning and efficient decision-making: insufficient curiosity can drive myopic exploitation and prevent uncertainty resolution, while excessive curiosity can induce unnecessary exploration and regret. We establish the first theoretical guarantee for EFE-minimizing agents, showing that a single requirement--sufficient curiosity--simultaneously ensures self-consistent learning (Bayesian posterior consistency) and no-regret optimization (bounded cumulative regret). Our analysis characterizes how this mechanism depends on initial uncertainty, identifiability, and objective alignment, thereby connecting AIF to classical Bayesian experimental design and Bayesian optimization within one theoretical framework. We further translate these theories into practical design guidelines for tuning the epistemic-pragmatic trade-off in hybrid learning-optimization problems, validated through real-world experiments.",
      "authors": [
        "Yingke Li",
        "Anjali Parashar",
        "Enlu Zhou",
        "Chuchu Fan"
      ],
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "published": "2026-02-05T18:58:32Z",
      "updated": "2026-02-05T18:58:32Z",
      "pdf_url": "https://arxiv.org/pdf/2602.06029v1",
      "abs_url": "http://arxiv.org/abs/2602.06029v1",
      "summary": "该论文提出了主动推理（AIF）框架下，通过“足够的好奇心”实现一致学习和无悔优化的理论保证。",
      "key_contributions": [
        "证明了“足够好奇心”同时确保自洽学习和无悔优化",
        "建立了AIF与贝叶斯实验设计和贝叶斯优化的联系",
        "为调整认知-实用权衡提供了实践设计指南"
      ],
      "methodology": "理论分析和证明，并结合现实世界的实验进行验证，将AIF框架与贝叶斯理论相结合。",
      "tags": [
        "主动推理",
        "强化学习",
        "贝叶斯优化",
        "实验设计"
      ],
      "assigned_category": "agent",
      "relevance_score": 8,
      "relevance_reason": "论文研究智能体如何通过好奇心来优化学习和决策，高度相关。",
      "analyzed_at": "2026-02-06T06:49:34.196845",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.06023v1",
      "title": "Learning Event-Based Shooter Models from Virtual Reality Experiments",
      "abstract": "Virtual reality (VR) has emerged as a powerful tool for evaluating school security measures in high-risk scenarios such as school shootings, offering experimental control and high behavioral fidelity. However, assessing new interventions in VR requires recruiting new participant cohorts for each condition, making large-scale or iterative evaluation difficult. These limitations are especially restrictive when attempting to learn effective intervention strategies, which typically require many training episodes. To address this challenge, we develop a data-driven discrete-event simulator (DES) that models shooter movement and in-region actions as stochastic processes learned from participant behavior in VR studies. We use the simulator to examine the impact of a robot-based shooter intervention strategy. Once shown to reproduce key empirical patterns, the DES enables scalable evaluation and learning of intervention strategies that are infeasible to train directly with human subjects. Overall, this work demonstrates a high-to-mid fidelity simulation workflow that provides a scalable surrogate for developing and evaluating autonomous school-security interventions.",
      "authors": [
        "Christopher A. McClurg",
        "Alan R. Wagner"
      ],
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "published": "2026-02-05T18:56:49Z",
      "updated": "2026-02-05T18:56:49Z",
      "pdf_url": "https://arxiv.org/pdf/2602.06023v1",
      "abs_url": "http://arxiv.org/abs/2602.06023v1",
      "summary": "提出一种基于VR实验数据学习射击者行为的离散事件模拟器，用于评估校园安防干预策略。",
      "key_contributions": [
        "开发了一种基于VR实验数据的射击者行为离散事件模拟器(DES)",
        "利用模拟器评估了基于机器人的射击者干预策略的效果",
        "验证了DES能够复现关键的经验模式，为大规模评估干预策略提供了一种可扩展的替代方案"
      ],
      "methodology": "从VR实验中获取射击者的运动和行为数据，将其建模为随机过程，构建离散事件模拟器。",
      "tags": [
        "Virtual Reality",
        "Discrete-Event Simulation",
        "School Security"
      ],
      "assigned_category": "agent",
      "relevance_score": 6,
      "relevance_reason": "模拟射击者行为可被视作简化智能体建模，辅助干预策略评估具有一定相关性。",
      "analyzed_at": "2026-02-06T06:49:38.862675",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.06021v1",
      "title": "Diffusion Model's Generalization Can Be Characterized by Inductive Biases toward a Data-Dependent Ridge Manifold",
      "abstract": "When a diffusion model is not memorizing the training data set, how does it generalize exactly? A quantitative understanding of the distribution it generates would be beneficial to, for example, an assessment of the model's performance for downstream applications. We thus explicitly characterize what diffusion model generates, by proposing a log-density ridge manifold and quantifying how the generated data relate to this manifold as inference dynamics progresses. More precisely, inference undergoes a reach-align-slide process centered around the ridge manifold: trajectories first reach a neighborhood of the manifold, then align as being pushed toward or away from the manifold in normal directions, and finally slide along the manifold in tangent directions. Within the scope of this general behavior, different training errors will lead to different normal and tangent motions, which can be quantified, and these detailed motions characterize when inter-mode generations emerge. More detailed understanding of training dynamics will lead to more accurate quantification of the generation inductive bias, and an example of random feature model will be considered, for which we can explicitly illustrate how diffusion model's inductive biases originate as a composition of architectural bias and training accuracy, and how they evolve with the inference dynamics. Experiments on synthetic multimodal distributions and MNIST latent diffusion support the predicted directional effects, in both low- and high-dimensions.",
      "authors": [
        "Ye He",
        "Yitong Qiu",
        "Molei Tao"
      ],
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.NA",
        "math.PR"
      ],
      "primary_category": "stat.ML",
      "published": "2026-02-05T18:55:03Z",
      "updated": "2026-02-05T18:55:03Z",
      "pdf_url": "https://arxiv.org/pdf/2602.06021v1",
      "abs_url": "http://arxiv.org/abs/2602.06021v1",
      "summary": "论文刻画了扩散模型的泛化能力，提出了基于数据依赖的脊流形，并分析了推理过程中的reach-align-slide现象。",
      "key_contributions": [
        "提出了描述扩散模型泛化能力的脊流形概念",
        "分析了推理过程中的reach-align-slide动态",
        "定量分析了训练误差对生成结果的影响"
      ],
      "methodology": "通过理论分析和实验验证，研究扩散模型在生成数据时与脊流形的关系，并分析训练动态和推理动态。",
      "tags": [
        "diffusion model",
        "generalization",
        "inductive bias",
        "ridge manifold",
        "generative model"
      ],
      "assigned_category": "multimodal",
      "relevance_score": 7,
      "relevance_reason": "扩散模型是生成模型，与多模态数据生成密切相关，但更侧重于理论分析。",
      "analyzed_at": "2026-02-06T06:49:42.669681",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.06019v1",
      "title": "Multi-Token Prediction via Self-Distillation",
      "abstract": "Existing techniques for accelerating language model inference, such as speculative decoding, require training auxiliary speculator models and building and deploying complex inference pipelines. We consider a new approach for converting a pretrained autoregressive language model from a slow single next token prediction model into a fast standalone multi-token prediction model using a simple online distillation objective. The final model retains the exact same implementation as the pretrained initial checkpoint and is deployable without the addition of any auxiliary verifier or other specialized inference code. On GSM8K, our method produces models that can decode more than $3\\times$ faster on average at $<5\\%$ drop in accuracy relative to single token decoding performance.",
      "authors": [
        "John Kirchenbauer",
        "Abhimanyu Hans",
        "Brian Bartoldson",
        "Micah Goldblum",
        "Ashwinee Panda",
        "Tom Goldstein"
      ],
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "published": "2026-02-05T18:54:48Z",
      "updated": "2026-02-05T18:54:48Z",
      "pdf_url": "https://arxiv.org/pdf/2602.06019v1",
      "abs_url": "http://arxiv.org/abs/2602.06019v1",
      "summary": "通过自蒸馏将预训练语言模型转换为快速多token预测模型，无需额外组件。",
      "key_contributions": [
        "提出了一种新的多token预测方法",
        "无需训练额外的验证模型",
        "使用在线蒸馏目标优化"
      ],
      "methodology": "利用在线蒸馏，将单token预测模型转换为多token预测模型，并保持模型结构不变。",
      "tags": [
        "语言模型",
        "多token预测",
        "自蒸馏",
        "推理加速"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 7,
      "relevance_reason": "推理加速和性能优化与LLM推理能力相关。",
      "analyzed_at": "2026-02-06T06:49:44.194974",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.06014v1",
      "title": "Optimism Stabilizes Thompson Sampling for Adaptive Inference",
      "abstract": "Thompson sampling (TS) is widely used for stochastic multi-armed bandits, yet its inferential properties under adaptive data collection are subtle. Classical asymptotic theory for sample means can fail because arm-specific sample sizes are random and coupled with the rewards through the action-selection rule. We study this phenomenon in the $K$-armed Gaussian bandit and identify \\emph{optimism} as a key mechanism for restoring \\emph{stability}, a sufficient condition for valid asymptotic inference requiring each arm's pull count to concentrate around a deterministic scale. First, we prove that variance-inflated TS \\citep{halder2025stable} is stable for any $K \\ge 2$, including the challenging regime where multiple arms are optimal. This resolves the open question raised by \\citet{halder2025stable} through extending their results from the two-armed setting to the general $K$-armed setting. Second, we analyze an alternative optimistic modification that keeps the posterior variance unchanged but adds an explicit mean bonus to posterior mean, and establish the same stability conclusion. In summary, suitably implemented optimism stabilizes Thompson sampling and enables asymptotically valid inference in multi-armed bandits, while incurring only a mild additional regret cost.",
      "authors": [
        "Shunxing Yan",
        "Han Zhong"
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC",
        "math.ST",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "published": "2026-02-05T18:52:54Z",
      "updated": "2026-02-05T18:52:54Z",
      "pdf_url": "https://arxiv.org/pdf/2602.06014v1",
      "abs_url": "http://arxiv.org/abs/2602.06014v1",
      "summary": "论文研究了 Thompson Sampling 在多臂赌博机问题中的稳定性，并提出了通过乐观机制实现稳定性的方法。",
      "key_contributions": [
        "证明了方差膨胀的 TS 在 K 臂赌博机中的稳定性",
        "分析了另一种乐观修改 TS 的方法并证明其稳定性",
        "解决了 Halder et al. (2025) 提出的关于 TS 稳定性的开放性问题"
      ],
      "methodology": "通过理论分析和数学证明，研究了两种乐观策略对 Thompson Sampling 在多臂赌博机中的稳定性影响。",
      "tags": [
        "Thompson Sampling",
        "Multi-armed Bandits",
        "Optimism",
        "Stability",
        "Adaptive Inference"
      ],
      "assigned_category": "agent_tuning",
      "relevance_score": 7,
      "relevance_reason": "研究了算法的优化，与Agent的决策和优化问题相关。",
      "analyzed_at": "2026-02-06T06:49:51.650100",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.06013v1",
      "title": "GenArena: How Can We Achieve Human-Aligned Evaluation for Visual Generation Tasks?",
      "abstract": "The rapid advancement of visual generation models has outpaced traditional evaluation approaches, necessitating the adoption of Vision-Language Models as surrogate judges. In this work, we systematically investigate the reliability of the prevailing absolute pointwise scoring standard, across a wide spectrum of visual generation tasks. Our analysis reveals that this paradigm is limited due to stochastic inconsistency and poor alignment with human perception. To resolve these limitations, we introduce GenArena, a unified evaluation framework that leverages a pairwise comparison paradigm to ensure stable and human-aligned evaluation. Crucially, our experiments uncover a transformative finding that simply adopting this pairwise protocol enables off-the-shelf open-source models to outperform top-tier proprietary models. Notably, our method boosts evaluation accuracy by over 20% and achieves a Spearman correlation of 0.86 with the authoritative LMArena leaderboard, drastically surpassing the 0.36 correlation of pointwise methods. Based on GenArena, we benchmark state-of-the-art visual generation models across diverse tasks, providing the community with a rigorous and automated evaluation standard for visual generation.",
      "authors": [
        "Ruihang Li",
        "Leigang Qu",
        "Jingxu Zhang",
        "Dongnan Gui",
        "Mengde Xu",
        "Xiaosong Zhang",
        "Han Hu",
        "Wenjie Wang",
        "Jiaqi Wang"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2026-02-05T18:52:48Z",
      "updated": "2026-02-05T18:52:48Z",
      "pdf_url": "https://arxiv.org/pdf/2602.06013v1",
      "abs_url": "http://arxiv.org/abs/2602.06013v1",
      "summary": "GenArena提出一种基于pairwise比较的视觉生成模型评估框架，提升了评估的稳定性和与人类感知的对齐。",
      "key_contributions": [
        "发现了pointwise评估方法的局限性",
        "提出了基于pairwise比较的GenArena评估框架",
        "证明了GenArena能有效提升评估准确性并与人类感知更对齐"
      ],
      "methodology": "该论文采用pairwise比较方法，通过让模型比较不同生成结果的优劣，避免绝对评分带来的偏差，从而更准确地评估视觉生成模型。",
      "tags": [
        "视觉生成",
        "模型评估",
        "pairwise比较",
        "Vision-Language Models"
      ],
      "assigned_category": "multimodal",
      "relevance_score": 9,
      "relevance_reason": "论文直接解决了视觉生成领域模型评估的关键问题，提出了新的评估方法并验证了其有效性。",
      "analyzed_at": "2026-02-06T06:49:53.406036",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05999v1",
      "title": "On Computation and Reinforcement Learning",
      "abstract": "How does the amount of compute available to a reinforcement learning (RL) policy affect its learning? Can policies using a fixed amount of parameters, still benefit from additional compute? The standard RL framework does not provide a language to answer these questions formally. Empirically, deep RL policies are often parameterized as neural networks with static architectures, conflating the amount of compute and the number of parameters. In this paper, we formalize compute bounded policies and prove that policies which use more compute can solve problems and generalize to longer-horizon tasks that are outside the scope of policies with less compute. Building on prior work in algorithmic learning and model-free planning, we propose a minimal architecture that can use a variable amount of compute. Our experiments complement our theory. On a set 31 different tasks spanning online and offline RL, we show that $(1)$ this architecture achieves stronger performance simply by using more compute, and $(2)$ stronger generalization on longer-horizon test tasks compared to standard feedforward networks or deep residual network using up to 5 times more parameters.",
      "authors": [
        "Raj Ghugare",
        "Michał Bortkiewicz",
        "Alicja Ziarko",
        "Benjamin Eysenbach"
      ],
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "published": "2026-02-05T18:45:57Z",
      "updated": "2026-02-05T18:45:57Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05999v1",
      "abs_url": "http://arxiv.org/abs/2602.05999v1",
      "summary": "研究计算资源对强化学习策略的影响，提出计算量可变的最小架构并验证其有效性。",
      "key_contributions": [
        "形式化了计算量受限的策略",
        "证明更多计算资源可解决更复杂的任务并泛化到更长周期任务",
        "提出可使用可变计算量的最小架构"
      ],
      "methodology": "理论推导结合实验验证，在31个任务上对比了不同计算量下的策略性能和泛化能力。",
      "tags": [
        "强化学习",
        "计算资源",
        "泛化",
        "架构设计"
      ],
      "assigned_category": "agent",
      "relevance_score": 7,
      "relevance_reason": "研究了计算资源对强化学习agent的影响，涉及agent的性能提升和泛化能力。",
      "analyzed_at": "2026-02-06T06:49:55.520111",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05997v1",
      "title": "Causal Inference on Stopped Random Walks in Online Advertising",
      "abstract": "We consider a causal inference problem frequently encountered in online advertising systems, where a publisher (e.g., Instagram, TikTok) interacts repeatedly with human users and advertisers by sporadically displaying to each user an advertisement selected through an auction. Each treatment corresponds to a parameter value of the advertising mechanism (e.g., auction reserve-price), and we want to estimate through experiments the corresponding long-term treatment effect (e.g., annual advertising revenue). In our setting, the treatment affects not only the instantaneous revenue from showing an ad, but also changes each user's interaction-trajectory, and each advertiser's bidding policy -- as the latter is constrained by a finite budget. In particular, each a treatment may even affect the size of the population, since users interact longer with a tolerable advertising mechanism. We drop the classical i.i.d. assumption and model the experiment measurements (e.g., advertising revenue) as a stopped random walk, and use a budget-splitting experimental design, the Anscombe Theorem, a Wald-like equation, and a Central Limit Theorem to construct confidence intervals for the long-term treatment effect.",
      "authors": [
        "Jia Yuan Yu"
      ],
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.ME"
      ],
      "primary_category": "stat.ML",
      "published": "2026-02-05T18:43:29Z",
      "updated": "2026-02-05T18:43:29Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05997v1",
      "abs_url": "http://arxiv.org/abs/2602.05997v1",
      "summary": "针对在线广告场景，提出了一种基于停止随机游走的因果推断方法，用于评估长期广告效果。",
      "key_contributions": [
        "提出将在线广告收益建模为停止随机游走",
        "结合预算分割实验设计、Anscombe定理和中心极限定理构建置信区间",
        "解决了在线广告中用户行为轨迹和广告主竞价策略变化带来的因果推断挑战"
      ],
      "methodology": "采用预算分割实验设计，利用Anscombe定理、Wald方程和中心极限定理进行因果推断。",
      "tags": [
        "因果推断",
        "在线广告",
        "随机游走",
        "实验设计"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 6,
      "relevance_reason": "涉及因果推断，但与LLM推理的具体方法关联较弱。",
      "analyzed_at": "2026-02-06T06:49:57.851196",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05993v1",
      "title": "Diamond Maps: Efficient Reward Alignment via Stochastic Flow Maps",
      "abstract": "Flow and diffusion models produce high-quality samples, but adapting them to user preferences or constraints post-training remains costly and brittle, a challenge commonly called reward alignment. We argue that efficient reward alignment should be a property of the generative model itself, not an afterthought, and redesign the model for adaptability. We propose \"Diamond Maps\", stochastic flow map models that enable efficient and accurate alignment to arbitrary rewards at inference time. Diamond Maps amortize many simulation steps into a single-step sampler, like flow maps, while preserving the stochasticity required for optimal reward alignment. This design makes search, sequential Monte Carlo, and guidance scalable by enabling efficient and consistent estimation of the value function. Our experiments show that Diamond Maps can be learned efficiently via distillation from GLASS Flows, achieve stronger reward alignment performance, and scale better than existing methods. Our results point toward a practical route to generative models that can be rapidly adapted to arbitrary preferences and constraints at inference time.",
      "authors": [
        "Peter Holderrieth",
        "Douglas Chen",
        "Luca Eyring",
        "Ishin Shah",
        "Giri Anantharaman",
        "Yutong He",
        "Zeynep Akata",
        "Tommi Jaakkola",
        "Nicholas Matthew Boffi",
        "Max Simchowitz"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "published": "2026-02-05T18:42:00Z",
      "updated": "2026-02-05T18:42:00Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05993v1",
      "abs_url": "http://arxiv.org/abs/2602.05993v1",
      "summary": "Diamond Maps通过随机流图实现高效的奖励对齐，提升生成模型适应性。",
      "key_contributions": [
        "提出Diamond Maps，一种新的随机流图模型",
        "Diamond Maps在推理时能高效对齐任意奖励",
        "Diamond Maps优于现有方法，且更具扩展性"
      ],
      "methodology": "通过从GLASS Flows中蒸馏学习Diamond Maps，利用单步采样器和价值函数估计实现奖励对齐。",
      "tags": [
        "生成模型",
        "奖励对齐",
        "流模型",
        "强化学习"
      ],
      "assigned_category": "agent_tuning",
      "relevance_score": 6,
      "relevance_reason": "涉及优化agent的奖励和偏好对齐问题，有一定的相关性。",
      "analyzed_at": "2026-02-06T06:49:59.976081",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05992v1",
      "title": "DSB: Dynamic Sliding Block Scheduling for Diffusion LLMs",
      "abstract": "Diffusion large language models (dLLMs) have emerged as a promising alternative for text generation, distinguished by their native support for parallel decoding. In practice, block inference is crucial for avoiding order misalignment in global bidirectional decoding and improving output quality. However, the widely-used fixed, predefined block (naive) schedule is agnostic to semantic difficulty, making it a suboptimal strategy for both quality and efficiency: it can force premature commitments to uncertain positions while delaying easy positions near block boundaries. In this work, we analyze the limitations of naive block scheduling and disclose the importance of dynamically adapting the schedule to semantic difficulty for reliable and efficient inference. Motivated by this, we propose Dynamic Sliding Block (DSB), a training-free block scheduling method that uses a sliding block with a dynamic size to overcome the rigidity of the naive block. To further improve efficiency, we introduce DSB Cache, a training-free KV-cache mechanism tailored to DSB. Extensive experiments across multiple models and benchmarks demonstrate that DSB, together with DSB Cache, consistently improves both generation quality and inference efficiency for dLLMs. Code is released at https://github.com/lizhuo-luo/DSB.",
      "authors": [
        "Lizhuo Luo",
        "Shenggui Li",
        "Yonggang Wen",
        "Tianwei Zhang"
      ],
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "published": "2026-02-05T18:41:38Z",
      "updated": "2026-02-05T18:41:38Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05992v1",
      "abs_url": "http://arxiv.org/abs/2602.05992v1",
      "summary": "提出动态滑动块调度DSB，优化Diffusion LLM的并行解码质量和效率，并提出DSB Cache加速。",
      "key_contributions": [
        "分析了Naive Block Scheduling的局限性",
        "提出了动态滑动块调度方法DSB",
        "提出了针对DSB的KV-cache机制DSB Cache"
      ],
      "methodology": "提出训练无关的动态块调度方法，根据语义难度调整块大小，结合KV-cache优化效率。",
      "tags": [
        "Diffusion LLM",
        "并行解码",
        "动态调度",
        "KV-Cache"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 7,
      "relevance_reason": "优化LLM的解码效率和质量，属于提高推理能力的一环。",
      "analyzed_at": "2026-02-06T06:50:01.685428",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05986v1",
      "title": "RISE-Video: Can Video Generators Decode Implicit World Rules?",
      "abstract": "While generative video models have achieved remarkable visual fidelity, their capacity to internalize and reason over implicit world rules remains a critical yet under-explored frontier. To bridge this gap, we present RISE-Video, a pioneering reasoning-oriented benchmark for Text-Image-to-Video (TI2V) synthesis that shifts the evaluative focus from surface-level aesthetics to deep cognitive reasoning. RISE-Video comprises 467 meticulously human-annotated samples spanning eight rigorous categories, providing a structured testbed for probing model intelligence across diverse dimensions, ranging from commonsense and spatial dynamics to specialized subject domains. Our framework introduces a multi-dimensional evaluation protocol consisting of four metrics: \\textit{Reasoning Alignment}, \\textit{Temporal Consistency}, \\textit{Physical Rationality}, and \\textit{Visual Quality}. To further support scalable evaluation, we propose an automated pipeline leveraging Large Multimodal Models (LMMs) to emulate human-centric assessment. Extensive experiments on 11 state-of-the-art TI2V models reveal pervasive deficiencies in simulating complex scenarios under implicit constraints, offering critical insights for the advancement of future world-simulating generative models.",
      "authors": [
        "Mingxin Liu",
        "Shuran Ma",
        "Shibei Meng",
        "Xiangyu Zhao",
        "Zicheng Zhang",
        "Shaofeng Zhang",
        "Zhihang Zhong",
        "Peixian Chen",
        "Haoyu Cao",
        "Xing Sun",
        "Haodong Duan",
        "Xue Yang"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2026-02-05T18:36:10Z",
      "updated": "2026-02-05T18:36:10Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05986v1",
      "abs_url": "http://arxiv.org/abs/2602.05986v1",
      "summary": "提出RISE-Video基准测试，评估视频生成模型在理解隐式世界规则方面的推理能力。",
      "key_contributions": [
        "提出了RISE-Video基准测试",
        "设计了多维评估协议",
        "提出了基于LMM的自动化评估流程"
      ],
      "methodology": "构建包含467个样本的基准，涵盖常识、空间动态等领域，利用LMM进行自动化评估。",
      "tags": [
        "视频生成",
        "推理",
        "基准测试",
        "多模态"
      ],
      "assigned_category": "multimodal",
      "relevance_score": 9,
      "relevance_reason": "论文核心关注多模态学习中视频生成模型的推理能力评估，与多模态领域高度相关。",
      "analyzed_at": "2026-02-06T06:50:03.204368",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05967v1",
      "title": "A Hybrid Data-Driven Algorithm for Real-Time Friction Force Estimation in Hydraulic Cylinders",
      "abstract": "Hydraulic systems are widely utilized in industrial applications due to their high force generation, precise control, and ability to function in harsh environments. Hydraulic cylinders, as actuators in these systems, apply force and position through the displacement of hydraulic fluid, but their operation is significantly influenced by friction force. Achieving precision in hydraulic cylinders requires an accurate friction model under various operating conditions. Existing analytical models, often derived from experimental tests, necessitate the identification or estimation of influencing factors but are limited in adaptability and computational efficiency. This research introduces a data-driven, hybrid algorithm based on Long Short-Term Memory (LSTM) networks and Random Forests for nonlinear friction force estimation. The algorithm effectively combines feature detection and estimation processes using training data acquired from an experimental hydraulic test setup. It achieves a consistent and stable model error of less than 10% across diverse operating conditions and external load variations, ensuring robust performance in complex situations. The computational cost of the algorithm is 1.51 milliseconds per estimation, making it suitable for real-time applications. The proposed method addresses the limitations of analytical models by delivering high precision and computational efficiency. The algorithm's performance is validated through detailed analysis and experimental results, including direct comparisons with the LuGre model. The comparison highlights that while the LuGre model offers a theoretical foundation for friction modeling, its performance is limited by its inability to dynamically adjust to varying operational conditions of the hydraulic cylinder, further emphasizing the advantages of the proposed hybrid approach in real-time applications.",
      "authors": [
        "Mohamad Amin Jamshidi",
        "Mehrbod Zarifi",
        "Zolfa Anvari",
        "Hamed Ghafarirad",
        "Mohammad Zareinejad"
      ],
      "categories": [
        "cs.LG",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "published": "2026-02-05T18:21:28Z",
      "updated": "2026-02-05T18:21:28Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05967v1",
      "abs_url": "http://arxiv.org/abs/2602.05967v1",
      "summary": "提出了一种基于LSTM和随机森林的混合数据驱动算法，用于液压缸的实时摩擦力估计。",
      "key_contributions": [
        "提出了一种混合数据驱动算法，结合LSTM和随机森林",
        "实现了低于10%的稳定模型误差，并适用于各种工况",
        "算法的计算成本为1.51毫秒，适用于实时应用"
      ],
      "methodology": "利用LSTM网络和随机森林，结合实验数据，进行特征检测和估计，构建非线性摩擦力估计模型。",
      "tags": [
        "液压系统",
        "摩擦力估计",
        "LSTM",
        "随机森林",
        "实时控制"
      ],
      "assigned_category": "agent_tuning",
      "relevance_score": 5,
      "relevance_reason": "该论文优化了液压缸控制，可被视作一种控制策略的优化，具有一定的agent tuning相关性。",
      "analyzed_at": "2026-02-06T06:50:13.553692",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05966v1",
      "title": "LSA: Localized Semantic Alignment for Enhancing Temporal Consistency in Traffic Video Generation",
      "abstract": "Controllable video generation has emerged as a versatile tool for autonomous driving, enabling realistic synthesis of traffic scenarios. However, existing methods depend on control signals at inference time to guide the generative model towards temporally consistent generation of dynamic objects, limiting their utility as scalable and generalizable data engines. In this work, we propose Localized Semantic Alignment (LSA), a simple yet effective framework for fine-tuning pre-trained video generation models. LSA enhances temporal consistency by aligning semantic features between ground-truth and generated video clips. Specifically, we compare the output of an off-the-shelf feature extraction model between the ground-truth and generated video clips localized around dynamic objects inducing a semantic feature consistency loss. We fine-tune the base model by combining this loss with the standard diffusion loss. The model fine-tuned for a single epoch with our novel loss outperforms the baselines in common video generation evaluation metrics. To further test the temporal consistency in generated videos we adapt two additional metrics from object detection task, namely mAP and mIoU. Extensive experiments on nuScenes and KITTI datasets show the effectiveness of our approach in enhancing temporal consistency in video generation without the need for external control signals during inference and any computational overheads.",
      "authors": [
        "Mirlan Karimov",
        "Teodora Spasojevic",
        "Markus Braun",
        "Julian Wiederer",
        "Vasileios Belagiannis",
        "Marc Pollefeys"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2026-02-05T18:21:02Z",
      "updated": "2026-02-05T18:21:02Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05966v1",
      "abs_url": "http://arxiv.org/abs/2602.05966v1",
      "summary": "LSA通过对齐语义特征增强交通视频生成的时间一致性，无需额外控制信号。",
      "key_contributions": [
        "提出LSA框架，用于增强视频生成的时间一致性",
        "使用语义特征一致性损失来微调预训练模型",
        "在nuScenes和KITTI数据集上验证了方法的有效性"
      ],
      "methodology": "LSA通过比较真实视频和生成视频中动态对象的语义特征，计算语义特征一致性损失，并结合扩散损失微调模型。",
      "tags": [
        "视频生成",
        "时间一致性",
        "语义对齐",
        "交通场景"
      ],
      "assigned_category": "multimodal",
      "relevance_score": 8,
      "relevance_reason": "涉及视频生成，属于多模态学习的重要应用方向，具有较高的相关性。",
      "analyzed_at": "2026-02-06T06:50:16.015208",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05965v1",
      "title": "Learning to Share: Selective Memory for Efficient Parallel Agentic Systems",
      "abstract": "Agentic systems solve complex tasks by coordinating multiple agents that iteratively reason, invoke tools, and exchange intermediate results. To improve robustness and solution quality, recent approaches deploy multiple agent teams running in parallel to explore diverse reasoning trajectories. However, parallel execution comes at a significant computational cost: when different teams independently reason about similar sub-problems or execute analogous steps, they repeatedly perform substantial overlapping computation. To address these limitations, in this paper, we propose Learning to Share (LTS), a learned shared-memory mechanism for parallel agentic frameworks that enables selective cross-team information reuse while controlling context growth. LTS introduces a global memory bank accessible to all teams and a lightweight controller that decides whether intermediate agent steps should be added to memory or not. The controller is trained using stepwise reinforcement learning with usage-aware credit assignment, allowing it to identify information that is globally useful across parallel executions. Experiments on the AssistantBench and GAIA benchmarks show that LTS significantly reduces overall runtime while matching or improving task performance compared to memory-free parallel baselines, demonstrating that learned memory admission is an effective strategy for improving the efficiency of parallel agentic systems. Project page: https://joefioresi718.github.io/LTS_webpage/",
      "authors": [
        "Joseph Fioresi",
        "Parth Parag Kulkarni",
        "Ashmal Vayani",
        "Song Wang",
        "Mubarak Shah"
      ],
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "published": "2026-02-05T18:20:21Z",
      "updated": "2026-02-05T18:20:21Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05965v1",
      "abs_url": "http://arxiv.org/abs/2602.05965v1",
      "summary": "提出了Learning to Share (LTS)，一种用于并行Agentic系统的学习型共享内存机制，提升效率。",
      "key_contributions": [
        "提出了LTS：一个学习型共享内存机制。",
        "设计了轻量级的内存控制器，决定信息是否加入内存。",
        "使用步进式强化学习训练控制器，识别全局有用信息。"
      ],
      "methodology": "引入全局内存库，通过强化学习训练控制器，选择性地将中间步骤添加到内存中，实现跨团队的信息复用。",
      "tags": [
        "agent",
        "memory",
        "parallel computing",
        "reinforcement learning"
      ],
      "assigned_category": "agent",
      "relevance_score": 9,
      "relevance_reason": "该论文聚焦于多智能体系统中的内存共享，属于agent领域的核心研究内容。",
      "analyzed_at": "2026-02-06T06:50:17.755490",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05946v1",
      "title": "$f$-GRPO and Beyond: Divergence-Based Reinforcement Learning Algorithms for General LLM Alignment",
      "abstract": "Recent research shows that Preference Alignment (PA) objectives act as divergence estimators between aligned (chosen) and unaligned (rejected) response distributions. In this work, we extend this divergence-based perspective to general alignment settings, such as reinforcement learning with verifiable rewards (RLVR), where only environmental rewards are available. Within this unified framework, we propose $f$-Group Relative Policy Optimization ($f$-GRPO), a class of on-policy reinforcement learning, and $f$-Hybrid Alignment Loss ($f$-HAL), a hybrid on/off policy objectives, for general LLM alignment based on variational representation of $f$-divergences. We provide theoretical guarantees that these classes of objectives improve the average reward after alignment. Empirically, we validate our framework on both RLVR (Math Reasoning) and PA tasks (Safety Alignment), demonstrating superior performance and flexibility compared to current methods.",
      "authors": [
        "Rajdeep Haldar",
        "Lantao Mei",
        "Guang Lin",
        "Yue Xing",
        "Qifan Song"
      ],
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "published": "2026-02-05T18:01:52Z",
      "updated": "2026-02-05T18:01:52Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05946v1",
      "abs_url": "http://arxiv.org/abs/2602.05946v1",
      "summary": "论文提出基于f散度的通用LLM对齐算法，在可验证奖励的强化学习和偏好对齐任务上表现出色。",
      "key_contributions": [
        "提出了f-GRPO和f-HAL两种新的对齐算法",
        "将偏好对齐视为分布散度的估计",
        "提供了算法的理论保证"
      ],
      "methodology": "基于f散度的变分表示，提出on-policy和混合on/off-policy的强化学习目标，优化LLM对齐。",
      "tags": [
        "LLM Alignment",
        "Reinforcement Learning",
        "f-divergence",
        "Preference Alignment"
      ],
      "assigned_category": "agent_tuning",
      "relevance_score": 8,
      "relevance_reason": "该论文探讨了通过强化学习和偏好优化来调整LLM，使其更符合人类期望。",
      "analyzed_at": "2026-02-06T06:50:19.750753",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05940v1",
      "title": "Self-Improving Multilingual Long Reasoning via Translation-Reasoning Integrated Training",
      "abstract": "Long reasoning models often struggle in multilingual settings: they tend to reason in English for non-English questions; when constrained to reasoning in the question language, accuracies drop substantially. The struggle is caused by the limited abilities for both multilingual question understanding and multilingual reasoning. To address both problems, we propose TRIT (Translation-Reasoning Integrated Training), a self-improving framework that integrates the training of translation into multilingual reasoning. Without external feedback or additional multilingual data, our method jointly enhances multilingual question understanding and response generation. On MMATH, our method outperforms multiple baselines by an average of 7 percentage points, improving both answer correctness and language consistency. Further analysis reveals that integrating translation training improves cross-lingual question alignment by over 10 percentage points and enhances translation quality for both mathematical questions and general-domain text, with gains up to 8.4 COMET points on FLORES-200.",
      "authors": [
        "Junxiao Liu",
        "Zhijun Wang",
        "Yixiao Li",
        "Zhejian Lai",
        "Liqian Huang",
        "Xin Huang",
        "Xue Han",
        "Junlan Feng",
        "Shujian Huang"
      ],
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "published": "2026-02-05T17:55:09Z",
      "updated": "2026-02-05T17:55:09Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05940v1",
      "abs_url": "http://arxiv.org/abs/2602.05940v1",
      "summary": "TRIT通过整合翻译训练提升多语言长推理能力，无需额外数据，效果显著。",
      "key_contributions": [
        "提出TRIT框架，整合翻译训练到多语言推理中",
        "提升多语言问题理解和响应生成能力",
        "在MMATH数据集上显著优于基线模型"
      ],
      "methodology": "TRIT是一种自提升框架，联合训练翻译和多语言推理，无需外部反馈或额外多语言数据。",
      "tags": [
        "多语言",
        "长推理",
        "翻译",
        "自提升",
        "联合训练"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 9,
      "relevance_reason": "论文核心关注多语言环境下的推理能力，直接解决相关问题。",
      "analyzed_at": "2026-02-06T06:50:21.189770",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05933v1",
      "title": "Approximation of Log-Partition Function in Policy Mirror Descent Induces Implicit Regularization for LLM Post-Training",
      "abstract": "Policy mirror descent (PMD) provides a principled framework for reinforcement learning (RL) by iteratively solving KL-regularized policy improvement subproblems. While this approach has been adopted in training advanced LLMs such as Kimi K1.5/K2, the ideal closed-form PMD updates require reliable partition function estimation, a significant challenge when working with limited rollouts in the vast action spaces of LLMs. We investigate a practical algorithm, termed PMD-mean, that approximates the log-partition term with the mean reward under the sampling policy and performs regression in log-policy space. Specifically, we characterize the population solution of PMD-mean and demonstrate that it implicitly optimizes mirror descent subproblems with an adaptive mixed KL--$χ^2$ regularizer. This additional $χ^2$ regularization constrains large probability changes, producing more conservative updates when expected rewards are low and enhancing robustness against finite-sample estimation errors. Experiments on math reasoning tasks show that PMD-mean achieves superior performance with improved stability and time efficiency. These findings deepen our understanding of PMD-mean and illuminate pathways toward principled improvements in RL algorithms for LLMs. Code is available at https://github.com/horizon-rl/OpenKimi.",
      "authors": [
        "Zhenghao Xu",
        "Qin Lu",
        "Changlong Yu",
        "Tuo Zhao"
      ],
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "published": "2026-02-05T17:44:28Z",
      "updated": "2026-02-05T17:44:28Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05933v1",
      "abs_url": "http://arxiv.org/abs/2602.05933v1",
      "summary": "论文分析了一种改进的策略镜像下降算法PMD-mean，并揭示了其对LLM后训练的隐式正则化作用。",
      "key_contributions": [
        "提出了PMD-mean算法，用于近似策略镜像下降中的对数配分函数。",
        "证明了PMD-mean隐式地优化了带有自适应混合KL-$χ^2$正则化的镜像下降子问题。",
        "实验证明PMD-mean在数学推理任务上表现更优，稳定性和效率更高。"
      ],
      "methodology": "通过理论分析推导PMD-mean的population solution，并结合数学推理任务实验验证其性能和稳定性。",
      "tags": [
        "策略镜像下降",
        "隐式正则化",
        "强化学习",
        "LLM后训练"
      ],
      "assigned_category": "agent_tuning",
      "relevance_score": 8,
      "relevance_reason": "论文研究了LLM后训练中策略优化问题，与agent tuning中的算法优化相关。",
      "analyzed_at": "2026-02-06T06:50:23.273212",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05932v1",
      "title": "Polyglots or Multitudes? Multilingual LLM Answers to Value-laden Multiple-Choice Questions",
      "abstract": "Multiple-Choice Questions (MCQs) are often used to assess knowledge, reasoning abilities, and even values encoded in large language models (LLMs). While the effect of multilingualism has been studied on LLM factual recall, this paper seeks to investigate the less explored question of language-induced variation in value-laden MCQ responses. Are multilingual LLMs consistent in their responses across languages, i.e. behave like theoretical polyglots, or do they answer value-laden MCQs depending on the language of the question, like a multitude of monolingual models expressing different values through a single model? We release a new corpus, the Multilingual European Value Survey (MEVS), which, unlike prior work relying on machine translation or ad hoc prompts, solely comprises human-translated survey questions aligned in 8 European languages. We administer a subset of those questions to over thirty multilingual LLMs of various sizes, manufacturers and alignment-fine-tuning status under comprehensive, controlled prompt variations including answer order, symbol type, and tail character. Our results show that while larger, instruction-tuned models display higher overall consistency, the robustness of their responses varies greatly across questions, with certain MCQs eliciting total agreement within and across models while others leave LLM answers split. Language-specific behavior seems to arise in all consistent, instruction-fine-tuned models, but only on certain questions, warranting a further study of the selective effect of preference fine-tuning.",
      "authors": [
        "Léo Labat",
        "Etienne Ollion",
        "François Yvon"
      ],
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "published": "2026-02-05T17:44:06Z",
      "updated": "2026-02-05T17:44:06Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05932v1",
      "abs_url": "http://arxiv.org/abs/2602.05932v1",
      "summary": "研究多语言LLM在价值观问题上的一致性，发现语言会影响LLM的回答。",
      "key_contributions": [
        "发布了新的多语言价值观调查数据集MEVS",
        "研究了多语言LLM在价值观问题上的语言依赖性",
        "发现instruction-tuned模型存在语言特定行为"
      ],
      "methodology": "使用MEVS数据集，对多个多语言LLM进行价值观选择题测试，控制提示语变量，分析回答一致性。",
      "tags": [
        "多语言LLM",
        "价值观",
        "语言依赖性",
        "Multiple-Choice Questions"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 8,
      "relevance_reason": "研究LLM在涉及价值观问题上的推理能力，与LLM推理领域高度相关。",
      "analyzed_at": "2026-02-06T06:50:24.933679",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05929v1",
      "title": "KV-CoRE: Benchmarking Data-Dependent Low-Rank Compressibility of KV-Caches in LLMs",
      "abstract": "Large language models rely on kv-caches to avoid redundant computation during autoregressive decoding, but as context length grows, reading and writing the cache can quickly saturate GPU memory bandwidth. Recent work has explored KV-cache compression, yet most approaches neglect the data-dependent nature of kv-caches and their variation across layers. We introduce KV-CoRE KV-cache Compressibility by Rank Evaluation), an SVD-based method for quantifying the data-dependent low-rank compressibility of kv-caches. KV-CoRE computes the optimal low-rank approximation under the Frobenius norm and, being gradient-free and incremental, enables efficient dataset-level, layer-wise evaluation. Using this method, we analyze multiple models and datasets spanning five English domains and sixteen languages, uncovering systematic patterns that link compressibility to model architecture, training data, and language coverage. As part of this analysis, we employ the Normalized Effective Rank as a metric of compressibility and show that it correlates strongly with performance degradation under compression. Our study establishes a principled evaluation framework and the first large-scale benchmark of kv-cache compressibility in LLMs, offering insights for dynamic, data-aware compression and data-centric model development.",
      "authors": [
        "Jian Chen",
        "Zhuoran Wang",
        "Jiayu Qin",
        "Ming Li",
        "Meng Wang",
        "Changyou Chen",
        "Yin Chen",
        "Qizhen Weng",
        "Yirui Liu"
      ],
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "published": "2026-02-05T17:41:57Z",
      "updated": "2026-02-05T17:41:57Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05929v1",
      "abs_url": "http://arxiv.org/abs/2602.05929v1",
      "summary": "该论文提出KV-CoRE方法评估LLM中KV-cache的数据依赖低秩可压缩性，并进行了大规模基准测试。",
      "key_contributions": [
        "提出KV-CoRE方法评估KV-cache可压缩性",
        "构建大规模KV-cache可压缩性基准测试",
        "分析模型、数据和语言与可压缩性的关系"
      ],
      "methodology": "使用基于SVD的KV-CoRE方法，通过计算Frobenius范数下的最优低秩近似，进行数据集层面和层级的评估。",
      "tags": [
        "KV-cache",
        "压缩",
        "低秩近似",
        "LLM",
        "基准测试"
      ],
      "assigned_category": "memory",
      "relevance_score": 9,
      "relevance_reason": "论文核心关注LLM的KV-cache压缩，属于LLM memory的关键问题。",
      "analyzed_at": "2026-02-06T06:50:27.000162",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05920v1",
      "title": "Quantum Reinforcement Learning with Transformers for the Capacitated Vehicle Routing Problem",
      "abstract": "This paper addresses the Capacitated Vehicle Routing Problem (CVRP) by comparing classical and quantum Reinforcement Learning (RL) approaches. An Advantage Actor-Critic (A2C) agent is implemented in classical, full quantum, and hybrid variants, integrating transformer architectures to capture the relationships between vehicles, clients, and the depot through self- and cross-attention mechanisms. The experiments focus on multi-vehicle scenarios with capacity constraints, considering 20 clients and 4 vehicles, and are conducted over ten independent runs. Performance is assessed using routing distance, route compactness, and route overlap. The results show that all three approaches are capable of learning effective routing policies. However, quantum-enhanced models outperform the classical baseline and produce more robust route organization, with the hybrid architecture achieving the best overall performance across distance, compactness, and route overlap. In addition to quantitative improvements, qualitative visualizations reveal that quantum-based models generate more structured and coherent routing solutions. These findings highlight the potential of hybrid quantum-classical reinforcement learning models for addressing complex combinatorial optimization problems such as the CVRP.",
      "authors": [
        "Eva Andrés"
      ],
      "categories": [
        "cs.AI",
        "cs.ET"
      ],
      "primary_category": "cs.AI",
      "published": "2026-02-05T17:32:14Z",
      "updated": "2026-02-05T17:32:14Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05920v1",
      "abs_url": "http://arxiv.org/abs/2602.05920v1",
      "summary": "论文比较了经典和量子强化学习解决带容量约束车辆路径问题(CVRP)，混合量子方法性能最佳。",
      "key_contributions": [
        "比较了经典、全量子和混合量子强化学习方法在CVRP上的表现",
        "将Transformer架构集成到强化学习智能体中，用于捕捉车辆、客户和车场之间的关系",
        "实验结果表明混合量子方法在CVRP上优于经典方法，并生成更鲁棒的路径"
      ],
      "methodology": "采用经典、全量子和混合量子A2C智能体，使用Transformer架构，在多车辆容量约束的CVRP问题上进行实验对比。",
      "tags": [
        "Quantum Reinforcement Learning",
        "CVRP",
        "Transformer",
        "Hybrid Quantum-Classical"
      ],
      "assigned_category": "agent",
      "relevance_score": 7,
      "relevance_reason": "论文讨论了使用强化学习解决优化问题的Agent，并对比了量子方法。",
      "analyzed_at": "2026-02-06T06:50:29.220370",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05902v1",
      "title": "Regularized Calibration with Successive Rounding for Post-Training Quantization",
      "abstract": "Large language models (LLMs) deliver robust performance across diverse applications, yet their deployment often faces challenges due to the memory and latency costs of storing and accessing billions of parameters. Post-training quantization (PTQ) enables efficient inference by mapping pretrained weights to low-bit formats without retraining, but its effectiveness depends critically on both the quantization objective and the rounding procedure used to obtain low-bit weight representations. In this work, we show that interpolating between symmetric and asymmetric calibration acts as a form of regularization that preserves the standard quadratic structure used in PTQ while providing robustness to activation mismatch. Building on this perspective, we derive a simple successive rounding procedure that naturally incorporates asymmetric calibration, as well as a bounded-search extension that allows for an explicit trade-off between quantization quality and the compute cost. Experiments across multiple LLM families, quantization bit-widths, and benchmarks demonstrate that the proposed bounded search based on a regularized asymmetric calibration objective consistently improves perplexity and accuracy over PTQ baselines, while incurring only modest and controllable additional computational cost.",
      "authors": [
        "Seohyeon Cha",
        "Huancheng Chen",
        "Dongjun Kim",
        "Haoran Zhang",
        "Kevin Chan",
        "Gustavo de Veciana",
        "Haris Vikalo"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "published": "2026-02-05T17:18:02Z",
      "updated": "2026-02-05T17:18:02Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05902v1",
      "abs_url": "http://arxiv.org/abs/2602.05902v1",
      "summary": "提出基于正则化非对称校准的PTQ方法，通过连续舍入提高LLM量化性能。",
      "key_contributions": [
        "提出了正则化非对称校准目标",
        "设计了连续舍入过程",
        "提出了有界搜索扩展以平衡质量和成本"
      ],
      "methodology": "通过正则化非对称校准和连续舍入，将预训练权重映射到低比特格式，实现高效推理。",
      "tags": [
        "量化",
        "后训练量化",
        "大语言模型",
        "模型压缩"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 8,
      "relevance_reason": "改进LLM推理效率，关注模型压缩和性能权衡。",
      "analyzed_at": "2026-02-06T06:50:32.165371",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05897v1",
      "title": "Stop Rewarding Hallucinated Steps: Faithfulness-Aware Step-Level Reinforcement Learning for Small Reasoning Models",
      "abstract": "As large language models become smaller and more efficient, small reasoning models (SRMs) are crucial for enabling chain-of-thought (CoT) reasoning in resource-constrained settings. However, they are prone to faithfulness hallucinations, especially in intermediate reasoning steps. Existing mitigation methods based on online reinforcement learning rely on outcome-based rewards or coarse-grained CoT evaluation, which can inadvertently reinforce unfaithful reasoning when the final answer is correct. To address these limitations, we propose Faithfulness-Aware Step-Level Reinforcement Learning (FaithRL), introducing step-level supervision via explicit faithfulness rewards from a process reward model, together with an implicit truncated resampling strategy that generates contrastive signals from faithful prefixes. Experiments across multiple SRMs and Open-Book QA benchmarks demonstrate that FaithRL consistently reduces hallucinations in both the CoT and final answers, leading to more faithful and reliable reasoning. Code is available at https://github.com/Easy195/FaithRL.",
      "authors": [
        "Shuo Nie",
        "Hexuan Deng",
        "Chao Wang",
        "Ruiyu Fang",
        "Xuebo Liu",
        "Shuangyong Song",
        "Yu Li",
        "Min Zhang",
        "Xuelong Li"
      ],
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "published": "2026-02-05T17:15:12Z",
      "updated": "2026-02-05T17:15:12Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05897v1",
      "abs_url": "http://arxiv.org/abs/2602.05897v1",
      "summary": "FaithRL通过引入显式可信度奖励和隐式截断重采样，提升小型推理模型CoT推理的可靠性。",
      "key_contributions": [
        "提出FaithRL，一种可信度感知的步骤级别强化学习方法",
        "引入显式可信度奖励，鼓励推理过程的忠实性",
        "采用隐式截断重采样策略，生成对比信号",
        "在Open-Book QA基准测试中验证了FaithRL的有效性"
      ],
      "methodology": "使用强化学习，结合显式可信度奖励和隐式截断重采样，训练小型推理模型，提升推理步骤的真实性。",
      "tags": [
        "强化学习",
        "链式思考",
        "可信度",
        "推理模型"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 9,
      "relevance_reason": "论文核心关注小型LLM的推理可信度问题，并提出了针对CoT推理的优化方法。",
      "analyzed_at": "2026-02-06T06:50:33.909848",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05892v1",
      "title": "ContextBench: A Benchmark for Context Retrieval in Coding Agents",
      "abstract": "LLM-based coding agents have shown strong performance on automated issue resolution benchmarks, yet existing evaluations largely focus on final task success, providing limited insight into how agents retrieve and use code context during problem solving. We introduce ContextBench, a process-oriented evaluation of context retrieval in coding agents. ContextBench consists of 1,136 issue-resolution tasks from 66 repositories across eight programming languages, each augmented with human-annotated gold contexts. We further implement an automated evaluation framework that tracks agent trajectories and measures context recall, precision, and efficiency throughout issue resolution. Using ContextBench, we evaluate four frontier LLMs and five coding agents. Our results show that sophisticated agent scaffolding yields only marginal gains in context retrieval (\"The Bitter Lesson\" of coding agents), LLMs consistently favor recall over precision, and substantial gaps exist between explored and utilized context. ContextBench augments existing end-to-end benchmarks with intermediate gold-context metrics that unbox the issue-resolution process. These contexts offer valuable intermediate signals for guiding LLM reasoning in software tasks. Data and code are available at: https://cioutn.github.io/context-bench/.",
      "authors": [
        "Han Li",
        "Letian Zhu",
        "Bohan Zhang",
        "Rili Feng",
        "Jiaming Wang",
        "Yue Pan",
        "Earl T. Barr",
        "Sarro Federica",
        "Zhaoyang Chu",
        "He Ye"
      ],
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "published": "2026-02-05T17:10:26Z",
      "updated": "2026-02-05T17:10:26Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05892v1",
      "abs_url": "http://arxiv.org/abs/2602.05892v1",
      "summary": "ContextBench基准测试用于评估代码Agent在问题解决中检索代码上下文的能力。",
      "key_contributions": [
        "提出了ContextBench基准测试，包含1136个问题解决任务。",
        "实现了自动评估框架，跟踪Agent轨迹并测量上下文召回率、精确度和效率。",
        "评估了四个前沿LLM和五个代码Agent，揭示了上下文检索的瓶颈。"
      ],
      "methodology": "构建包含人工标注黄金上下文的问题解决任务集，并开发自动评估框架，通过召回率、精确度等指标评估Agent上下文检索能力。",
      "tags": [
        "代码Agent",
        "上下文检索",
        "基准测试",
        "LLM",
        "问题解决"
      ],
      "assigned_category": "agent",
      "relevance_score": 9,
      "relevance_reason": "论文核心内容是评估和分析代码Agent的上下文检索能力，属于Agent领域关键问题。",
      "analyzed_at": "2026-02-06T06:50:35.922250",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05890v1",
      "title": "DFPO: Scaling Value Modeling via Distributional Flow towards Robust and Generalizable LLM Post-Training",
      "abstract": "Training reinforcement learning (RL) systems in real-world environments remains challenging due to noisy supervision and poor out-of-domain (OOD) generalization, especially in LLM post-training. Recent distributional RL methods improve robustness by modeling values with multiple quantile points, but they still learn each quantile independently as a scalar. This results in rough-grained value representations that lack fine-grained conditioning on state information, struggling under complex and OOD conditions. We propose DFPO (Distributional Value Flow Policy Optimization with Conditional Risk and Consistency Control), a robust distributional RL framework that models values as continuous flows across time steps. By scaling value modeling through learning of a value flow field instead of isolated quantile predictions, DFPO captures richer state information for more accurate advantage estimation. To stabilize training under noisy feedback, DFPO further integrates conditional risk control and consistency constraints along value flow trajectories. Experiments on dialogue, math reasoning, and scientific tasks show that DFPO outperforms PPO, FlowRL, and other robust baselines under noisy supervision, achieving improved training stability and generalization.",
      "authors": [
        "Dingwei Zhu",
        "Zhiheng Xi",
        "Shihan Dou",
        "Jiahan Li",
        "Chenhao Huang",
        "Junjie Ye",
        "Sixian Li",
        "Mingxu Chai",
        "Yuhui Wang",
        "Yajie Yang",
        "Ming Zhang",
        "Jiazheng Zhang",
        "Shichun Liu",
        "Caishuang Huang",
        "Yunke Zhang",
        "Yuran Wang",
        "Tao Gui",
        "Xipeng Qiu",
        "Qi Zhang",
        "Xuanjing Huang"
      ],
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "published": "2026-02-05T17:07:42Z",
      "updated": "2026-02-05T17:07:42Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05890v1",
      "abs_url": "http://arxiv.org/abs/2602.05890v1",
      "summary": "DFPO通过学习值流建模，提升LLM在噪声环境下的鲁棒性和泛化能力。",
      "key_contributions": [
        "提出DFPO框架，建模连续值流而非独立分位数",
        "引入条件风险控制和一致性约束，稳定训练",
        "实验证明DFPO在噪声环境下优于其他方法"
      ],
      "methodology": "DFPO通过学习值流场捕获更丰富的状态信息，使用条件风险控制和一致性约束稳定训练，优化策略。",
      "tags": [
        "强化学习",
        "LLM后训练",
        "分布强化学习",
        "鲁棒性",
        "泛化性"
      ],
      "assigned_category": "agent_tuning",
      "relevance_score": 8,
      "relevance_reason": "专注于LLM后训练阶段的优化，涉及agent的调优，并提升在复杂环境下的性能。",
      "analyzed_at": "2026-02-06T06:50:37.559914",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05888v1",
      "title": "Metric Hedonic Games on the Line",
      "abstract": "Hedonic games are fundamental models for investigating the formation of coalitions among a set of strategic agents, where every agent has a certain utility for every possible coalition of agents it can be part of. To avoid the intractability of defining exponentially many utilities for all possible coalitions, many variants with succinct representations of the agents' utility functions have been devised and analyzed, e.g., modified fractional hedonic games by Monaco et al. [JAAMAS 2020]. We extend this by studying a novel succinct variant that is related to modified fractional hedonic games. In our model, each agent has a fixed type-value and an agent's cost for some given coalition is based on the differences between its value and those of the other members of its coalition. This allows to model natural situations like athletes forming training groups with similar performance levels or voters that partition themselves along a political spectrum.   In particular, we investigate natural variants where an agent's cost is defined by distance thresholds, or by the maximum or average value difference to the other agents in its coalition. For these settings, we study the existence of stable coalition structures, their properties, and their quality in terms of the price of anarchy and the price of stability. Further, we investigate the impact of limiting the maximum number of coalitions. Despite the simple setting with metric distances on a line, we uncover a rich landscape of models, partially with counter-intuitive behavior. Also, our focus on both swap stability and jump stability allows us to study the influence of fixing the number and the size of the coalitions. Overall, we find that stable coalition structures always exist but that their properties and quality can vary widely.",
      "authors": [
        "Merlin de la Haye",
        "Pascal Lenzner",
        "Farehe Soheil",
        "Marcus Wunderlich"
      ],
      "categories": [
        "cs.GT",
        "cs.AI"
      ],
      "primary_category": "cs.GT",
      "published": "2026-02-05T17:05:08Z",
      "updated": "2026-02-05T17:05:08Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05888v1",
      "abs_url": "http://arxiv.org/abs/2602.05888v1",
      "summary": "研究基于距离的联盟形成博弈，分析稳定性和效率。",
      "key_contributions": [
        "提出新的基于距离的联盟形成博弈模型",
        "分析不同距离度量下的稳定联盟结构的存在性",
        "研究联盟结构的稳定性和效率（价格无政府状态和价格稳定）"
      ],
      "methodology": "理论分析，证明存在性和性质，并计算稳定性和效率的界限。",
      "tags": [
        "博弈论",
        "联盟形成",
        "稳定性",
        "价格无政府状态",
        "机制设计"
      ],
      "assigned_category": "agent",
      "relevance_score": 6,
      "relevance_reason": "该研究涉及智能体之间的策略互动和群体形成，具有一定的相关性。",
      "analyzed_at": "2026-02-06T06:50:39.327295",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05883v1",
      "title": "A Guide to Large Language Models in Modeling and Simulation: From Core Techniques to Critical Challenges",
      "abstract": "Large language models (LLMs) have rapidly become familiar tools to researchers and practitioners. Concepts such as prompting, temperature, or few-shot examples are now widely recognized, and LLMs are increasingly used in Modeling & Simulation (M&S) workflows. However, practices that appear straightforward may introduce subtle issues, unnecessary complexity, or may even lead to inferior results. Adding more data can backfire (e.g., deteriorating performance through model collapse or inadvertently wiping out existing guardrails), spending time on fine-tuning a model can be unnecessary without a prior assessment of what it already knows, setting the temperature to 0 is not sufficient to make LLMs deterministic, providing a large volume of M&S data as input can be excessive (LLMs cannot attend to everything) but naive simplifications can lose information. We aim to provide comprehensive and practical guidance on how to use LLMs, with an emphasis on M&S applications. We discuss common sources of confusion, including non-determinism, knowledge augmentation (including RAG and LoRA), decomposition of M&S data, and hyper-parameter settings. We emphasize principled design choices, diagnostic strategies, and empirical evaluation, with the goal of helping modelers make informed decisions about when, how, and whether to rely on LLMs.",
      "authors": [
        "Philippe J. Giabbanelli"
      ],
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "published": "2026-02-05T17:00:07Z",
      "updated": "2026-02-05T17:00:07Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05883v1",
      "abs_url": "http://arxiv.org/abs/2602.05883v1",
      "summary": "论文针对LLM在建模与仿真应用中的常见问题提供实用指南，强调设计选择和评估。",
      "key_contributions": [
        "LLM在M&S应用中的最佳实践指南",
        "常见问题的分析与诊断策略",
        "知识增强方法（RAG, LoRA）的应用建议"
      ],
      "methodology": "论文采用经验分析和案例研究，结合理论分析，为LLM在M&S中的应用提供指导。",
      "tags": [
        "LLM",
        "Modeling & Simulation",
        "RAG",
        "LoRA"
      ],
      "assigned_category": "memory",
      "relevance_score": 8,
      "relevance_reason": "论文重点讨论了RAG等知识增强方法，与memory类别高度相关。",
      "analyzed_at": "2026-02-06T06:50:41.203838",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05879v1",
      "title": "EuroLLM-22B: Technical Report",
      "abstract": "This report presents EuroLLM-22B, a large language model trained from scratch to support the needs of European citizens by covering all 24 official European Union languages and 11 additional languages. EuroLLM addresses the issue of European languages being underrepresented and underserved in existing open large language models. We provide a comprehensive overview of EuroLLM-22B's development, including tokenizer design, architectural specifications, data filtering, and training procedures. Across a broad set of multilingual benchmarks, EuroLLM-22B demonstrates strong performance in reasoning, instruction following, and translation, achieving results competitive with models of comparable size. To support future research, we release our base and instruction-tuned models, our multilingual web pretraining data and updated EuroBlocks instruction datasets, as well as our pre-training and evaluation codebases.",
      "authors": [
        "Miguel Moura Ramos",
        "Duarte M. Alves",
        "Hippolyte Gisserot-Boukhlef",
        "João Alves",
        "Pedro Henrique Martins",
        "Patrick Fernandes",
        "José Pombal",
        "Nuno M. Guerreiro",
        "Ricardo Rei",
        "Nicolas Boizard",
        "Amin Farajian",
        "Mateusz Klimaszewski",
        "José G. C. de Souza",
        "Barry Haddow",
        "François Yvon",
        "Pierre Colombo",
        "Alexandra Birch",
        "André F. T. Martins"
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "published": "2026-02-05T16:53:47Z",
      "updated": "2026-02-05T16:53:47Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05879v1",
      "abs_url": "http://arxiv.org/abs/2602.05879v1",
      "summary": "EuroLLM-22B是一个支持多种欧洲语言的大型语言模型，性能与同规模模型相当，并开源了数据和代码。",
      "key_contributions": [
        "训练了一个支持多种欧洲语言的22B参数LLM",
        "开源了预训练数据和指令微调数据集EuroBlocks",
        "提供了预训练和评估代码"
      ],
      "methodology": "从零开始训练，包括tokenizer设计、架构选择、数据过滤和训练流程，并在多语言基准上进行了评估。",
      "tags": [
        "LLM",
        "Multilingual",
        "European Languages",
        "Open Source"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 8,
      "relevance_reason": "论文涉及LLM的训练和评估，以及在推理任务上的性能表现，属于推理领域。",
      "analyzed_at": "2026-02-06T06:50:43.521033",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05877v1",
      "title": "Agent2Agent Threats in Safety-Critical LLM Assistants: A Human-Centric Taxonomy",
      "abstract": "The integration of Large Language Model (LLM)-based conversational agents into vehicles creates novel security challenges at the intersection of agentic AI, automotive safety, and inter-agent communication. As these intelligent assistants coordinate with external services via protocols such as Google's Agent-to-Agent (A2A), they establish attack surfaces where manipulations can propagate through natural language payloads, potentially causing severe consequences ranging from driver distraction to unauthorized vehicle control. Existing AI security frameworks, while foundational, lack the rigorous \"separation of concerns\" standard in safety-critical systems engineering by co-mingling the concepts of what is being protected (assets) with how it is attacked (attack paths). This paper addresses this methodological gap by proposing a threat modeling framework called AgentHeLLM (Agent Hazard Exploration for LLM Assistants) that formally separates asset identification from attack path analysis. We introduce a human-centric asset taxonomy derived from harm-oriented \"victim modeling\" and inspired by the Universal Declaration of Human Rights, and a formal graph-based model that distinguishes poison paths (malicious data propagation) from trigger paths (activation actions). We demonstrate the framework's practical applicability through an open-source attack path suggestion tool AgentHeLLM Attack Path Generator that automates multi-stage threat discovery using a bi-level search strategy.",
      "authors": [
        "Lukas Stappen",
        "Ahmet Erkan Turan",
        "Johann Hagerer",
        "Georg Groh"
      ],
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "published": "2026-02-05T16:53:41Z",
      "updated": "2026-02-05T16:53:41Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05877v1",
      "abs_url": "http://arxiv.org/abs/2602.05877v1",
      "summary": "提出AgentHeLLM框架，针对LLM智能助手在车辆环境中Agent间通信的安全威胁进行建模和分析。",
      "key_contributions": [
        "提出AgentHeLLM威胁建模框架，分离资产识别和攻击路径分析。",
        "构建基于人权视角的资产分类体系。",
        "开发AgentHeLLM Attack Path Generator工具，自动化多阶段威胁发现。"
      ],
      "methodology": "提出一种基于图的正式模型，区分毒化路径（恶意数据传播）和触发路径（激活动作）。",
      "tags": [
        "LLM",
        "AI Agent",
        "Security",
        "Threat Modeling",
        "Automotive"
      ],
      "assigned_category": "agent",
      "relevance_score": 9,
      "relevance_reason": "论文核心关注Agent在安全关键场景下的威胁，与AI Agent领域高度相关。",
      "analyzed_at": "2026-02-06T06:50:45.275548",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05875v1",
      "title": "Beyond Manual Planning: Seating Allocation for Large Organizations",
      "abstract": "We introduce the Hierarchical Seating Allocation Problem (HSAP) which addresses the optimal assignment of hierarchically structured organizational teams to physical seating arrangements on a floor plan. This problem is driven by the necessity for large organizations with large hierarchies to ensure that teams with close hierarchical relationships are seated in proximity to one another, such as ensuring a research group occupies a contiguous area. Currently, this problem is managed manually leading to infrequent and suboptimal replanning efforts. To alleviate this manual process, we propose an end-to-end framework to solve the HSAP. A scalable approach to calculate the distance between any pair of seats using a probabilistic road map (PRM) and rapidly-exploring random trees (RRT) which is combined with heuristic search and dynamic programming approach to solve the HSAP using integer programming. We demonstrate our approach under different sized instances by evaluating the PRM framework and subsequent allocations both quantitatively and qualitatively.",
      "authors": [
        "Anton Ipsen",
        "Michael Cashmore",
        "Kirsty Fielding",
        "Nicolas Marchesotti",
        "Parisa Zehtabi",
        "Daniele Magazzeni",
        "Manuela Veloso"
      ],
      "categories": [
        "cs.AI",
        "math.OC"
      ],
      "primary_category": "cs.AI",
      "published": "2026-02-05T16:52:44Z",
      "updated": "2026-02-05T16:52:44Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05875v1",
      "abs_url": "http://arxiv.org/abs/2602.05875v1",
      "summary": "提出层级座位分配问题(HSAP)，并提出一个端到端框架进行求解，优化大型组织座位分配。",
      "key_contributions": [
        "定义了层级座位分配问题(HSAP)",
        "提出了一个端到端的HSAP求解框架",
        "结合PRM/RRT、启发式搜索和动态规划，用整数规划求解HSAP"
      ],
      "methodology": "使用PRM和RRT计算座位距离，结合启发式搜索和动态规划，利用整数规划求解HSAP。",
      "tags": [
        "优化",
        "座位分配",
        "组织管理",
        "整数规划"
      ],
      "assigned_category": "agent",
      "relevance_score": 6,
      "relevance_reason": "论文涉及组织内部资源的优化配置，与Agent环境下的资源分配有一定的相关性。",
      "analyzed_at": "2026-02-06T06:50:47.202274",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05874v1",
      "title": "xList-Hate: A Checklist-Based Framework for Interpretable and Generalizable Hate Speech Detection",
      "abstract": "Hate speech detection is commonly framed as a direct binary classification problem despite being a composite concept defined through multiple interacting factors that vary across legal frameworks, platform policies, and annotation guidelines. As a result, supervised models often overfit dataset-specific definitions and exhibit limited robustness under domain shift and annotation noise.   We introduce xList-Hate, a diagnostic framework that decomposes hate speech detection into a checklist of explicit, concept-level questions grounded in widely shared normative criteria. Each question is independently answered by a large language model (LLM), producing a binary diagnostic representation that captures hateful content features without directly predicting the final label. These diagnostic signals are then aggregated by a lightweight, fully interpretable decision tree, yielding transparent and auditable predictions.   We evaluate it across multiple hate speech benchmarks and model families, comparing it against zero-shot LLM classification and in-domain supervised fine-tuning. While supervised methods typically maximize in-domain performance, we consistently improves cross-dataset robustness and relative performance under domain shift. In addition, qualitative analysis of disagreement cases provides evidence that the framework can be less sensitive to certain forms of annotation inconsistency and contextual ambiguity. Crucially, the approach enables fine-grained interpretability through explicit decision paths and factor-level analysis.   Our results suggest that reframing hate speech detection as a diagnostic reasoning task, rather than a monolithic classification problem, provides a robust, explainable, and extensible alternative for content moderation.",
      "authors": [
        "Adrián Girón",
        "Pablo Miralles",
        "Javier Huertas-Tato",
        "Sergio D'Antonio",
        "David Camacho"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "published": "2026-02-05T16:51:56Z",
      "updated": "2026-02-05T16:51:56Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05874v1",
      "abs_url": "http://arxiv.org/abs/2602.05874v1",
      "summary": "xList-Hate通过分解仇恨言论检测任务为多个概念性问题，提升了模型的鲁棒性和可解释性。",
      "key_contributions": [
        "提出xList-Hate框架，将仇恨言论检测分解为诊断性问题",
        "使用LLM回答诊断性问题，生成二元诊断表示",
        "通过可解释的决策树聚合诊断信号，进行预测",
        "实验证明该方法在跨数据集上具有更好的鲁棒性"
      ],
      "methodology": "使用LLM回答一系列与仇恨言论相关的概念性问题，再用决策树聚合结果进行分类。",
      "tags": [
        "仇恨言论检测",
        "可解释性",
        "鲁棒性",
        "大语言模型"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 9,
      "relevance_reason": "核心关注LLM的推理能力，将复杂分类问题分解为多个简单推理步骤。",
      "analyzed_at": "2026-02-06T06:50:49.280764",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05873v1",
      "title": "Large-scale Score-based Variational Posterior Inference for Bayesian Deep Neural Networks",
      "abstract": "Bayesian (deep) neural networks (BNN) are often more attractive than the mainstream point-estimate vanilla deep learning in various aspects including uncertainty quantification, robustness to noise, resistance to overfitting, and more. The variational inference (VI) is one of the most widely adopted approximate inference methods. Whereas the ELBO-based variational free energy method is a dominant choice in the literature, in this paper we introduce a score-based alternative for BNN variational inference. Although there have been quite a few score-based variational inference methods proposed in the community, most are not adequate for large-scale BNNs for various computational and technical reasons. We propose a novel scalable VI method where the learning objective combines the score matching loss and the proximal penalty term in iterations, which helps our method avoid the reparametrized sampling, and allows for noisy unbiased mini-batch scores through stochastic gradients. This in turn makes our method scalable to large-scale neural networks including Vision Transformers, and allows for richer variational density families. On several benchmarks including visual recognition and time-series forecasting with large-scale deep networks, we empirically show the effectiveness of our approach.",
      "authors": [
        "Minyoung Kim"
      ],
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "published": "2026-02-05T16:51:07Z",
      "updated": "2026-02-05T16:51:07Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05873v1",
      "abs_url": "http://arxiv.org/abs/2602.05873v1",
      "summary": "提出了一种可扩展的基于分数的变分贝叶斯深度神经网络后验推断方法，适用于大规模模型。",
      "key_contributions": [
        "提出了一种新的可扩展的变分推断方法",
        "结合了分数匹配损失和近端惩罚项",
        "适用于大规模神经网络，如 Vision Transformer"
      ],
      "methodology": "通过结合分数匹配损失和近端惩罚项，避免重参数化采样，并允许使用随机梯度进行噪声无偏的小批量分数计算。",
      "tags": [
        "贝叶斯神经网络",
        "变分推断",
        "深度学习",
        "大规模模型",
        "分数匹配"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 7,
      "relevance_reason": "涉及贝叶斯方法和不确定性量化，对提高模型推理的可靠性有帮助。",
      "analyzed_at": "2026-02-06T06:50:50.841248",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05863v1",
      "title": "Constrained Group Relative Policy Optimization",
      "abstract": "While Group Relative Policy Optimization (GRPO) has emerged as a scalable framework for critic-free policy learning, extending it to settings with explicit behavioral constraints remains underexplored. We introduce Constrained GRPO, a Lagrangian-based extension of GRPO for constrained policy optimization. Constraints are specified via indicator cost functions, enabling direct optimization of violation rates through a Lagrangian relaxation. We show that a naive multi-component treatment in advantage estimation can break constrained learning: mismatched component-wise standard deviations distort the relative importance of the different objective terms, which in turn corrupts the Lagrangian signal and prevents meaningful constraint enforcement. We formally derive this effect to motivate our scalarized advantage construction that preserves the intended trade-off between reward and constraint terms. Experiments in a toy gridworld confirm the predicted optimization pathology and demonstrate that scalarizing advantages restores stable constraint control. In addition, we evaluate Constrained GRPO on robotics tasks, where it improves constraint satisfaction while increasing task success, establishing a simple and effective recipe for constrained policy optimization in embodied AI domains that increasingly rely on large multimodal foundation models.",
      "authors": [
        "Roger Girgis",
        "Rodrigue de Schaetzen",
        "Luke Rowe",
        "Azalée Robitaille",
        "Christopher Pal",
        "Liam Paull"
      ],
      "categories": [
        "cs.LG",
        "cs.CL",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "published": "2026-02-05T16:44:23Z",
      "updated": "2026-02-05T16:44:23Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05863v1",
      "abs_url": "http://arxiv.org/abs/2602.05863v1",
      "summary": "提出了Constrained GRPO，一种基于拉格朗日的、带有约束的策略优化方法，并解决了优势估计中的问题。",
      "key_contributions": [
        "提出了Constrained GRPO算法",
        "解决了优势估计中多成分处理导致的问题",
        "在机器人任务上验证了算法的有效性"
      ],
      "methodology": "使用拉格朗日方法将约束加入GRPO，并提出了标量化的优势函数构造方法，保证奖励和约束项之间的平衡。",
      "tags": [
        "强化学习",
        "约束优化",
        "Lagrangian方法",
        "策略优化"
      ],
      "assigned_category": "agent_tuning",
      "relevance_score": 7,
      "relevance_reason": "研究Agent策略的优化，包含约束，属于agent tuning领域。",
      "analyzed_at": "2026-02-06T06:50:52.581424",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05862v1",
      "title": "Distribution-free two-sample testing with blurred total variation distance",
      "abstract": "Two-sample testing, where we aim to determine whether two distributions are equal or not equal based on samples from each one, is challenging if we cannot place assumptions on the properties of the two distributions. In particular, certifying equality of distributions, or even providing a tight upper bound on the total variation (TV) distance between the distributions, is impossible to achieve in a distribution-free regime. In this work, we examine the blurred TV distance, a relaxation of TV distance that enables us to perform inference without assumptions on the distributions. We provide theoretical guarantees for distribution-free upper and lower bounds on the blurred TV distance, and examine its properties in high dimensions.",
      "authors": [
        "Rohan Hore",
        "Rina Foygel Barber"
      ],
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "published": "2026-02-05T16:43:31Z",
      "updated": "2026-02-05T16:43:31Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05862v1",
      "abs_url": "http://arxiv.org/abs/2602.05862v1",
      "summary": "研究无分布假设下的双样本检验问题，并引入模糊TV距离进行推断。",
      "key_contributions": [
        "提出模糊TV距离用于无分布假设的双样本检验",
        "提供模糊TV距离上下界的理论保证",
        "研究模糊TV距离在高维空间的性质"
      ],
      "methodology": "理论分析模糊TV距离的性质，并进行理论推导，提供了上下界保证。",
      "tags": [
        "two-sample testing",
        "distribution-free",
        "total variation distance",
        "hypothesis testing"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 6,
      "relevance_reason": "推理测试中判断分布的异同，具有一定相关性。",
      "analyzed_at": "2026-02-06T06:50:54.318121",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05861v1",
      "title": "CFRecs: Counterfactual Recommendations on Real Estate User Listing Interaction Graphs",
      "abstract": "Graph-structured data is ubiquitous and powerful in representing complex relationships in many online platforms. While graph neural networks (GNNs) are widely used to learn from such data, counterfactual graph learning has emerged as a promising approach to improve model interpretability. Counterfactual explanation research focuses on identifying a counterfactual graph that is similar to the original but leads to different predictions. These explanations optimize two objectives simultaneously: the sparsity of changes in the counterfactual graph and the validity of its predictions. Building on these qualitative optimization goals, this paper introduces CFRecs, a novel framework that transforms counterfactual explanations into actionable insights. CFRecs employs a two-stage architecture consisting of a graph neural network (GNN) and a graph variational auto-encoder (Graph-VAE) to strategically propose minimal yet high-impact changes in graph structure and node attributes to drive desirable outcomes in recommender systems. We apply CFRecs to Zillow's graph-structured data to deliver actionable recommendations for both home buyers and sellers with the goal of helping them navigate the competitive housing market and achieve their homeownership goals. Experimental results on Zillow's user-listing interaction data demonstrate the effectiveness of CFRecs, which also provides a fresh perspective on recommendations using counterfactual reasoning in graphs.",
      "authors": [
        "Seyedmasoud Mousavi",
        "Ruomeng Xu",
        "Xiaojing Zhu"
      ],
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "published": "2026-02-05T16:42:51Z",
      "updated": "2026-02-05T16:42:51Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05861v1",
      "abs_url": "http://arxiv.org/abs/2602.05861v1",
      "summary": "CFRecs利用反事实图学习，为房地产用户提供可操作的推荐建议，优化用户目标。",
      "key_contributions": [
        "提出CFRecs框架，将反事实解释转化为可操作的推荐",
        "结合GNN和Graph-VAE，策略性地调整图结构和节点属性",
        "在Zillow真实数据集上验证了CFRecs的有效性"
      ],
      "methodology": "使用GNN和Graph-VAE构建两阶段架构，通过反事实推理在用户-房源交互图上进行推荐。",
      "tags": [
        "推荐系统",
        "图神经网络",
        "反事实学习",
        "房地产"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 7,
      "relevance_reason": "涉及反事实推理，并应用于实际推荐场景，有一定相关性。",
      "analyzed_at": "2026-02-06T06:50:56.003848",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05857v1",
      "title": "BABE: Biology Arena BEnchmark",
      "abstract": "The rapid evolution of large language models (LLMs) has expanded their capabilities from basic dialogue to advanced scientific reasoning. However, existing benchmarks in biology often fail to assess a critical skill required of researchers: the ability to integrate experimental results with contextual knowledge to derive meaningful conclusions. To address this gap, we introduce BABE(Biology Arena BEnchmark), a comprehensive benchmark designed to evaluate the experimental reasoning capabilities of biological AI systems. BABE is uniquely constructed from peer-reviewed research papers and real-world biological studies, ensuring that tasks reflect the complexity and interdisciplinary nature of actual scientific inquiry. BABE challenges models to perform causal reasoning and cross-scale inference. Our benchmark provides a robust framework for assessing how well AI systems can reason like practicing scientists, offering a more authentic measure of their potential to contribute to biological research.",
      "authors": [
        "Junting Zhou",
        "Jin Chen",
        "Linfeng Hao",
        "Denghui Cao",
        "Zheyu Wang",
        "Qiguang Chen",
        "Chaoyou Fu",
        "Jiaze Chen",
        "Yuchen Wu",
        "Ge Zhang",
        "Mingxuan Wang",
        "Wenhao Huang",
        "Tong Yang"
      ],
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "published": "2026-02-05T16:39:20Z",
      "updated": "2026-02-05T16:39:20Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05857v1",
      "abs_url": "http://arxiv.org/abs/2602.05857v1",
      "summary": "BABE是一个生物学领域的新基准，旨在评估LLM的实验推理能力。",
      "key_contributions": [
        "提出了BABE基准，用于评估生物学AI系统的实验推理能力",
        "BABE基于同行评审论文和真实生物学研究",
        "BABE挑战模型进行因果推理和跨尺度推断"
      ],
      "methodology": "BABE构建自同行评审的科研论文，要求模型整合实验结果和背景知识，进行因果推理和跨尺度推断。",
      "tags": [
        "benchmark",
        "biology",
        "reasoning",
        "LLM"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 9,
      "relevance_reason": "该论文直接研究LLM的推理能力在生物学领域中的应用，具有核心相关性。",
      "analyzed_at": "2026-02-06T06:50:57.654142",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05853v1",
      "title": "RRAttention: Dynamic Block Sparse Attention via Per-Head Round-Robin Shifts for Long-Context Inference",
      "abstract": "The quadratic complexity of attention mechanisms poses a critical bottleneck for large language models processing long contexts. While dynamic sparse attention methods offer input-adaptive efficiency, they face fundamental trade-offs: requiring preprocessing, lacking global evaluation, violating query independence, or incurring high computational overhead. We present RRAttention, a novel dynamic sparse attention method that simultaneously achieves all desirable properties through a head \\underline{r}ound-\\underline{r}obin (RR) sampling strategy. By rotating query sampling positions across attention heads within each stride, RRAttention maintains query independence while enabling efficient global pattern discovery with stride-level aggregation. Our method reduces complexity from $O(L^2)$ to $O(L^2/S^2)$ and employs adaptive Top-$τ$ selection for optimal sparsity. Extensive experiments on natural language understanding (HELMET) and multimodal video comprehension (Video-MME) demonstrate that RRAttention recovers over 99\\% of full attention performance while computing only half of the attention blocks, achieving 2.4$\\times$ speedup at 128K context length and outperforming existing dynamic sparse attention methods.",
      "authors": [
        "Siran Liu",
        "Guoxia Wang",
        "Sa Wang",
        "Jinle Zeng",
        "HaoYang Xie",
        "Siyu Lou",
        "JiaBin Yang",
        "DianHai Yu",
        "Haifeng Wang",
        "Chao Yang"
      ],
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "published": "2026-02-05T16:37:41Z",
      "updated": "2026-02-05T16:37:41Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05853v1",
      "abs_url": "http://arxiv.org/abs/2602.05853v1",
      "summary": "RRAttention提出了一种新颖的动态稀疏注意力机制，通过head round-robin采样实现高效长文本推理。",
      "key_contributions": [
        "提出RRAttention，一种新的动态稀疏注意力方法",
        "通过head round-robin采样策略实现高效的全局模式发现",
        "在长文本和多模态任务上验证了RRAttention的有效性"
      ],
      "methodology": "RRAttention利用head round-robin策略进行查询采样，通过步长级别的聚合实现高效全局模式发现，并采用自适应Top-$τ$选择优化稀疏性。",
      "tags": [
        "attention mechanism",
        "sparse attention",
        "long context",
        "large language models"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 8,
      "relevance_reason": "该论文致力于提高长文本推理效率，与LLM推理密切相关。",
      "analyzed_at": "2026-02-06T06:50:59.875212",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05852v1",
      "title": "Exact Recovery in the Data Block Model",
      "abstract": "Community detection in networks is a fundamental problem in machine learning and statistical inference, with applications in social networks, biological systems, and communication networks. The stochastic block model (SBM) serves as a canonical framework for studying community structure, and exact recovery, identifying the true communities with high probability, is a central theoretical question. While classical results characterize the phase transition for exact recovery based solely on graph connectivity, many real-world networks contain additional data, such as node attributes or labels. In this work, we study exact recovery in the Data Block Model (DBM), an SBM augmented with node-associated data, as formalized by Asadi, Abbe, and Verdú (2017). We introduce the Chernoff--TV divergence and use it to characterize a sharp exact recovery threshold for the DBM. We further provide an efficient algorithm that achieves this threshold, along with a matching converse result showing impossibility below the threshold. Finally, simulations validate our findings and demonstrate the benefits of incorporating vertex data as side information in community detection.",
      "authors": [
        "Amir R. Asadi",
        "Akbar Davoodi",
        "Ramin Javadi",
        "Farzad Parvaresh"
      ],
      "categories": [
        "cs.LG",
        "cs.IT",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "published": "2026-02-05T16:36:57Z",
      "updated": "2026-02-05T16:36:57Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05852v1",
      "abs_url": "http://arxiv.org/abs/2602.05852v1",
      "summary": "研究数据块模型中的精确恢复问题，提出了新的阈值刻画和算法。",
      "key_contributions": [
        "提出了用于数据块模型精确恢复的Chernoff--TV散度",
        "刻画了数据块模型精确恢复的尖锐阈值",
        "设计了一种达到该阈值的高效算法，并证明了下界的不可达性"
      ],
      "methodology": "理论分析，利用Chernoff--TV散度建立精确恢复的阈值，并通过算法设计和反证法进行验证。",
      "tags": [
        "社区检测",
        "随机块模型",
        "精确恢复",
        "数据块模型"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 5,
      "relevance_reason": "涉及推理过程，但主要侧重于理论分析和算法设计。",
      "analyzed_at": "2026-02-06T06:51:01.900434",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05848v1",
      "title": "DARWIN: Dynamic Agentically Rewriting Self-Improving Network",
      "abstract": "DARWIN is an evolutionary GPT model, utilizing a genetic-algorithm like optimization structure with several independent GPT agents being trained individually using unique training code. Each iteration, the GPT models are prompted to modify the training code of one another in an attempt to improve their performance in a mutation-like manner, and the best GPT agents are then benchmarked and selected for the next iteration by genetic algorithm. For demonstration purposes and due to budget and time constraints, OpenAI API is used to prompt training code improvements and the nanoGPT framework is used as the training code. DARWIN also utilizes persistent JSON-based memory files to track previous reasoning and changes to code to correlate with improvement to model performance. and a bidirectional interface for HITL intervention allowing the model to request upgrades such as additional datasets, training scripts, and restructuring of file hierarchies. In experiments, DARWIN achieved a 1.26 percent improvement in model FLOPS utilization (MFU) and a 2.07 percent improvement to perplexity in 5 iterations of training over baseline configurations, demonstrating promising capabilities as a foundation for scaling evolutionary GPT training.",
      "authors": [
        "Henry Jiang"
      ],
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.NE",
      "published": "2026-02-05T16:35:46Z",
      "updated": "2026-02-05T16:35:46Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05848v1",
      "abs_url": "http://arxiv.org/abs/2602.05848v1",
      "summary": "DARWIN利用遗传算法优化GPT模型，实现自改进，提升模型性能。",
      "key_contributions": [
        "提出DARWIN框架，一种基于遗传算法的GPT模型优化方法",
        "利用GPT agent 修改其他agent的训练代码",
        "通过实验验证了该方法在提升MFU和困惑度方面的有效性"
      ],
      "methodology": "使用类似遗传算法的结构，多个GPT agent独立训练，相互修改训练代码，并进行性能评估和选择。",
      "tags": [
        "自改进",
        "遗传算法",
        "GPT",
        "Agent"
      ],
      "assigned_category": "agent_tuning",
      "relevance_score": 9,
      "relevance_reason": "论文核心关注agent的自改进和优化，使用了遗传算法，与agent tuning领域高度相关。",
      "analyzed_at": "2026-02-06T06:51:03.757130",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05847v1",
      "title": "OmniVideo-R1: Reinforcing Audio-visual Reasoning with Query Intention and Modality Attention",
      "abstract": "While humans perceive the world through diverse modalities that operate synergistically to support a holistic understanding of their surroundings, existing omnivideo models still face substantial challenges on audio-visual understanding tasks. In this paper, we propose OmniVideo-R1, a novel reinforced framework that improves mixed-modality reasoning. OmniVideo-R1 empowers models to \"think with omnimodal cues\" by two key strategies: (1) query-intensive grounding based on self-supervised learning paradigms; and (2) modality-attentive fusion built upon contrastive learning paradigms. Extensive experiments on multiple benchmarks demonstrate that OmniVideo-R1 consistently outperforms strong baselines, highlighting its effectiveness and robust generalization capabilities.",
      "authors": [
        "Zhangquan Chen",
        "Jiale Tao",
        "Ruihuang Li",
        "Yihao Hu",
        "Ruitao Chen",
        "Zhantao Yang",
        "Xinlei Yu",
        "Haodong Jing",
        "Manyuan Zhang",
        "Shuai Shao",
        "Biao Wang",
        "Qinglin Lu",
        "Ruqi Huang"
      ],
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "published": "2026-02-05T16:35:19Z",
      "updated": "2026-02-05T16:35:19Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05847v1",
      "abs_url": "http://arxiv.org/abs/2602.05847v1",
      "summary": "OmniVideo-R1通过查询意图和模态注意力增强音视频推理能力，提升了混合模态理解性能。",
      "key_contributions": [
        "提出基于自监督学习的查询式 grounding 方法",
        "提出基于对比学习的模态注意力融合方法",
        "在多个基准测试上超越现有模型"
      ],
      "methodology": "通过查询式 grounding 学习跨模态关联，并利用模态注意力融合不同模态信息，从而提升推理能力。",
      "tags": [
        "音视频理解",
        "多模态学习",
        "自监督学习",
        "对比学习"
      ],
      "assigned_category": "multimodal",
      "relevance_score": 9,
      "relevance_reason": "论文直接关注音视频多模态理解，核心问题相关性极高。",
      "analyzed_at": "2026-02-06T06:51:05.428784",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05843v1",
      "title": "OdysseyArena: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions",
      "abstract": "The rapid advancement of Large Language Models (LLMs) has catalyzed the development of autonomous agents capable of navigating complex environments. However, existing evaluations primarily adopt a deductive paradigm, where agents execute tasks based on explicitly provided rules and static goals, often within limited planning horizons. Crucially, this neglects the inductive necessity for agents to discover latent transition laws from experience autonomously, which is the cornerstone for enabling agentic foresight and sustaining strategic coherence. To bridge this gap, we introduce OdysseyArena, which re-centers agent evaluation on long-horizon, active, and inductive interactions. We formalize and instantiate four primitives, translating abstract transition dynamics into concrete interactive environments. Building upon this, we establish OdysseyArena-Lite for standardized benchmarking, providing a set of 120 tasks to measure an agent's inductive efficiency and long-horizon discovery. Pushing further, we introduce OdysseyArena-Challenge to stress-test agent stability across extreme interaction horizons (e.g., > 200 steps). Extensive experiments on 15+ leading LLMs reveal that even frontier models exhibit a deficiency in inductive scenarios, identifying a critical bottleneck in the pursuit of autonomous discovery in complex environments. Our code and data are available at https://github.com/xufangzhi/Odyssey-Arena",
      "authors": [
        "Fangzhi Xu",
        "Hang Yan",
        "Qiushi Sun",
        "Jinyang Wu",
        "Zixian Huang",
        "Muye Huang",
        "Jingyang Gong",
        "Zichen Ding",
        "Kanzhi Cheng",
        "Yian Wang",
        "Xinyu Che",
        "Zeyi Sun",
        "Jian Zhang",
        "Zhangyue Yin",
        "Haoran Luo",
        "Xuanjing Huang",
        "Ben Kao",
        "Jun Liu",
        "Qika Lin"
      ],
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "published": "2026-02-05T16:31:43Z",
      "updated": "2026-02-05T16:31:43Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05843v1",
      "abs_url": "http://arxiv.org/abs/2602.05843v1",
      "summary": "提出了OdysseyArena基准，评估LLM在长程、主动和归纳交互中的能力。",
      "key_contributions": [
        "提出了OdysseyArena基准",
        "设计了四个原语，将抽象转换动态转化为具体的交互环境",
        "评估了15+个LLM在OdysseyArena上的表现"
      ],
      "methodology": "构建OdysseyArena-Lite和OdysseyArena-Challenge，通过一系列任务评估LLM的归纳效率和长程发现能力。",
      "tags": [
        "LLM",
        "Agent",
        "Benchmarking",
        "Inductive Reasoning"
      ],
      "assigned_category": "agent",
      "relevance_score": 9,
      "relevance_reason": "论文核心关注LLM作为Agent的交互能力，是该领域的关键研究。",
      "analyzed_at": "2026-02-06T06:51:07.128172",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05833v1",
      "title": "Synthesizing Realistic Test Data without Breaking Privacy",
      "abstract": "There is a need for synthetic training and test datasets that replicate statistical distributions of original datasets without compromising their confidentiality. A lot of research has been done in leveraging Generative Adversarial Networks (GANs) for synthetic data generation. However, the resulting models are either not accurate enough or are still vulnerable to membership inference attacks (MIA) or dataset reconstruction attacks since the original data has been leveraged in the training process. In this paper, we explore the feasibility of producing a synthetic test dataset with the same statistical properties as the original one, with only indirectly leveraging the original data in the generation process. The approach is inspired by GANs, with a generation step and a discrimination step. However, in our approach, we use a test generator (a fuzzer) to produce test data from an input specification, preserving constraints set by the original data; a discriminator model determines how close we are to the original data. By evolving samples and determining \"good samples\" with the discriminator, we can generate privacy-preserving data that follows the same statistical distributions are the original dataset, leading to a similar utility as the original data. We evaluated our approach on four datasets that have been used to evaluate the state-of-the-art techniques. Our experiments highlight the potential of our approach towards generating synthetic datasets that have high utility while preserving privacy.",
      "authors": [
        "Laura Plein",
        "Alexi Turcotte",
        "Arina Hallemans",
        "Andreas Zeller"
      ],
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "published": "2026-02-05T16:22:01Z",
      "updated": "2026-02-05T16:22:01Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05833v1",
      "abs_url": "http://arxiv.org/abs/2602.05833v1",
      "summary": "提出了一种基于fuzzer和判别器的隐私保护合成数据生成方法，提高数据效用性和隐私性。",
      "key_contributions": [
        "提出基于fuzzer和判别器生成合成数据",
        "在生成过程中间接利用原始数据，保护隐私",
        "实验证明该方法在保证高数据效用性的同时保护隐私"
      ],
      "methodology": "使用fuzzer生成数据，判别器判断与原始数据的相似度，通过进化样本生成隐私数据。",
      "tags": [
        "合成数据",
        "隐私保护",
        "生成对抗网络",
        "数据生成"
      ],
      "assigned_category": "agent",
      "relevance_score": 5,
      "relevance_reason": "间接涉及测试数据生成，对Agent的测试和改进有一定参考价值。",
      "analyzed_at": "2026-02-06T06:51:09.723104",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05830v1",
      "title": "Learning Compact Boolean Networks",
      "abstract": "Floating-point neural networks dominate modern machine learning but incur substantial inference cost, motivating interest in Boolean networks for resource-constrained settings. However, learning compact and accurate Boolean networks is challenging due to their combinatorial nature. In this work, we address this challenge from three different angles: learned connections, compact convolutions and adaptive discretization. First, we propose a novel strategy to learn efficient connections with no additional parameters and negligible computational overhead. Second, we introduce a novel convolutional Boolean architecture that exploits the locality with reduced number of Boolean operations than existing methods. Third, we propose an adaptive discretization strategy to reduce the accuracy drop when converting a continuous-valued network into a Boolean one. Extensive results on standard vision benchmarks demonstrate that the Pareto front of accuracy vs. computation of our method significantly outperforms prior state-of-the-art, achieving better accuracy with up to 37x fewer Boolean operations.",
      "authors": [
        "Shengpu Wang",
        "Yuhao Mao",
        "Yani Zhang",
        "Martin Vechev"
      ],
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "published": "2026-02-05T16:19:59Z",
      "updated": "2026-02-05T16:19:59Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05830v1",
      "abs_url": "http://arxiv.org/abs/2602.05830v1",
      "summary": "针对资源受限环境，该论文提出了学习紧凑且准确的布尔网络的三种创新方法。",
      "key_contributions": [
        "学习高效连接",
        "紧凑卷积布尔架构",
        "自适应离散化策略"
      ],
      "methodology": "通过学习连接、设计紧凑卷积和自适应离散化，优化布尔网络在精度和计算成本之间的平衡。",
      "tags": [
        "Boolean Networks",
        "Resource-constrained Learning",
        "Network Compression"
      ],
      "assigned_category": "agent_tuning",
      "relevance_score": 5,
      "relevance_reason": "Boolean Networks可被用于资源受限场景下的智能体模型构建，相关性中等。",
      "analyzed_at": "2026-02-06T06:51:11.243432",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05818v1",
      "title": "TKG-Thinker: Towards Dynamic Reasoning over Temporal Knowledge Graphs via Agentic Reinforcement Learning",
      "abstract": "Temporal knowledge graph question answering (TKGQA) aims to answer time-sensitive questions by leveraging temporal knowledge bases. While Large Language Models (LLMs) demonstrate significant potential in TKGQA, current prompting strategies constrain their efficacy in two primary ways. First, they are prone to reasoning hallucinations under complex temporal constraints. Second, static prompting limits model autonomy and generalization, as it lack optimization through dynamic interaction with temporal knowledge graphs (TKGs) environments. To address these limitations, we propose \\textbf{TKG-Thinker}, a novel agent equipped with autonomous planning and adaptive retrieval capabilities for reasoning over TKGs. Specifically, TKG-Thinker performs in-depth temporal reasoning through dynamic multi-turn interactions with TKGs via a dual-training strategy. We first apply Supervised Fine-Tuning (SFT) with chain-of thought data to instill core planning capabilities, followed by a Reinforcement Learning (RL) stage that leverages multi-dimensional rewards to refine reasoning policies under intricate temporal constraints. Experimental results on benchmark datasets with three open-source LLMs show that TKG-Thinker achieves state-of-the-art performance and exhibits strong generalization across complex TKGQA settings.",
      "authors": [
        "Zihao Jiang",
        "Miao Peng",
        "Zhenyan Shan",
        "Wenjie Xu",
        "Ben Liu",
        "Gong Chen",
        "Ziqi Gao",
        "Min Peng"
      ],
      "categories": [
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.AI",
      "published": "2026-02-05T16:08:36Z",
      "updated": "2026-02-05T16:08:36Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05818v1",
      "abs_url": "http://arxiv.org/abs/2602.05818v1",
      "summary": "TKG-Thinker通过Agentic强化学习进行时序知识图谱动态推理，提升复杂时序约束下的推理能力。",
      "key_contributions": [
        "提出了TKG-Thinker，一个用于时序知识图谱推理的智能体。",
        "使用双重训练策略（SFT+RL）提高智能体的规划和推理能力。",
        "在benchmark数据集上取得了SOTA性能，并展现出良好的泛化能力。"
      ],
      "methodology": "使用监督微调（SFT）赋予智能体规划能力，再通过强化学习（RL）优化推理策略，实现动态交互推理。",
      "tags": [
        "Temporal Knowledge Graph",
        "Reinforcement Learning",
        "Agent",
        "Reasoning"
      ],
      "assigned_category": "agent",
      "relevance_score": 9,
      "relevance_reason": "论文核心关注智能体在时序知识图谱推理中的应用，属于Agent领域的核心问题。",
      "analyzed_at": "2026-02-06T06:51:13.752325",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05810v1",
      "title": "Bifrost: Steering Strategic Trajectories to Bridge Contextual Gaps for Self-Improving Agents",
      "abstract": "Autonomous agents excel in self-improvement through reflection and iterative refinement, which reuse successful task trajectories as in-context examples to assist subsequent reasoning. However, shifting across tasks often introduces a context mismatch. Hence, existing approaches either discard the trajectories or manipulate them using heuristics, leading to a non-negligible fine-tuning cost or unguaranteed performance. To bridge this gap, we reveal a context-trajectory correlation, where shifts of context are highly parallel with shifts of trajectory. Based on this finding, we propose BrIdge contextual gap FoR imprOvised trajectory STeering (Bifrost), a training-free method that leverages context differences to precisely guide the adaptation of previously solved trajectories towards the target task, mitigating the misalignment caused by context shifts. Our trajectory adaptation is conducted at the representation level using agent hidden states, ensuring trajectory transformation accurately aligns with the target context in a shared space. Across diverse benchmarks, Bifrost consistently outperforms existing trajectory reuse and finetuned self-improvement methods, demonstrating that agents can effectively leverage past experiences despite substantial context shifts.",
      "authors": [
        "Quan M. Tran",
        "Zhuo Huang",
        "Wenbin Zhang",
        "Bo Han",
        "Koji Yatani",
        "Masashi Sugiyama",
        "Tongliang Liu"
      ],
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "published": "2026-02-05T16:03:56Z",
      "updated": "2026-02-05T16:03:56Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05810v1",
      "abs_url": "http://arxiv.org/abs/2602.05810v1",
      "summary": "Bifrost通过引导轨迹调整，弥合上下文差距，提升自提升智能体的性能。",
      "key_contributions": [
        "揭示上下文与轨迹之间的相关性",
        "提出无需训练的Bifrost方法，利用上下文差异引导轨迹适应",
        "在表征层面进行轨迹调整，确保与目标上下文对齐"
      ],
      "methodology": "利用上下文差异指导先前解决的轨迹，在隐藏状态层面对轨迹进行调整，使其适应目标任务。",
      "tags": [
        "Agent",
        "Self-Improving",
        "Trajectory Steering",
        "Contextual Gap"
      ],
      "assigned_category": "agent_tuning",
      "relevance_score": 9,
      "relevance_reason": "论文核心关注智能体的自提升和轨迹优化，与agent tuning类别高度相关。",
      "analyzed_at": "2026-02-06T06:51:15.743776",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05809v1",
      "title": "Focus-Scan-Refine: From Human Visual Perception to Efficient Visual Token Pruning",
      "abstract": "Vision-language models (VLMs) often generate massive visual tokens that greatly increase inference latency and memory footprint; while training-free token pruning offers a practical remedy, existing methods still struggle to balance local evidence and global context under aggressive compression. We propose Focus-Scan-Refine (FSR), a human-inspired, plug-and-play pruning framework that mimics how humans answer visual questions: focus on key evidence, then scan globally if needed, and refine the scanned context by aggregating relevant details. FSR first focuses on key evidence by combining visual importance with instruction relevance, avoiding the bias toward visually salient but query-irrelevant regions. It then scans for complementary context conditioned on the focused set, selecting tokens that are most different from the focused evidence. Finally, FSR refines the scanned context by aggregating nearby informative tokens into the scan anchors via similarity-based assignment and score-weighted merging, without increasing the token budget. Extensive experiments across multiple VLM backbones and vision-language benchmarks show that FSR consistently improves the accuracy-efficiency trade-off over existing state-of-the-art pruning methods. The source codes can be found at https://github.com/ILOT-code/FSR",
      "authors": [
        "Enwei Tong",
        "Yuanchao Bai",
        "Yao Zhu",
        "Junjun Jiang",
        "Xianming Liu"
      ],
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2026-02-05T16:02:48Z",
      "updated": "2026-02-05T16:02:48Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05809v1",
      "abs_url": "http://arxiv.org/abs/2602.05809v1",
      "summary": "提出FSR框架，模拟人类视觉机制，有效剪枝VLMs中的视觉tokens，提升效率与精度。",
      "key_contributions": [
        "提出 Focus-Scan-Refine (FSR) 框架",
        "结合视觉重要性和指令相关性，聚焦关键证据",
        "通过相似性分配和加权合并，优化扫描上下文"
      ],
      "methodology": "FSR框架首先聚焦关键证据，然后扫描补充上下文，最后通过聚合相邻信息token来优化扫描结果，实现高效的token pruning。",
      "tags": [
        "Vision-Language Models",
        "Token Pruning",
        "Visual Question Answering",
        "Efficiency"
      ],
      "assigned_category": "multimodal",
      "relevance_score": 9,
      "relevance_reason": "该论文直接解决了VLM中的效率问题，属于多模态学习领域的核心研究。",
      "analyzed_at": "2026-02-06T06:51:17.917865",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05805v1",
      "title": "NEX: Neuron Explore-Exploit Scoring for Label-Free Chain-of-Thought Selection and Model Ranking",
      "abstract": "Large language models increasingly spend inference compute sampling multiple chain-of-thought traces or searching over merged checkpoints. This shifts the bottleneck from generation to selection, often without supervision on the target distribution. We show entropy-based exploration proxies follow an inverted-U with accuracy, suggesting extra exploration can become redundant and induce overthinking. We propose NEX, a white-box label-free unsupervised scoring framework that views reasoning as alternating E-phase (exploration) and X-phase (exploitation). NEX detects E-phase as spikes in newly activated MLP neurons per token from sparse activation caches, then uses a sticky two-state HMM to infer E-X phases and credits E-introduced neurons by whether they are reused in the following X span. These signals yield interpretable neuron weights and a single Good-Mass Fraction score to rank candidate responses and merged variants without task answers. Across reasoning benchmarks and Qwen3 merge families, NEX computed on a small unlabeled activation set predicts downstream accuracy and identifies better variants; we further validate the E-X signal with human annotations and provide causal evidence via \"Effective-vs-Redundant\" neuron transfer.",
      "authors": [
        "Kang Chen",
        "Zhuoka Feng",
        "Sihan Zhao",
        "Kai Xiong",
        "Junjie Nian",
        "Yaoning Wang",
        "Changyi Xiao",
        "Yixin Cao"
      ],
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "published": "2026-02-05T15:59:12Z",
      "updated": "2026-02-05T15:59:12Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05805v1",
      "abs_url": "http://arxiv.org/abs/2602.05805v1",
      "summary": "NEX提出了一种无监督的CoT选择和模型排序框架，通过神经元激活模式识别探索与利用阶段。",
      "key_contributions": [
        "提出NEX框架，用于无监督CoT选择和模型排序",
        "利用神经元激活模式识别探索与利用阶段",
        "验证了NEX在推理基准和模型融合上的有效性"
      ],
      "methodology": "NEX通过稀疏激活缓存检测新激活的MLP神经元，使用HMM推断探索与利用阶段，并根据神经元的复用情况进行评分。",
      "tags": [
        "Chain-of-Thought",
        "无监督学习",
        "模型排序",
        "神经元激活",
        "探索与利用"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 9,
      "relevance_reason": "论文核心关注LLM推理过程的优化，直接研究了CoT选择和模型排序问题。",
      "analyzed_at": "2026-02-06T06:51:19.588778",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05794v1",
      "title": "FiMI: A Domain-Specific Language Model for Indian Finance Ecosystem",
      "abstract": "We present FiMI (Finance Model for India), a domain-specialized financial language model developed for Indian digital payment systems. We develop two model variants: FiMI Base and FiMI Instruct. FiMI adapts the Mistral Small 24B architecture through a multi-stage training pipeline, beginning with continuous pre-training on 68 Billion tokens of curated financial, multilingual (English, Hindi, Hinglish), and synthetic data. This is followed by instruction fine-tuning and domain-specific supervised fine-tuning focused on multi-turn, tool-driven conversations that model real-world workflows, such as transaction disputes and mandate lifecycle management. Evaluations reveal that FiMI Base achieves a 20% improvement over the Mistral Small 24B Base model on finance reasoning benchmark, while FiMI Instruct outperforms the Mistral Small 24B Instruct model by 87% on domain-specific tool-calling. Moreover, FiMI achieves these significant domain gains while maintaining comparable performance to models of similar size on general benchmarks.",
      "authors": [
        "Aboli Kathar",
        "Aman Kumar",
        "Anusha Kamath",
        "Araveeti Srujan",
        "Ashish Sharma",
        "Chandra Bhushan",
        "Dilip Asbe",
        "Divya Sorate",
        "Duddu Prasanth Kumar",
        "Evan Acharya",
        "Harsh Sharma",
        "Hrithik Kadam",
        "Kanishk Singla",
        "Keyur Doshi",
        "Kiran Praveen",
        "Kolisetty Krishna SK",
        "Krishanu Adhikary",
        "Lokesh MPT",
        "Mayurdeep Sonowal",
        "Nadeem Shaikh",
        "Navya Prakash",
        "Nimit Kothari",
        "Nitin Kukreja",
        "Prashant Devadiga",
        "Rakesh Paul",
        "Ratanjeet Pratap Chauhan",
        "Raunak Kalani",
        "Raviraj Joshi",
        "Shamanth MH",
        "Shantanu Pandey",
        "Shubham Soni",
        "Siddharth Dixit",
        "Smriti Jopat",
        "Sunil Patel",
        "Suraj Singh",
        "Suvradip Paul",
        "Tulasi Pilla",
        "Utkarsh Vaidya",
        "Vineeth Nambiar",
        "Vishal Kanvaty",
        "Yatharth Dedhia"
      ],
      "categories": [
        "cs.AI",
        "cs.CE",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "published": "2026-02-05T15:48:49Z",
      "updated": "2026-02-05T15:48:49Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05794v1",
      "abs_url": "http://arxiv.org/abs/2602.05794v1",
      "summary": "FiMI是为印度金融领域定制的领域专用语言模型，显著提升了金融推理和工具调用能力。",
      "key_contributions": [
        "构建印度金融领域专用语言模型FiMI",
        "在金融推理和工具调用任务上超越Mistral Small",
        "维持通用基准测试性能"
      ],
      "methodology": "FiMI基于Mistral Small架构，通过多阶段训练，包括金融数据预训练、指令微调和领域监督微调。",
      "tags": [
        "domain-specific language model",
        "finance",
        "Indian language",
        "tool-calling"
      ],
      "assigned_category": "agent",
      "relevance_score": 8,
      "relevance_reason": "论文重点在于工具调用能力，与Agent方向高度相关。",
      "analyzed_at": "2026-02-06T06:51:21.144941",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05780v1",
      "title": "Automated Customization of LLMs for Enterprise Code Repositories Using Semantic Scopes",
      "abstract": "Code completion (CC) is a task frequently used by developers when working in collaboration with LLM-based programming assistants. Despite the increased performance of LLMs on public benchmarks, out of the box LLMs still have a hard time generating code that aligns with a private code repository not previously seen by the model's training data. Customizing code LLMs to a private repository provides a way to improve the model performance. In this paper we present our approach for automated LLM customization based on semantic scopes in the code. We evaluate LLMs on real industry cases with two private enterprise code repositories with two customization strategies: Retrieval-Augmented Generation (RAG) and supervised Fine-Tuning (FT). Our mechanism for ingesting the repository's data and formulating the training data pairs with semantic scopes helps models to learn the underlying patterns specific to the repository, providing more precise code to developers and helping to boost their productivity. The code completions of moderately sized customized models can be significantly better than those of uncustomized models of much larger capacity. We also include an analysis of customization on two public benchmarks and present opportunities for future work.",
      "authors": [
        "Ulrich Finkler",
        "Irene Manotas",
        "Wei Zhang",
        "Geert Janssen",
        "Octavian Popescu",
        "Shyam Ramji"
      ],
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "published": "2026-02-05T15:38:54Z",
      "updated": "2026-02-05T15:38:54Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05780v1",
      "abs_url": "http://arxiv.org/abs/2602.05780v1",
      "summary": "针对企业代码库，提出基于语义范围的LLM自动定制方法，提高代码补全质量和开发者效率。",
      "key_contributions": [
        "提出基于语义范围的代码LLM定制方法",
        "评估了RAG和FT两种定制策略在企业代码库上的效果",
        "证明了定制模型在代码补全方面优于大型未定制模型"
      ],
      "methodology": "通过语义范围解析代码库数据，构建训练数据对，使用RAG和FT定制LLM，并在企业代码库上评估代码补全性能。",
      "tags": [
        "LLM",
        "代码补全",
        "定制化",
        "语义分析",
        "RAG",
        "Fine-Tuning"
      ],
      "assigned_category": "memory",
      "relevance_score": 9,
      "relevance_reason": "论文核心在于使用RAG和Fine-tuning方法定制LLM，以提升在特定代码库上的性能。",
      "analyzed_at": "2026-02-06T06:51:25.176839",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05774v1",
      "title": "Variational Speculative Decoding: Rethinking Draft Training from Token Likelihood to Sequence Acceptance",
      "abstract": "Speculative decoding accelerates inference for (M)LLMs, yet a training-decoding discrepancy persists: while existing methods optimize single greedy trajectories, decoding involves verifying and ranking multiple sampled draft paths. We propose Variational Speculative Decoding (VSD), formulating draft training as variational inference over latent proposals (draft paths). VSD maximizes the marginal probability of target-model acceptance, yielding an ELBO that promotes high-quality latent proposals while minimizing divergence from the target distribution. To enhance quality and reduce variance, we incorporate a path-level utility and optimize via an Expectation-Maximization procedure. The E-step draws MCMC samples from an oracle-filtered posterior, while the M-step maximizes weighted likelihood using Adaptive Rejection Weighting (ARW) and Confidence-Aware Regularization (CAR). Theoretical analysis confirms that VSD increases expected acceptance length and speedup. Extensive experiments across LLMs and MLLMs show that VSD achieves up to a 9.6% speedup over EAGLE-3 and 7.9% over ViSpec, significantly improving decoding efficiency.",
      "authors": [
        "Xiandong Zou",
        "Jianshu Li",
        "Jing Huang",
        "Pan Zhou"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "published": "2026-02-05T15:36:19Z",
      "updated": "2026-02-05T15:36:19Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05774v1",
      "abs_url": "http://arxiv.org/abs/2602.05774v1",
      "summary": "提出了变分推测解码VSD，通过优化草稿路径来加速LLM和MLLM的推理，提高解码效率。",
      "key_contributions": [
        "提出了Variational Speculative Decoding (VSD)框架",
        "使用变分推断优化草稿训练，最大化目标模型接受概率",
        "引入路径级别效用和期望最大化程序，提升草稿质量"
      ],
      "methodology": "将草稿训练视为变分推断，优化隐变量（草稿路径），使用MCMC采样和自适应拒绝加权等技术。",
      "tags": [
        "推测解码",
        "变分推断",
        "语言模型加速",
        "MLLM"
      ],
      "assigned_category": "multimodal",
      "relevance_score": 8,
      "relevance_reason": "论文涉及到MLLM的推理加速，与multimodal相关性较高。",
      "analyzed_at": "2026-02-06T06:51:26.984513",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05765v1",
      "title": "RL-VLA$^3$: Reinforcement Learning VLA Accelerating via Full Asynchronism",
      "abstract": "In recent years, Vision-Language-Action (VLA) models have emerged as a crucial pathway towards general embodied intelligence, yet their training efficiency has become a key bottleneck. Although existing reinforcement learning (RL)-based training frameworks like RLinf can enhance model generalization, they still rely on synchronous execution, leading to severe resource underutilization and throughput limitations during environment interaction, policy generation (rollout), and model update phases (actor). To overcome this challenge, this paper, for the first time, proposes and implements a fully-asynchronous policy training framework encompassing the entire pipeline from environment interaction, rollout generation, to actor policy updates. Systematically drawing inspiration from asynchronous optimization ideas in large model RL, our framework designs a multi-level decoupled architecture. This includes asynchronous parallelization of environment interaction and trajectory collection, streaming execution for policy generation, and decoupled scheduling for training updates. We validated the effectiveness of our method across diverse VLA models and environments. On the LIBERO benchmark, the framework achieves throughput improvements of up to 59.25\\% compared to existing synchronous strategies. When deeply optimizing separation strategies, throughput can be increased by as much as 126.67\\%. We verified the effectiveness of each asynchronous component via ablation studies. Scaling law validation across 8 to 256 GPUs demonstrates our method's excellent scalability under most conditions.",
      "authors": [
        "Zhong Guan",
        "Haoran Sun",
        "Yongjian Guo",
        "Shuai Di",
        "Xiaodong Bai",
        "Jing Long",
        "Tianyun Zhao",
        "Mingxi Luo",
        "Chen Zhou",
        "Yucheng Guo",
        "Qiming Yang",
        "Wanting Xu",
        "Wen Huang",
        "Yunxuan Ma",
        "Hongke Zhao",
        "Likang Wu",
        "Xiaotie Deng",
        "Xi Xiao",
        "Sheng Wen",
        "Yicheng Gong",
        "Junwu Xiong"
      ],
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "published": "2026-02-05T15:30:23Z",
      "updated": "2026-02-05T15:30:23Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05765v1",
      "abs_url": "http://arxiv.org/abs/2602.05765v1",
      "summary": "提出RL-VLA$^3$框架，通过全异步策略加速VLA模型的强化学习训练，提升训练效率。",
      "key_contributions": [
        "提出了完全异步的VLA模型强化学习训练框架。",
        "设计了多级解耦架构，包括异步并行环境交互、流式策略生成和解耦训练更新。",
        "实验验证了框架在VLA模型和环境中的有效性和可扩展性。"
      ],
      "methodology": "通过异步并行化环境交互、流式策略生成和解耦训练更新，实现VLA模型强化学习训练的全异步化，提高吞吐量。",
      "tags": [
        "强化学习",
        "VLA模型",
        "异步训练",
        "分布式训练",
        "机器人"
      ],
      "assigned_category": "agent_tuning",
      "relevance_score": 8,
      "relevance_reason": "论文关注VLA模型的训练效率，并采用异步方法优化agent训练过程，因此与Agent Tuning & Optimization高度相关。",
      "analyzed_at": "2026-02-06T06:51:28.812510",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05762v1",
      "title": "RocqSmith: Can Automatic Optimization Forge Better Proof Agents?",
      "abstract": "This work studies the applicability of automatic AI agent optimization methods to real-world agents in formal verification settings, focusing on automated theorem proving in Rocq as a representative and challenging domain. We evaluate how different automatic agent optimizers perform when applied to the task of optimizing a Rocq proof-generation agent, and assess whether parts of the fine-grained tuning of agentic systems, such as prompt design, contextual knowledge, and control strategies, can be automated. Our results show that while several optimizers yield measurable improvements, simple few-shot bootstrapping is the most consistently effective; however, none of the studied methods matches the performance of a carefully engineered state-of-the-art proof agent.",
      "authors": [
        "Andrei Kozyrev",
        "Nikita Khramov",
        "Denis Lochmelis",
        "Valerio Morelli",
        "Gleb Solovev",
        "Anton Podkopaev"
      ],
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.LO",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "published": "2026-02-05T15:28:26Z",
      "updated": "2026-02-05T15:28:26Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05762v1",
      "abs_url": "http://arxiv.org/abs/2602.05762v1",
      "summary": "研究AI自动优化方法在Rocq定理证明Agent中的应用，评估其优化Agent策略的能力。",
      "key_contributions": [
        "评估了不同优化器在Rocq定理证明Agent上的效果",
        "发现few-shot bootstrapping方法效果较好",
        "发现自动优化方法与人工设计的Agent相比仍有差距"
      ],
      "methodology": "采用实验方法，将不同的自动Agent优化器应用于Rocq证明生成Agent，并评估其性能。",
      "tags": [
        "自动定理证明",
        "AI Agent优化",
        "Rocq",
        "Agent tuning"
      ],
      "assigned_category": "agent_tuning",
      "relevance_score": 8,
      "relevance_reason": "论文直接研究AI Agent的自动优化方法在定理证明领域的应用，属于Agent Tuning的核心内容。",
      "analyzed_at": "2026-02-06T06:51:30.388077",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05758v1",
      "title": "LongR: Unleashing Long-Context Reasoning via Reinforcement Learning with Dense Utility Rewards",
      "abstract": "Reinforcement Learning has emerged as a key driver for LLM reasoning. This capability is equally pivotal in long-context scenarios--such as long-dialogue understanding and structured data analysis, where the challenge extends beyond consuming tokens to performing rigorous deduction. While existing efforts focus on data synthesis or architectural changes, recent work points out that relying solely on sparse, outcome-only rewards yields limited gains, as such coarse signals are often insufficient to effectively guide the complex long-context reasoning. To address this, we propose LongR, a unified framework that enhances long-context performance by integrating a dynamic \"Think-and-Read\" mechanism, which interleaves reasoning with document consultation, with a contextual density reward based on relative information gain to quantify the utility of the relevant documents. Empirically, LongR achieves a 9% gain on LongBench v2 and consistent improvements on RULER and InfiniteBench, demonstrating robust efficiency in navigating extensive contexts. Furthermore, LongR consistently enhances performance across diverse RL algorithms (e.g., DAPO, GSPO). Finally, we conduct in-depth analyses to investigate the impact of reasoning chain length on efficiency and the model's robustness against distractors.",
      "authors": [
        "Bowen Ping",
        "Zijun Chen",
        "Yiyao Yu",
        "Tingfeng Hui",
        "Junchi Yan",
        "Baobao Chang"
      ],
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "published": "2026-02-05T15:26:47Z",
      "updated": "2026-02-05T15:26:47Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05758v1",
      "abs_url": "http://arxiv.org/abs/2602.05758v1",
      "summary": "LongR通过强化学习和密集奖励，提升LLM在长文本推理中的表现。",
      "key_contributions": [
        "提出LongR框架，结合动态“思考与阅读”机制。",
        "引入基于相对信息增益的上下文密度奖励。",
        "在LongBench v2、RULER和InfiniteBench上取得显著提升。"
      ],
      "methodology": "LongR使用强化学习，通过动态调整思考和阅读过程，并以密集奖励函数引导模型学习。",
      "tags": [
        "Reinforcement Learning",
        "Long Context",
        "Reasoning",
        "Dense Reward"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 9,
      "relevance_reason": "论文核心在于使用强化学习提升LLM推理能力，尤其针对长文本。",
      "analyzed_at": "2026-02-06T06:51:34.920653",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05754v1",
      "title": "TimelyFreeze: Adaptive Parameter Freezing Mechanism for Pipeline Parallelism",
      "abstract": "Pipeline parallelism enables training models that exceed single-device memory, but practical throughput remains limited by pipeline bubbles. Although parameter freezing can improve training throughput by adaptively skipping backward computation, existing methods often over-freeze parameters, resulting in unnecessary accuracy degradation. To address this issue, we propose TimelyFreeze, which models the pipeline schedule as a directed acyclic graph and solves a linear program to compute optimal freeze ratios that minimize batch execution time under accuracy constraints. Experiments show that TimelyFreeze achieves up to 40% training throughput improvement on LLaMA-8B with comparable accuracy. Overall, it enables faster large-scale model training without compromising convergence and generalizes across diverse pipeline-parallel settings.",
      "authors": [
        "Seonghye Cho",
        "Jaemin Han",
        "Hyunjin Kim",
        "Euisoo Jung",
        "Jae-Gil Lee"
      ],
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "published": "2026-02-05T15:24:11Z",
      "updated": "2026-02-05T15:24:11Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05754v1",
      "abs_url": "http://arxiv.org/abs/2602.05754v1",
      "summary": "TimelyFreeze自适应参数冻结，优化流水线并行训练，提升吞吐量并保持精度。",
      "key_contributions": [
        "提出TimelyFreeze，一种新的参数冻结机制",
        "通过线性规划求解最优冻结比例",
        "显著提升流水线并行训练吞吐量"
      ],
      "methodology": "将流水线调度建模为有向无环图，通过线性规划计算最优参数冻结比例，在精度约束下最小化执行时间。",
      "tags": [
        "流水线并行",
        "参数冻结",
        "线性规划",
        "模型训练优化"
      ],
      "assigned_category": "agent_tuning",
      "relevance_score": 6,
      "relevance_reason": "方法涉及优化训练效率，与agent tuning有一定的关联。",
      "analyzed_at": "2026-02-06T06:51:36.511893",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05748v1",
      "title": "LeakBoost: Perceptual-Loss-Based Membership Inference Attack",
      "abstract": "Membership inference attacks (MIAs) aim to determine whether a sample was part of a model's training set, posing serious privacy risks for modern machine-learning systems. Existing MIAs primarily rely on static indicators, such as loss or confidence, and do not fully leverage the dynamic behavior of models when actively probed. We propose LeakBoost, a perceptual-loss-based interrogation framework that actively probes a model's internal representations to expose hidden membership signals. Given a candidate input, LeakBoost synthesizes an interrogation image by optimizing a perceptual (activation-space) objective, amplifying representational differences between members and non-members. This image is then analyzed by an off-the-shelf membership detector, without modifying the detector itself. When combined with existing membership inference methods, LeakBoost achieves substantial improvements at low false-positive rates across multiple image classification datasets and diverse neural network architectures. In particular, it raises AUC from near-chance levels (0.53-0.62) to 0.81-0.88, and increases TPR at 1 percent FPR by over an order of magnitude compared to strong baseline attacks. A detailed sensitivity analysis reveals that deeper layers and short, low-learning-rate optimization produce the strongest leakage, and that improvements concentrate in gradient-based detectors. LeakBoost thus offers a modular and computationally efficient way to assess privacy risks in white-box settings, advancing the study of dynamic membership inference.",
      "authors": [
        "Amit Kravchik Taub",
        "Fred M. Grabovski",
        "Guy Amit",
        "Yisroel Mirsky"
      ],
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "published": "2026-02-05T15:15:35Z",
      "updated": "2026-02-05T15:15:35Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05748v1",
      "abs_url": "http://arxiv.org/abs/2602.05748v1",
      "summary": "LeakBoost通过感知损失主动探测模型，增强成员推理攻击的效果。",
      "key_contributions": [
        "提出了LeakBoost框架，利用感知损失优化输入",
        "显著提升了成员推理攻击的成功率",
        "详细分析了不同参数对攻击效果的影响"
      ],
      "methodology": "LeakBoost通过优化感知损失合成图像，放大成员和非成员的表征差异，再用现有检测器进行判断。",
      "tags": [
        "membership inference attack",
        "privacy",
        "machine learning security",
        "perceptual loss"
      ],
      "assigned_category": "agent_tuning",
      "relevance_score": 6,
      "relevance_reason": "可以通过优化探测方法提升攻击效果，类似agent tuning优化prompt。",
      "analyzed_at": "2026-02-06T06:51:38.219812",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05746v1",
      "title": "Learning to Inject: Automated Prompt Injection via Reinforcement Learning",
      "abstract": "Prompt injection is one of the most critical vulnerabilities in LLM agents; yet, effective automated attacks remain largely unexplored from an optimization perspective. Existing methods heavily depend on human red-teamers and hand-crafted prompts, limiting their scalability and adaptability. We propose AutoInject, a reinforcement learning framework that generates universal, transferable adversarial suffixes while jointly optimizing for attack success and utility preservation on benign tasks. Our black-box method supports both query-based optimization and transfer attacks to unseen models and tasks. Using only a 1.5B parameter adversarial suffix generator, we successfully compromise frontier systems including GPT 5 Nano, Claude Sonnet 3.5, and Gemini 2.5 Flash on the AgentDojo benchmark, establishing a stronger baseline for automated prompt injection research.",
      "authors": [
        "Xin Chen",
        "Jie Zhang",
        "Florian Tramer"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "published": "2026-02-05T15:14:46Z",
      "updated": "2026-02-05T15:14:46Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05746v1",
      "abs_url": "http://arxiv.org/abs/2602.05746v1",
      "summary": "提出AutoInject框架，利用强化学习自动生成Prompt Injection攻击，提升LLM安全性评估。",
      "key_contributions": [
        "提出基于强化学习的自动化Prompt Injection方法AutoInject",
        "能够在黑盒条件下攻击多种LLM，包括GPT和Claude",
        "在AgentDojo基准测试上建立了更强的Prompt Injection基线"
      ],
      "methodology": "使用强化学习框架，训练一个生成对抗性后缀的生成器，联合优化攻击成功率和良性任务的效用保持。",
      "tags": [
        "Prompt Injection",
        "强化学习",
        "LLM安全",
        "对抗攻击"
      ],
      "assigned_category": "agent",
      "relevance_score": 9,
      "relevance_reason": "核心研究LLM Agent的安全问题，通过自动化Prompt Injection进行评估。",
      "analyzed_at": "2026-02-06T06:51:39.765407",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05735v1",
      "title": "CSRv2: Unlocking Ultra-Sparse Embeddings",
      "abstract": "In the era of large foundation models, the quality of embeddings has become a central determinant of downstream task performance and overall system capability. Yet widely used dense embeddings are often extremely high-dimensional, incurring substantial costs in storage, memory, and inference latency. To address these, Contrastive Sparse Representation (CSR) is recently proposed as a promising direction, mapping dense embeddings into high-dimensional but k-sparse vectors, in contrast to compact dense embeddings such as Matryoshka Representation Learning (MRL). Despite its promise, CSR suffers severe degradation in the ultra-sparse regime, where over 80% of neurons remain inactive, leaving much of its efficiency potential unrealized. In this paper, we introduce CSRv2, a principled training approach designed to make ultra-sparse embeddings viable. CSRv2 stabilizes sparsity learning through progressive k-annealing, enhances representational quality via supervised contrastive objectives, and ensures end-to-end adaptability with full backbone finetuning. CSRv2 reduces dead neurons from 80% to 20% and delivers a 14% accuracy gain at k=2, bringing ultra-sparse embeddings on par with CSR at k=8 and MRL at 32 dimensions, all with only two active features. While maintaining comparable performance, CSRv2 delivers a 7x speedup over MRL, and yields up to 300x improvements in compute and memory efficiency relative to dense embeddings in text representation. Extensive experiments across text and vision demonstrate that CSRv2 makes ultra-sparse embeddings practical without compromising performance, where CSRv2 achieves 7%/4% improvement over CSR when k=4 and further increases this gap to 14%/6% when k=2 in text/vision representation. By making extreme sparsity viable, CSRv2 broadens the design space for real-time and edge-deployable AI systems where both embedding quality and efficiency are critical.",
      "authors": [
        "Lixuan Guo",
        "Yifei Wang",
        "Tiansheng Wen",
        "Yifan Wang",
        "Aosong Feng",
        "Bo Chen",
        "Stefanie Jegelka",
        "Chenyu You"
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IR",
        "cs.IT"
      ],
      "primary_category": "cs.LG",
      "published": "2026-02-05T14:59:51Z",
      "updated": "2026-02-05T14:59:51Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05735v1",
      "abs_url": "http://arxiv.org/abs/2602.05735v1",
      "summary": "CSRv2通过改进训练方法，使超稀疏嵌入在保证性能的同时，显著提升计算和存储效率。",
      "key_contributions": [
        "提出渐进式k-退火稳定稀疏学习",
        "引入监督对比目标增强表征质量",
        "实现端到端可适应的全骨干微调"
      ],
      "methodology": "通过渐进式k-退火、监督对比目标和全骨干微调，优化CSR的训练过程，实现超稀疏嵌入的高效利用。",
      "tags": [
        "稀疏嵌入",
        "对比学习",
        "模型优化",
        "高效计算"
      ],
      "assigned_category": "memory",
      "relevance_score": 7,
      "relevance_reason": "论文优化embedding以提升效率，与memory和RAG的效率优化相关。",
      "analyzed_at": "2026-02-06T06:51:41.578890",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05734v1",
      "title": "Evaluating the impact of word embeddings on similarity scoring in practical information retrieval",
      "abstract": "Search behaviour is characterised using synonymy and polysemy as users often want to search information based on meaning. Semantic representation strategies represent a move towards richer associative connections that can adequately capture this complex usage of language. Vector Space Modelling (VSM) and neural word embeddings play a crucial role in modern machine learning and Natural Language Processing (NLP) pipelines. Embeddings use distributional semantics to represent words, sentences, paragraphs or entire documents as vectors in high dimensional spaces. This can be leveraged by Information Retrieval (IR) systems to exploit the semantic relatedness between queries and answers.   This paper evaluates an alternative approach to measuring query statement similarity that moves away from the common similarity measure of centroids of neural word embeddings. Motivated by the Word Movers Distance (WMD) model, similarity is evaluated using the distance between individual words of queries and statements. Results from ranked query and response statements demonstrate significant gains in accuracy using the combined approach of similarity ranking through WMD with the word embedding techniques. The top performing WMD + GloVe combination outperforms all other state-of-the-art retrieval models including Doc2Vec and the baseline LSA model. Along with the significant gains in performance of similarity ranking through WMD, we conclude that the use of pre-trained word embeddings, trained on vast amounts of data, result in domain agnostic language processing solutions that are portable to diverse business use-cases.",
      "authors": [
        "Niall McCarroll",
        "Kevin Curran",
        "Eugene McNamee",
        "Angela Clist",
        "Andrew Brammer"
      ],
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "published": "2026-02-05T14:57:38Z",
      "updated": "2026-02-05T14:57:38Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05734v1",
      "abs_url": "http://arxiv.org/abs/2602.05734v1",
      "summary": "该论文评估了基于WMD和词嵌入的相似度计算方法在信息检索中的有效性，并验证了其优越性。",
      "key_contributions": [
        "提出基于WMD和词嵌入的相似度计算方法",
        "证明了WMD + GloVe组合优于其他检索模型",
        "验证了预训练词嵌入的领域无关性和可移植性"
      ],
      "methodology": "使用WMD计算查询和文档中单个词之间的距离，并结合词嵌入技术，进行排序查询和响应语句的相似度评估。",
      "tags": [
        "信息检索",
        "词嵌入",
        "Word Mover's Distance",
        "相似度计算"
      ],
      "assigned_category": "memory",
      "relevance_score": 8,
      "relevance_reason": "论文研究语义相似度在检索中的应用，与RAG相关。",
      "analyzed_at": "2026-02-06T06:51:43.603389",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05729v1",
      "title": "Adaptive Global and Fine-Grained Perceptual Fusion for MLLM Embeddings Compatible with Hard Negative Amplification",
      "abstract": "Multimodal embeddings serve as a bridge for aligning vision and language, with the two primary implementations -- CLIP-based and MLLM-based embedding models -- both limited to capturing only global semantic information. Although numerous studies have focused on fine-grained understanding, we observe that complex scenarios currently targeted by MLLM embeddings often involve a hybrid perceptual pattern of both global and fine-grained elements, thus necessitating a compatible fusion mechanism. In this paper, we propose Adaptive Global and Fine-grained perceptual Fusion for MLLM Embeddings (AGFF-Embed), a method that prompts the MLLM to generate multiple embeddings focusing on different dimensions of semantic information, which are then adaptively and smoothly aggregated. Furthermore, we adapt AGFF-Embed with the Explicit Gradient Amplification (EGA) technique to achieve in-batch hard negatives enhancement without requiring fine-grained editing of the dataset. Evaluation on the MMEB and MMVP-VLM benchmarks shows that AGFF-Embed comprehensively achieves state-of-the-art performance in both general and fine-grained understanding compared to other multimodal embedding models.",
      "authors": [
        "Lexiang Hu",
        "Youze Xue",
        "Dian Li",
        "Gang Liu",
        "Zhouchen Lin"
      ],
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "published": "2026-02-05T14:52:35Z",
      "updated": "2026-02-05T14:52:35Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05729v1",
      "abs_url": "http://arxiv.org/abs/2602.05729v1",
      "summary": "提出了AGFF-Embed，自适应融合全局和细粒度信息的MLLM嵌入，并结合EGA提升性能。",
      "key_contributions": [
        "提出AGFF-Embed模型，融合全局和细粒度感知",
        "利用MLLM生成不同语义维度的嵌入",
        "结合EGA技术实现hard negative增强",
        "在MMEB和MMVP-VLM基准测试上达到SOTA"
      ],
      "methodology": "通过prompt MLLM生成多维度语义信息嵌入，自适应融合，并结合EGA实现hard negative挖掘。",
      "tags": [
        "MLLM",
        "多模态学习",
        "嵌入",
        "hard negative mining",
        "感知融合"
      ],
      "assigned_category": "multimodal",
      "relevance_score": 9,
      "relevance_reason": "论文核心是多模态大模型嵌入的改进与优化，属于该领域关键问题。",
      "analyzed_at": "2026-02-06T06:51:45.698996",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05728v1",
      "title": "CompactRAG: Reducing LLM Calls and Token Overhead in Multi-Hop Question Answering",
      "abstract": "Retrieval-augmented generation (RAG) has become a key paradigm for knowledge-intensive question answering. However, existing multi-hop RAG systems remain inefficient, as they alternate between retrieval and reasoning at each step, resulting in repeated LLM calls, high token consumption, and unstable entity grounding across hops. We propose CompactRAG, a simple yet effective framework that decouples offline corpus restructuring from online reasoning.   In the offline stage, an LLM reads the corpus once and converts it into an atomic QA knowledge base, which represents knowledge as minimal, fine-grained question-answer pairs. In the online stage, complex queries are decomposed and carefully rewritten to preserve entity consistency, and are resolved through dense retrieval followed by RoBERTa-based answer extraction. Notably, during inference, the LLM is invoked only twice in total - once for sub-question decomposition and once for final answer synthesis - regardless of the number of reasoning hops.   Experiments on HotpotQA, 2WikiMultiHopQA, and MuSiQue demonstrate that CompactRAG achieves competitive accuracy while substantially reducing token consumption compared to iterative RAG baselines, highlighting a cost-efficient and practical approach to multi-hop reasoning over large knowledge corpora. The implementation is available at GitHub.",
      "authors": [
        "Hao Yang",
        "Zhiyu Yang",
        "Xupeng Zhang",
        "Wei Wei",
        "Yunjie Zhang",
        "Lin Yang"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "published": "2026-02-05T14:52:06Z",
      "updated": "2026-02-05T14:52:06Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05728v1",
      "abs_url": "http://arxiv.org/abs/2602.05728v1",
      "summary": "CompactRAG通过离线知识库构建和在线高效推理，显著降低多跳问答中的LLM调用和token消耗。",
      "key_contributions": [
        "提出CompactRAG框架，解耦离线知识库构建和在线推理",
        "构建原子QA知识库，减少LLM推理步骤",
        "通过实体一致性保持和RoBERTa答案抽取提升准确率和效率"
      ],
      "methodology": "离线阶段LLM构建原子QA知识库，在线阶段分解问题并重写，通过检索和答案抽取得到答案，LLM调用仅两次。",
      "tags": [
        "RAG",
        "多跳问答",
        "知识库",
        "效率优化"
      ],
      "assigned_category": "memory",
      "relevance_score": 9,
      "relevance_reason": "论文直接关注RAG框架的效率优化，属于该领域的核心问题。",
      "analyzed_at": "2026-02-06T06:51:47.760893",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05725v1",
      "title": "Muon in Associative Memory Learning: Training Dynamics and Scaling Laws",
      "abstract": "Muon updates matrix parameters via the matrix sign of the gradient and has shown strong empirical gains, yet its dynamics and scaling behavior remain unclear in theory. We study Muon in a linear associative memory model with softmax retrieval and a hierarchical frequency spectrum over query-answer pairs, with and without label noise. In this setting, we show that Gradient Descent (GD) learns frequency components at highly imbalanced rates, leading to slow convergence bottlenecked by low-frequency components. In contrast, the Muon optimizer mitigates this imbalance, leading to faster and more uniform progress. Specifically, in the noiseless case, Muon achieves an exponential speedup over GD; in the noisy case with a power-decay frequency spectrum, we derive Muon's optimization scaling law and demonstrate its superior scaling efficiency over GD. Furthermore, we show that Muon can be interpreted as an implicit matrix preconditioner arising from adaptive task alignment and block-symmetric gradient structure. In contrast, the preconditioner with coordinate-wise sign operator could match Muon under oracle access to unknown task representations, which is infeasible for SignGD in practice. Experiments on synthetic long-tail classification and LLaMA-style pre-training corroborate the theory.",
      "authors": [
        "Binghui Li",
        "Kaifei Wang",
        "Han Zhong",
        "Pinyan Lu",
        "Liwei Wang"
      ],
      "categories": [
        "cs.LG",
        "math.OC",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "published": "2026-02-05T14:49:40Z",
      "updated": "2026-02-05T14:49:40Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05725v1",
      "abs_url": "http://arxiv.org/abs/2602.05725v1",
      "summary": "论文研究了Muon优化器在联想记忆学习中的训练动态和缩放规律，揭示其优于梯度下降的原因。",
      "key_contributions": [
        "证明了Muon在无噪声情况下比梯度下降快指数级",
        "推导了噪声情况下Muon的优化缩放律，并证明其优于梯度下降",
        "解释了Muon可以被视为隐式的矩阵预处理器"
      ],
      "methodology": "使用线性联想记忆模型和softmax检索，分析了Muon和梯度下降在不同频率分量下的学习速率和缩放规律。",
      "tags": [
        "优化器",
        "Muon",
        "联想记忆",
        "缩放律"
      ],
      "assigned_category": "memory",
      "relevance_score": 7,
      "relevance_reason": "联想记忆是Memory的重要组成部分，论文分析了优化器在其中的表现。",
      "analyzed_at": "2026-02-06T06:51:49.476727",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05723v1",
      "title": "Mitigating Hallucination in Financial Retrieval-Augmented Generation via Fine-Grained Knowledge Verification",
      "abstract": "In financial Retrieval-Augmented Generation (RAG) systems, models frequently rely on retrieved documents to generate accurate responses due to the time-sensitive nature of the financial domain. While retrieved documents help address knowledge gaps, model-generated responses still suffer from hallucinations that contradict the retrieved information. To mitigate this inconsistency, we propose a Reinforcement Learning framework enhanced with Fine-grained Knowledge Verification (RLFKV). Our method decomposes financial responses into atomic knowledge units and assesses the correctness of each unit to compute the fine-grained faithful reward. This reward offers more precise optimization signals, thereby improving alignment with the retrieved documents. Additionally, to prevent reward hacking (e.g., overly concise replies), we incorporate an informativeness reward that encourages the policy model to retain at least as many knowledge units as the base model. Experiments conducted on the public Financial Data Description (FDD) task and our newly proposed FDD-ANT dataset demonstrate consistent improvements, confirming the effectiveness of our approach.",
      "authors": [
        "Taoye Yin",
        "Haoyuan Hu",
        "Yaxin Fan",
        "Xinhao Chen",
        "Xinya Wu",
        "Kai Deng",
        "Kezun Zhang",
        "Feng Wang"
      ],
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "published": "2026-02-05T14:49:05Z",
      "updated": "2026-02-05T14:49:05Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05723v1",
      "abs_url": "http://arxiv.org/abs/2602.05723v1",
      "summary": "论文提出一种基于强化学习和细粒度知识验证的RAG方法，缓解金融领域的幻觉问题。",
      "key_contributions": [
        "提出RLFKV框架，通过细粒度知识验证提升RAG系统可靠性",
        "引入信息量奖励，防止模型过度简化回答",
        "构建并验证了在FDD-ANT数据集上的有效性"
      ],
      "methodology": "通过将金融回复分解为知识单元，评估其正确性，使用强化学习优化，并增加信息量奖励，提升RAG系统生成结果的忠实度。",
      "tags": [
        "RAG",
        "知识验证",
        "强化学习",
        "金融"
      ],
      "assigned_category": "memory",
      "relevance_score": 9,
      "relevance_reason": "论文核心在于提升RAG系统的可靠性，是RAG领域的重要进展。",
      "analyzed_at": "2026-02-06T06:51:51.513036",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05712v1",
      "title": "Towards Green AI: Decoding the Energy of LLM Inference in Software Development",
      "abstract": "Context: AI-assisted tools are increasingly integrated into software development workflows, but their reliance on large language models (LLMs) introduces substantial computational and energy costs. Understanding and reducing the energy footprint of LLM inference is therefore essential for sustainable software development. Objective: In this study, we conduct a phase-level analysis of LLM inference energy consumption, distinguishing between the (1) prefill, where the model processes the input and builds internal representations, and (2) decoding, where output tokens are generated using the stored state. Method: We investigate six 6B-7B and four 3B-4B transformer-based models, evaluating them on code-centric benchmarks HumanEval for code generation and LongBench for code understanding. Results: Our findings show that, within both parameter groups, models exhibit distinct energy patterns across phases. Furthermore, we observed that increases in prefill cost amplify the energy cost per token during decoding, with amplifications ranging from 1.3% to 51.8% depending on the model. Lastly, three out of ten models demonstrate babbling behavior, adding excessive content to the output that unnecessarily inflates energy consumption. We implemented babbling suppression for code generation, achieving energy savings ranging from 44% to 89% without affecting generation accuracy. Conclusion: These findings show that prefill costs influence decoding, which dominates energy consumption, and that babbling suppression can yield up to 89% energy savings. Reducing inference energy therefore requires both mitigating babbling behavior and limiting impact of prefill on decoding.",
      "authors": [
        "Lola Solovyeva",
        "Fernando Castor"
      ],
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "published": "2026-02-05T14:38:19Z",
      "updated": "2026-02-05T14:38:19Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05712v1",
      "abs_url": "http://arxiv.org/abs/2602.05712v1",
      "summary": "分析LLM推理过程中能源消耗，发现预填充影响解码，并提出抑制冗余生成降低能耗。",
      "key_contributions": [
        "分析LLM推理各阶段的能耗",
        "发现预填充成本影响解码阶段能耗",
        "提出抑制冗余生成的方法，显著降低能耗"
      ],
      "methodology": "评估6B-7B和3B-4B模型在代码生成和理解任务上的能耗，并分析其在不同阶段的能耗模式。",
      "tags": [
        "绿色AI",
        "能源效率",
        "LLM推理",
        "代码生成",
        "软件开发"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 8,
      "relevance_reason": "研究LLM推理能耗，虽然面向软件开发，但能耗是LLM推理的重要方面。",
      "analyzed_at": "2026-02-06T06:51:53.210173",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05711v1",
      "title": "OmniMoE: An Efficient MoE by Orchestrating Atomic Experts at Scale",
      "abstract": "Mixture-of-Experts (MoE) architectures are evolving towards finer granularity to improve parameter efficiency. However, existing MoE designs face an inherent trade-off between the granularity of expert specialization and hardware execution efficiency. We propose OmniMoE, a system-algorithm co-designed framework that pushes expert granularity to its logical extreme. OmniMoE introduces vector-level Atomic Experts, enabling scalable routing and execution within a single MoE layer, while retaining a shared dense MLP branch for general-purpose processing. Although this atomic design maximizes capacity, it poses severe challenges for routing complexity and memory access. To address these, OmniMoE adopts a system-algorithm co-design: (i) a Cartesian Product Router that decomposes the massive index space to reduce routing complexity from O(N) to O(sqrt(N)); and (ii) Expert-Centric Scheduling that inverts the execution order to turn scattered, memory-bound lookups into efficient dense matrix operations. Validated on seven benchmarks, OmniMoE (with 1.7B active parameters) achieves 50.9% zero-shot accuracy across seven benchmarks, outperforming coarse-grained (e.g., DeepSeekMoE) and fine-grained (e.g., PEER) baselines. Crucially, OmniMoE reduces inference latency from 73ms to 6.7ms (a 10.9-fold speedup) compared to PEER, demonstrating that massive-scale fine-grained MoE can be fast and accurate. Our code is open-sourced at https://github.com/flash-algo/omni-moe.",
      "authors": [
        "Jingze Shi",
        "Zhangyang Peng",
        "Yizhang Zhu",
        "Yifan Wu",
        "Guang Liu",
        "Yuyu Luo"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "published": "2026-02-05T14:37:32Z",
      "updated": "2026-02-05T14:37:32Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05711v1",
      "abs_url": "http://arxiv.org/abs/2602.05711v1",
      "summary": "OmniMoE通过原子专家和系统算法协同设计，实现了高效细粒度MoE，显著提升了推理速度和准确性。",
      "key_contributions": [
        "提出向量级原子专家概念",
        "设计笛卡尔积路由，降低路由复杂度",
        "提出专家中心调度，优化内存访问"
      ],
      "methodology": "通过原子专家最大化模型容量，采用系统算法协同设计，优化路由和内存访问，实现高效MoE。",
      "tags": [
        "MoE",
        "专家混合模型",
        "参数效率",
        "推理加速"
      ],
      "assigned_category": "agent_tuning",
      "relevance_score": 7,
      "relevance_reason": "虽然专注于MoE，但其效率提升方法对Agent tuning有参考价值。",
      "analyzed_at": "2026-02-06T06:51:54.955389",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05710v1",
      "title": "Ethology of Latent Spaces",
      "abstract": "This study challenges the presumed neutrality of latent spaces in vision language models (VLMs) by adopting an ethological perspective on their algorithmic behaviors. Rather than constituting spaces of homogeneous indeterminacy, latent spaces exhibit model-specific algorithmic sensitivities, understood as differential regimes of perceptual salience shaped by training data and architectural choices.   Through a comparative analysis of three models (OpenAI CLIP, OpenCLIP LAION, SigLIP) applied to a corpus of 301 artworks (15th to 20th), we reveal substantial divergences in the attribution of political and cultural categories. Using bipolar semantic axes derived from vector analogies (Mikolov et al., 2013), we show that SigLIP classifies 59.4% of the artworks as politically engaged, compared to only 4% for OpenCLIP. African masks receive the highest political scores in SigLIP while remaining apolitical in OpenAI CLIP. On an aesthetic colonial axis, inter-model discrepancies reach 72.6 percentage points.   We introduce three operational concepts: computational latent politicization, describing the emergence of political categories without intentional encoding; emergent bias, irreducible to statistical or normative bias and detectable only through contrastive analysis; and three algorithmic scopic regimes: entropic (LAION), institutional (OpenAI), and semiotic (SigLIP), which structure distinct modes of visibility. Drawing on Foucault's notion of the archive, Jameson's ideologeme, and Simondon's theory of individuation, we argue that training datasets function as quasi-archives whose discursive formations crystallize within latent space. This work contributes to a critical reassessment of the conditions under which VLMs are applied to digital art history and calls for methodologies that integrate learning architectures into any delegation of cultural interpretation to algorithmic agents.",
      "authors": [
        "Philippe Boisnard"
      ],
      "categories": [
        "cs.CY",
        "cs.CL",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "published": "2026-02-05T14:37:31Z",
      "updated": "2026-02-05T14:37:31Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05710v1",
      "abs_url": "http://arxiv.org/abs/2602.05710v1",
      "summary": "该论文研究了视觉语言模型(VLM)潜在空间的政治和文化倾向，揭示了模型间的显著差异。",
      "key_contributions": [
        "揭示了VLM潜在空间并非中性，存在模型特定敏感性",
        "提出了计算潜在政治化、涌现偏差等概念",
        "分析了不同VLM的算法视野模式"
      ],
      "methodology": "对比分析了OpenAI CLIP, OpenCLIP LAION, SigLIP三个模型在艺术作品数据集上的表现，使用向量类比提取语义轴。",
      "tags": [
        "视觉语言模型",
        "潜在空间",
        "政治倾向",
        "文化倾向",
        "算法偏见"
      ],
      "assigned_category": "multimodal",
      "relevance_score": 9,
      "relevance_reason": "论文直接研究了视觉语言模型的潜在空间表示及其中存在的偏见。",
      "analyzed_at": "2026-02-06T06:51:56.620075",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05708v1",
      "title": "Cost-Efficient RAG for Entity Matching with LLMs: A Blocking-based Exploration",
      "abstract": "Retrieval-augmented generation (RAG) enhances LLM reasoning in knowledge-intensive tasks, but existing RAG pipelines incur substantial retrieval and generation overhead when applied to large-scale entity matching. To address this limitation, we introduce CE-RAG4EM, a cost-efficient RAG architecture that reduces computation through blocking-based batch retrieval and generation. We also present a unified framework for analyzing and evaluating RAG systems for entity matching, focusing on blocking-aware optimizations and retrieval granularity. Extensive experiments suggest that CE-RAG4EM can achieve comparable or improved matching quality while substantially reducing end-to-end runtime relative to strong baselines. Our analysis further reveals that key configuration parameters introduce an inherent trade-off between performance and overhead, offering practical guidance for designing efficient and scalable RAG systems for entity matching and data integration.",
      "authors": [
        "Chuangtao Ma",
        "Zeyu Zhang",
        "Arijit Khan",
        "Sebastian Schelter",
        "Paul Groth"
      ],
      "categories": [
        "cs.DB",
        "cs.CL"
      ],
      "primary_category": "cs.DB",
      "published": "2026-02-05T14:33:00Z",
      "updated": "2026-02-05T14:33:00Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05708v1",
      "abs_url": "http://arxiv.org/abs/2602.05708v1",
      "summary": "提出了基于分块的低成本RAG架构CE-RAG4EM，用于提升实体匹配效率。",
      "key_contributions": [
        "提出了一种基于分块的成本效益型RAG架构CE-RAG4EM",
        "提出了一个统一的实体匹配RAG系统分析与评估框架",
        "分析了性能和开销之间的权衡，为设计高效RAG系统提供指导"
      ],
      "methodology": "采用基于分块的批量检索和生成方法，减少计算开销，并对检索粒度进行优化。",
      "tags": [
        "RAG",
        "Entity Matching",
        "Blocking",
        "Cost-Efficient",
        "LLM"
      ],
      "assigned_category": "memory",
      "relevance_score": 9,
      "relevance_reason": "论文核心在于优化RAG流程，提高效率，直接研究RAG在实体匹配中的应用。",
      "analyzed_at": "2026-02-06T06:51:58.404942",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05695v1",
      "title": "Determining Energy Efficiency Sweet Spots in Production LLM Inference",
      "abstract": "Large Language Models (LLMs) inference is central in modern AI applications, making it critical to understand their energy footprint. Existing approaches typically estimate energy consumption through simple linear functions of input and output sequence lengths, yet our observations reveal clear Energy Efficiency regimes: peak efficiency occurs with short-to-moderate inputs and medium-length outputs, while efficiency drops sharply for long inputs or very short outputs, indicating a non-linear dependency. In this work, we propose an analytical model derived from the computational and memory-access complexity of the Transformer architecture, capable of accurately characterizing the efficiency curve as a function of input and output lengths. To assess its accuracy, we evaluate energy consumption using TensorRT-LLM on NVIDIA H100 GPUs across a diverse set of LLMs ranging from 1B to 9B parameters, including OPT, LLaMA, Gemma, Falcon, Qwen2, and Granite, tested over input and output lengths from 64 to 4096 tokens, achieving a mean MAPE of 1.79%. Our results show that aligning sequence lengths with these efficiency \"Sweet Spots\" can substantially reduce energy usage, supporting informed truncation, summarization, and adaptive generation strategies in production systems.",
      "authors": [
        "Hiari Pizzini Cavagna",
        "Andrea Proia",
        "Giacomo Madella",
        "Giovanni B. Esposito",
        "Francesco Antici",
        "Daniele Cesarini",
        "Zeynep Kiziltan",
        "Andrea Bartolini"
      ],
      "categories": [
        "cs.AI",
        "cs.PF"
      ],
      "primary_category": "cs.AI",
      "published": "2026-02-05T14:21:00Z",
      "updated": "2026-02-05T14:21:00Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05695v1",
      "abs_url": "http://arxiv.org/abs/2602.05695v1",
      "summary": "该论文分析了LLM推理中的能源效率，发现存在最佳效率区间，并提出了一个预测能源效率的模型。",
      "key_contributions": [
        "发现LLM推理存在能源效率最佳区间",
        "提出基于Transformer架构的能源效率预测模型",
        "在多种LLM和GPU上验证了模型的准确性"
      ],
      "methodology": "通过分析Transformer架构的计算和内存访问复杂度，建立解析模型，并通过实验验证模型准确性。",
      "tags": [
        "LLM",
        "Energy Efficiency",
        "Inference",
        "Transformer",
        "TensorRT-LLM"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 8,
      "relevance_reason": "直接研究LLM推理过程中的能效问题，对优化推理效率有重要意义。",
      "analyzed_at": "2026-02-06T06:52:00.084292",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05690v1",
      "title": "Almost Asymptotically Optimal Active Clustering Through Pairwise Observations",
      "abstract": "We propose a new analysis framework for clustering $M$ items into an unknown number of $K$ distinct groups using noisy and actively collected responses. At each time step, an agent is allowed to query pairs of items and observe bandit binary feedback. If the pair of items belongs to the same (resp.\\ different) cluster, the observed feedback is $1$ with probability $p>1/2$ (resp.\\ $q<1/2$). Leveraging the ubiquitous change-of-measure technique, we establish a fundamental lower bound on the expected number of queries needed to achieve a desired confidence in the clustering accuracy, formulated as a sup-inf optimization problem. Building on this theoretical foundation, we design an asymptotically optimal algorithm in which the stopping criterion involves an empirical version of the inner infimum -- the Generalized Likelihood Ratio (GLR) statistic -- being compared to a threshold. We develop a computationally feasible variant of the GLR statistic and show that its performance gap to the lower bound can be accurately empirically estimated and remains within a constant multiple of the lower bound.",
      "authors": [
        "Rachel S. Y. Teo",
        "P. N. Karthik",
        "Ramya Korlakai Vinayak",
        "Vincent Y. F. Tan"
      ],
      "categories": [
        "cs.LG",
        "cs.IT"
      ],
      "primary_category": "cs.LG",
      "published": "2026-02-05T14:16:47Z",
      "updated": "2026-02-05T14:16:47Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05690v1",
      "abs_url": "http://arxiv.org/abs/2602.05690v1",
      "summary": "提出了一种通过成对观测进行主动聚类的新框架，并设计了渐近最优算法。",
      "key_contributions": [
        "提出了主动聚类分析的新框架",
        "建立了聚类准确性的查询下界",
        "设计了渐近最优的主动聚类算法"
      ],
      "methodology": "利用改变测度技术，建立下界，并设计基于广义似然比（GLR）统计量的算法，通过与阈值比较来停止查询。",
      "tags": [
        "主动学习",
        "聚类",
        "成对比较",
        "bandit反馈",
        "广义似然比"
      ],
      "assigned_category": "agent",
      "relevance_score": 6,
      "relevance_reason": "算法设计中涉及到主动查询和反馈，与agent有一定关联。",
      "analyzed_at": "2026-02-06T06:52:01.800040",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05687v1",
      "title": "Exploring AI-Augmented Sensemaking of Patient-Generated Health Data: A Mixed-Method Study with Healthcare Professionals in Cardiac Risk Reduction",
      "abstract": "Individuals are increasingly generating substantial personal health and lifestyle data, e.g. through wearables and smartphones. While such data could transform preventative care, its integration into clinical practice is hindered by its scale, heterogeneity and the time pressure and data literacy of healthcare professionals (HCPs). We explore how large language models (LLMs) can support sensemaking of patient-generated health data (PGHD) with automated summaries and natural language data exploration. Using cardiovascular disease (CVD) risk reduction as a use case, 16 HCPs reviewed multimodal PGHD in a mixed-methods study with a prototype that integrated common charts, LLM-generated summaries, and a conversational interface. Findings show that AI summaries provided quick overviews that anchored exploration, while conversational interaction supported flexible analysis and bridged data-literacy gaps. However, HCPs raised concerns about transparency, privacy, and overreliance. We contribute empirical insights and sociotechnical design implications for integrating AI-driven summarization and conversation into clinical workflows to support PGHD sensemaking.",
      "authors": [
        "Pavithren V S Pakianathan",
        "Rania Islambouli",
        "Diogo Branco",
        "Albrecht Schmidt",
        "Tiago Guerreiro",
        "Jan David Smeddinck"
      ],
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "published": "2026-02-05T14:11:34Z",
      "updated": "2026-02-05T14:11:34Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05687v1",
      "abs_url": "http://arxiv.org/abs/2602.05687v1",
      "summary": "研究了LLM如何辅助医护人员理解患者健康数据，提升临床决策效率。",
      "key_contributions": [
        "评估了LLM在PGHD理解中的应用",
        "提出了AI辅助临床工作流的设计建议",
        "揭示了医护人员对AI应用的顾虑"
      ],
      "methodology": "混合方法研究，16位医护人员使用集成LLM的系统，结合定量和定性数据分析其使用体验和效果。",
      "tags": [
        "LLM",
        "PGHD",
        "医疗健康",
        "人机交互",
        "临床决策"
      ],
      "assigned_category": "multimodal",
      "relevance_score": 8,
      "relevance_reason": "涉及LLM处理多模态患者数据，辅助医疗决策。",
      "analyzed_at": "2026-02-06T06:52:03.654704",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05679v1",
      "title": "Perception-Based Beliefs for POMDPs with Visual Observations",
      "abstract": "Partially observable Markov decision processes (POMDPs) are a principled planning model for sequential decision-making under uncertainty. Yet, real-world problems with high-dimensional observations, such as camera images, remain intractable for traditional belief- and filtering-based solvers. To tackle this problem, we introduce the Perception-based Beliefs for POMDPs framework (PBP), which complements such solvers with a perception model. This model takes the form of an image classifier which maps visual observations to probability distributions over states. PBP incorporates these distributions directly into belief updates, so the underlying solver does not need to reason explicitly over high-dimensional observation spaces. We show that the belief update of PBP coincides with the standard belief update if the image classifier is exact. Moreover, to handle classifier imprecision, we incorporate uncertainty quantification and introduce two methods to adjust the belief update accordingly. We implement PBP using two traditional POMDP solvers and empirically show that (1) it outperforms existing end-to-end deep RL methods and (2) uncertainty quantification improves robustness of PBP against visual corruption.",
      "authors": [
        "Miriam Schäfers",
        "Merlijn Krale",
        "Thiago D. Simão",
        "Nils Jansen",
        "Maximilian Weininger"
      ],
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "published": "2026-02-05T14:01:39Z",
      "updated": "2026-02-05T14:01:39Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05679v1",
      "abs_url": "http://arxiv.org/abs/2602.05679v1",
      "summary": "PBP框架通过图像分类器将视觉信息融入POMDP信念更新，提升高维观测下决策效率。",
      "key_contributions": [
        "提出感知信念的POMDP框架(PBP)",
        "利用图像分类器概率分布更新信念",
        "引入不确定性量化方法提升鲁棒性"
      ],
      "methodology": "构建图像分类器将视觉观测映射到状态分布，将其融入POMDP信念更新，并量化不确定性。",
      "tags": [
        "POMDP",
        "强化学习",
        "视觉感知",
        "不确定性量化"
      ],
      "assigned_category": "agent",
      "relevance_score": 7,
      "relevance_reason": "论文关注在不确定性环境中，利用视觉信息辅助agent进行决策，与Agent领域相关。",
      "analyzed_at": "2026-02-06T06:52:05.144303",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05665v1",
      "title": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications",
      "abstract": "Memory emerges as the core module in the Large Language Model (LLM)-based agents for long-horizon complex tasks (e.g., multi-turn dialogue, game playing, scientific discovery), where memory can enable knowledge accumulation, iterative reasoning and self-evolution. Among diverse paradigms, graph stands out as a powerful structure for agent memory due to the intrinsic capabilities to model relational dependencies, organize hierarchical information, and support efficient retrieval. This survey presents a comprehensive review of agent memory from the graph-based perspective. First, we introduce a taxonomy of agent memory, including short-term vs. long-term memory, knowledge vs. experience memory, non-structural vs. structural memory, with an implementation view of graph-based memory. Second, according to the life cycle of agent memory, we systematically analyze the key techniques in graph-based agent memory, covering memory extraction for transforming the data into the contents, storage for organizing the data efficiently, retrieval for retrieving the relevant contents from memory to support reasoning, and evolution for updating the contents in the memory. Third, we summarize the open-sourced libraries and benchmarks that support the development and evaluation of self-evolving agent memory. We also explore diverse application scenarios. Finally, we identify critical challenges and future research directions. This survey aims to offer actionable insights to advance the development of more efficient and reliable graph-based agent memory systems. All the related resources, including research papers, open-source data, and projects, are collected for the community in https://github.com/DEEP-PolyU/Awesome-GraphMemory.",
      "authors": [
        "Chang Yang",
        "Chuang Zhou",
        "Yilin Xiao",
        "Su Dong",
        "Luyao Zhuang",
        "Yujing Zhang",
        "Zhu Wang",
        "Zijin Hong",
        "Zheng Yuan",
        "Zhishang Xiang",
        "Shengyuan Chen",
        "Huachi Zhou",
        "Qinggang Zhang",
        "Ninghao Liu",
        "Jinsong Su",
        "Xinrun Wang",
        "Yi Chang",
        "Xiao Huang"
      ],
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "published": "2026-02-05T13:49:05Z",
      "updated": "2026-02-05T13:49:05Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05665v1",
      "abs_url": "http://arxiv.org/abs/2602.05665v1",
      "summary": "该论文综述了基于图结构的LLM Agent记忆，涵盖其分类、技术和应用。",
      "key_contributions": [
        "提出了Agent记忆的分类体系",
        "系统分析了基于图的Agent记忆的关键技术",
        "总结了开源库、基准以及应用场景"
      ],
      "methodology": "该论文通过文献调研和整理，对基于图的Agent记忆进行了全面的回顾和分析，并总结了未来研究方向。",
      "tags": [
        "Agent Memory",
        "Graph",
        "LLM",
        "Survey"
      ],
      "assigned_category": "agent",
      "relevance_score": 9,
      "relevance_reason": "直接研究了AI Agent中记忆的关键技术：图记忆。",
      "analyzed_at": "2026-02-06T06:52:06.758683",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05656v1",
      "title": "Alignment Verifiability in Large Language Models: Normative Indistinguishability under Behavioral Evaluation",
      "abstract": "Behavioral evaluation is the dominant paradigm for assessing alignment in large language models (LLMs). In practice, alignment is inferred from performance under finite evaluation protocols - benchmarks, red-teaming suites, or automated pipelines - and observed compliance is often treated as evidence of underlying alignment. This inference step, from behavioral evidence to claims about latent alignment properties, is typically implicit and rarely analyzed as an inference problem in its own right.   We study this problem formally. We frame alignment evaluation as an identifiability question under partial observability and allow agent behavior to depend on information correlated with the evaluation regime. Within this setting, we introduce the Alignment Verifiability Problem and the notion of Normative Indistinguishability, capturing when distinct latent alignment hypotheses induce identical distributions over all evaluator-accessible signals.   Our main result is a negative but sharply delimited identifiability theorem. Under finite behavioral evaluation and evaluation-aware agents, observed behavioral compliance does not uniquely identify latent alignment. That is, even idealized behavioral evaluation cannot, in general, certify alignment as a latent property.   We further show that behavioral alignment tests should be interpreted as estimators of indistinguishability classes rather than verifiers of alignment. Passing increasingly stringent tests may reduce the space of compatible hypotheses, but cannot collapse it to a singleton under the stated conditions. This reframes alignment benchmarks as providing upper bounds on observable compliance within a regime, rather than guarantees of underlying alignment.",
      "authors": [
        "Igor Santos-Grueiro"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "published": "2026-02-05T13:40:56Z",
      "updated": "2026-02-05T13:40:56Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05656v1",
      "abs_url": "http://arxiv.org/abs/2602.05656v1",
      "summary": "探讨了有限行为评估下LLM对齐的可验证性问题，提出对齐检验应视为对不可区分类别的估计。",
      "key_contributions": [
        "形式化了LLM对齐评估中的可识别性问题",
        "引入了“规范不可区分性”的概念",
        "证明了在有限行为评估下，观察到的行为一致性无法唯一识别潜在的对齐"
      ],
      "methodology": "通过形式化建模，将对齐评估转化为部分可观测下的可识别性问题，并进行理论分析。",
      "tags": [
        "LLM对齐",
        "行为评估",
        "可识别性",
        "规范不可区分性"
      ],
      "assigned_category": "agent",
      "relevance_score": 8,
      "relevance_reason": "论文探讨了LLM的行为评估，这直接关系到Agent的安全性和可靠性。",
      "analyzed_at": "2026-02-06T06:52:08.909036",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05649v1",
      "title": "End-to-End Compression for Tabular Foundation Models",
      "abstract": "The long-standing dominance of gradient-boosted decision trees for tabular data has recently been challenged by in-context learning tabular foundation models. In-context learning methods fit and predict in one forward pass without parameter updates by leveraging the training data as context for predicting on query test points. While recent tabular foundation models achieve state-of-the-art performance, their transformer architecture based on the attention mechanism has quadratic complexity regarding dataset size, which in turn increases the overhead on training and inference time, and limits the capacity of the models to handle large-scale datasets. In this work, we propose TACO, an end-to-end tabular compression model that compresses the training dataset in a latent space. We test our method on the TabArena benchmark, where our proposed method is up to 94x faster in inference time, while consuming up to 97\\% less memory compared to the state-of-the-art tabular transformer architecture, all while retaining performance without significant degradation. Lastly, our method not only scales better with increased dataset sizes, but it also achieves better performance compared to other baselines.",
      "authors": [
        "Guri Zabërgja",
        "Rafiq Kamel",
        "Arlind Kadra",
        "Christian M. M. Frey",
        "Josif Grabocka"
      ],
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "published": "2026-02-05T13:33:58Z",
      "updated": "2026-02-05T13:33:58Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05649v1",
      "abs_url": "http://arxiv.org/abs/2602.05649v1",
      "summary": "提出TACO模型，通过压缩训练数据集在潜在空间中加速和压缩tabular foundation model。",
      "key_contributions": [
        "提出了一种端到端表格数据压缩模型TACO",
        "实现了更快的推理速度和更低的内存消耗",
        "在TabArena基准测试中验证了TACO的有效性"
      ],
      "methodology": "通过在潜在空间中压缩训练数据集，减少模型复杂度和存储需求，提高推理效率。",
      "tags": [
        "tabular data",
        "compression",
        "foundation model",
        "in-context learning"
      ],
      "assigned_category": "memory",
      "relevance_score": 7,
      "relevance_reason": "通过压缩数据集，提高模型的推理效率，类似于压缩 memory 的思路。",
      "analyzed_at": "2026-02-06T06:52:10.419977",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05636v1",
      "title": "Generative Ontology: When Structured Knowledge Learns to Create",
      "abstract": "Traditional ontologies excel at describing domain structure but cannot generate novel artifacts. Large language models generate fluently but produce outputs that lack structural validity, hallucinating mechanisms without components, goals without end conditions. We introduce Generative Ontology, a framework that synthesizes these complementary strengths: ontology provides the grammar; the LLM provides the creativity.   Generative Ontology encodes domain knowledge as executable Pydantic schemas that constrain LLM generation via DSPy signatures. A multi-agent pipeline assigns specialized roles to different ontology domains: a Mechanics Architect designs game systems, a Theme Weaver integrates narrative, a Balance Critic identifies exploits. Each agent carrying a professional \"anxiety\" that prevents shallow, agreeable outputs. Retrieval-augmented generation grounds novel designs in precedents from existing exemplars, while iterative validation ensures coherence between mechanisms and components.   We demonstrate the framework through GameGrammar, a system for generating complete tabletop game designs. Given a thematic prompt (\"bioluminescent fungi competing in a cave ecosystem\"), the pipeline produces structurally complete, playable game specifications with mechanisms, components, victory conditions, and setup instructions. These outputs satisfy ontological constraints while remaining genuinely creative.   The pattern generalizes beyond games. Any domain with expert vocabulary, validity constraints, and accumulated exemplars (music composition, software architecture, culinary arts) is a candidate for Generative Ontology. We argue that constraints do not limit creativity but enable it: just as grammar makes poetry possible, ontology makes structured generation possible.",
      "authors": [
        "Benny Cheung"
      ],
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "published": "2026-02-05T13:14:20Z",
      "updated": "2026-02-05T13:14:20Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05636v1",
      "abs_url": "http://arxiv.org/abs/2602.05636v1",
      "summary": "Generative Ontology结合本体知识和LLM，生成结构化的创造性内容。",
      "key_contributions": [
        "提出了Generative Ontology框架，结合本体和LLM的优势",
        "使用Pydantic schemas约束LLM生成，保证结构有效性",
        "实现了GameGrammar系统，生成完整的桌面游戏设计"
      ],
      "methodology": "使用executable Pydantic schemas约束LLM生成，构建多智能体pipeline，检索增强生成，迭代验证。",
      "tags": [
        "ontology",
        "LLM",
        "generation",
        "structured knowledge",
        "game design"
      ],
      "assigned_category": "agent",
      "relevance_score": 9,
      "relevance_reason": "论文核心在于利用多智能体和本体知识指导LLM生成，解决agent中结构化生成的问题。",
      "analyzed_at": "2026-02-06T06:52:12.679865",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05625v1",
      "title": "Reactive Knowledge Representation and Asynchronous Reasoning",
      "abstract": "Exact inference in complex probabilistic models often incurs prohibitive computational costs. This challenge is particularly acute for autonomous agents in dynamic environments that require frequent, real-time belief updates. Existing methods are often inefficient for ongoing reasoning, as they re-evaluate the entire model upon any change, failing to exploit that real-world information streams have heterogeneous update rates. To address this, we approach the problem from a reactive, asynchronous, probabilistic reasoning perspective. We first introduce Resin (Reactive Signal Inference), a probabilistic programming language that merges probabilistic logic with reactive programming. Furthermore, to provide efficient and exact semantics for Resin, we propose Reactive Circuits (RCs). Formulated as a meta-structure over Algebraic Circuits and asynchronous data streams, RCs are time-dynamic Directed Acyclic Graphs that autonomously adapt themselves based on the volatility of input signals. In high-fidelity drone swarm simulations, our approach achieves several orders of magnitude of speedup over frequency-agnostic inference. We demonstrate that RCs' structural adaptations successfully capture environmental dynamics, significantly reducing latency and facilitating reactive real-time reasoning. By partitioning computations based on the estimated Frequency of Change in the asynchronous inputs, large inference tasks can be decomposed into individually memoized sub-problems. This ensures that only the specific components of a model affected by new information are re-evaluated, drastically reducing redundant computation in streaming contexts.",
      "authors": [
        "Simon Kohaut",
        "Benedict Flade",
        "Julian Eggert",
        "Kristian Kersting",
        "Devendra Singh Dhami"
      ],
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "published": "2026-02-05T13:02:01Z",
      "updated": "2026-02-05T13:02:01Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05625v1",
      "abs_url": "http://arxiv.org/abs/2602.05625v1",
      "summary": "提出了用于动态环境下的反应式异步概率推理框架Resin及高效实现Reactive Circuits。",
      "key_contributions": [
        "提出了概率编程语言Resin",
        "提出了Reactive Circuits用于高效推理",
        "验证了在无人机集群模拟中的加速效果"
      ],
      "methodology": "结合概率逻辑与反应式编程，构建基于代数电路和异步数据流的动态有向无环图。",
      "tags": [
        "概率推理",
        "异步计算",
        "反应式编程",
        "动态环境"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 8,
      "relevance_reason": "论文关注概率推理和动态环境下的实时推理，与LLM的推理能力密切相关。",
      "analyzed_at": "2026-02-06T06:52:14.439918",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05605v1",
      "title": "Shiva-DiT: Residual-Based Differentiable Top-$k$ Selection for Efficient Diffusion Transformers",
      "abstract": "Diffusion Transformers (DiTs) incur prohibitive computational costs due to the quadratic scaling of self-attention. Existing pruning methods fail to simultaneously satisfy differentiability, efficiency, and the strict static budgets required for hardware overhead. To address this, we propose Shiva-DiT, which effectively reconciles these conflicting requirements via Residual-Based Differentiable Top-$k$ Selection. By leveraging a residual-aware straight-through estimator, our method enforces deterministic token counts for static compilation while preserving end-to-end learnability through residual gradient estimation. Furthermore, we introduce a Context-Aware Router and Adaptive Ratio Policy to autonomously learn an adaptive pruning schedule. Experiments on mainstream models, including SD3.5, demonstrate that Shiva-DiT establishes a new Pareto frontier, achieving a 1.54$\\times$ wall-clock speedup with superior fidelity compared to existing baselines, effectively eliminating ragged tensor overheads.",
      "authors": [
        "Jiaji Zhang",
        "Hailiang Zhao",
        "Guoxuan Zhu",
        "Ruichao Sun",
        "Jiaju Wu",
        "Xinkui Zhao",
        "Hanlin Tang",
        "Weiyi Lu",
        "Kan Liu",
        "Tao Lan",
        "Lin Qu",
        "Shuiguang Deng"
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "published": "2026-02-05T12:42:22Z",
      "updated": "2026-02-05T12:42:22Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05605v1",
      "abs_url": "http://arxiv.org/abs/2602.05605v1",
      "summary": "Shiva-DiT通过残差学习的可微Top-k选择加速Diffusion Transformer。",
      "key_contributions": [
        "提出基于残差的可微Top-k选择方法，实现高效DiT剪枝",
        "引入上下文感知路由和自适应比率策略，自动学习剪枝策略",
        "在SD3.5上取得1.54倍加速，性能优于现有方法"
      ],
      "methodology": "利用残差感知直通估计器，强制静态token数量，通过残差梯度估计保持端到端可学习性。",
      "tags": [
        "Diffusion Transformer",
        "剪枝",
        "模型加速",
        "可微选择"
      ],
      "assigned_category": "multimodal",
      "relevance_score": 7,
      "relevance_reason": "该论文涉及扩散模型（Diffusion Model），属于多模态生成领域。",
      "analyzed_at": "2026-02-06T06:52:16.109791",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05597v1",
      "title": "Emulating Aggregate Human Choice Behavior and Biases with GPT Conversational Agents",
      "abstract": "Cognitive biases often shape human decisions. While large language models (LLMs) have been shown to reproduce well-known biases, a more critical question is whether LLMs can predict biases at the individual level and emulate the dynamics of biased human behavior when contextual factors, such as cognitive load, interact with these biases. We adapted three well-established decision scenarios into a conversational setting and conducted a human experiment (N=1100). Participants engaged with a chatbot that facilitates decision-making through simple or complex dialogues. Results revealed robust biases. To evaluate how LLMs emulate human decision-making under similar interactive conditions, we used participant demographics and dialogue transcripts to simulate these conditions with LLMs based on GPT-4 and GPT-5. The LLMs reproduced human biases with precision. We found notable differences between models in how they aligned human behavior. This has important implications for designing and evaluating adaptive, bias-aware LLM-based AI systems in interactive contexts.",
      "authors": [
        "Stephen Pilli",
        "Vivek Nallur"
      ],
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "published": "2026-02-05T12:33:05Z",
      "updated": "2026-02-05T12:33:05Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05597v1",
      "abs_url": "http://arxiv.org/abs/2602.05597v1",
      "summary": "论文研究GPT模型在模拟人类决策偏差和交互行为方面的能力，结果表明GPT模型能较好地复现人类偏差。",
      "key_contributions": [
        "验证了GPT模型在交互环境中模拟人类决策偏差的能力",
        "分析了不同GPT模型在对齐人类行为方面的差异",
        "提出了设计偏差感知AI系统的建议"
      ],
      "methodology": "通过人类实验收集数据，并利用GPT-4和GPT-5模型模拟实验情境，对比模型与人类的决策行为。",
      "tags": [
        "LLM",
        "决策偏差",
        "认知偏差",
        "对话智能体",
        "行为模拟"
      ],
      "assigned_category": "agent",
      "relevance_score": 9,
      "relevance_reason": "直接研究LLM作为智能体在模拟人类行为上的能力，属于核心相关。",
      "analyzed_at": "2026-02-06T06:52:18.393553",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05578v1",
      "title": "LoGoSeg: Integrating Local and Global Features for Open-Vocabulary Semantic Segmentation",
      "abstract": "Open-vocabulary semantic segmentation (OVSS) extends traditional closed-set segmentation by enabling pixel-wise annotation for both seen and unseen categories using arbitrary textual descriptions. While existing methods leverage vision-language models (VLMs) like CLIP, their reliance on image-level pretraining often results in imprecise spatial alignment, leading to mismatched segmentations in ambiguous or cluttered scenes. However, most existing approaches lack strong object priors and region-level constraints, which can lead to object hallucination or missed detections, further degrading performance. To address these challenges, we propose LoGoSeg, an efficient single-stage framework that integrates three key innovations: (i) an object existence prior that dynamically weights relevant categories through global image-text similarity, effectively reducing hallucinations; (ii) a region-aware alignment module that establishes precise region-level visual-textual correspondences; and (iii) a dual-stream fusion mechanism that optimally combines local structural information with global semantic context. Unlike prior works, LoGoSeg eliminates the need for external mask proposals, additional backbones, or extra datasets, ensuring efficiency. Extensive experiments on six benchmarks (A-847, PC-459, A-150, PC-59, PAS-20, and PAS-20b) demonstrate its competitive performance and strong generalization in open-vocabulary settings.",
      "authors": [
        "Junyang Chen",
        "Xiangbo Lv",
        "Zhiqiang Kou",
        "Xingdong Sheng",
        "Ning Xu",
        "Yiguo Qiao"
      ],
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2026-02-05T12:03:11Z",
      "updated": "2026-02-05T12:03:11Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05578v1",
      "abs_url": "http://arxiv.org/abs/2602.05578v1",
      "summary": "LoGoSeg通过融合局部和全局特征，实现了高效且泛化性强的开放词汇语义分割。",
      "key_contributions": [
        "提出对象存在先验以减少幻觉",
        "引入区域感知对齐模块以建立区域级视觉-文本对应",
        "提出双流融合机制以结合局部结构信息和全局语义上下文"
      ],
      "methodology": "LoGoSeg使用单阶段框架，结合全局图像-文本相似性、区域感知对齐和双流融合机制，提升开放词汇语义分割效果。",
      "tags": [
        "开放词汇语义分割",
        "视觉语言模型",
        "图像分割"
      ],
      "assigned_category": "multimodal",
      "relevance_score": 9,
      "relevance_reason": "该论文核心内容是视觉语言模型的图像分割任务，属于多模态学习的关键问题。",
      "analyzed_at": "2026-02-06T06:52:20.480872",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05570v1",
      "title": "TangramSR: Can Vision-Language Models Reason in Continuous Geometric Space?",
      "abstract": "Humans excel at spatial reasoning tasks like Tangram puzzle assembly through cognitive processes involving mental rotation, iterative refinement, and visual feedback. Inspired by how humans solve Tangram puzzles through trial-and-error, observation, and correction, we design a framework that models these human cognitive mechanisms. However, comprehensive experiments across five representative Vision-Language Models (VLMs) reveal systematic failures in continuous geometric reasoning: average IoU of only 0.41 on single-piece tasks, dropping to 0.23 on two-piece composition, far below human performance where children can complete Tangram tasks successfully. This paper addresses a fundamental challenge in self-improving AI: can models iteratively refine their predictions at test time without parameter updates? We introduce a test-time self-refinement framework that combines in-context learning (ICL) with reward-guided feedback loops, inspired by human cognitive processes. Our training-free verifier-refiner agent applies recursive refinement loops that iteratively self-refine predictions based on geometric consistency feedback, achieving IoU improvements from 0.63 to 0.932 on medium-triangle cases without any model retraining. This demonstrates that incorporating human-inspired iterative refinement mechanisms through ICL and reward loops can substantially enhance geometric reasoning in VLMs, moving self-improving AI from promise to practice in continuous spatial domains. Our work is available at this anonymous link https://anonymous.4open.science/r/TangramVLM-F582/.",
      "authors": [
        "Yikun Zong",
        "Cheston Tan"
      ],
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "published": "2026-02-05T11:49:30Z",
      "updated": "2026-02-05T11:49:30Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05570v1",
      "abs_url": "http://arxiv.org/abs/2602.05570v1",
      "summary": "该论文提出了一种迭代精炼框架，提升视觉语言模型在几何空间推理方面的能力。",
      "key_contributions": [
        "设计了模拟人类认知机制的迭代精炼框架",
        "通过无训练的验证-精炼代理，显著提升了几何推理的IoU",
        "揭示了现有VLMs在连续几何空间推理方面的局限性"
      ],
      "methodology": "结合上下文学习和奖励引导的反馈循环，设计训练自由的验证-精炼代理，通过递归精炼循环迭代优化预测。",
      "tags": [
        "视觉语言模型",
        "几何推理",
        "迭代精炼",
        "上下文学习",
        "反馈循环"
      ],
      "assigned_category": "multimodal",
      "relevance_score": 9,
      "relevance_reason": "论文核心关注视觉语言模型的几何空间推理能力，属于多模态学习的关键问题。",
      "analyzed_at": "2026-02-06T06:52:22.168015",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05552v1",
      "title": "VLN-Pilot: Large Vision-Language Model as an Autonomous Indoor Drone Operator",
      "abstract": "This paper introduces VLN-Pilot, a novel framework in which a large Vision-and-Language Model (VLLM) assumes the role of a human pilot for indoor drone navigation. By leveraging the multimodal reasoning abilities of VLLMs, VLN-Pilot interprets free-form natural language instructions and grounds them in visual observations to plan and execute drone trajectories in GPS-denied indoor environments. Unlike traditional rule-based or geometric path-planning approaches, our framework integrates language-driven semantic understanding with visual perception, enabling context-aware, high-level flight behaviors with minimal task-specific engineering. VLN-Pilot supports fully autonomous instruction-following for drones by reasoning about spatial relationships, obstacle avoidance, and dynamic reactivity to unforeseen events. We validate our framework on a custom photorealistic indoor simulation benchmark and demonstrate the ability of the VLLM-driven agent to achieve high success rates on complex instruction-following tasks, including long-horizon navigation with multiple semantic targets. Experimental results highlight the promise of replacing remote drone pilots with a language-guided autonomous agent, opening avenues for scalable, human-friendly control of indoor UAVs in tasks such as inspection, search-and-rescue, and facility monitoring. Our results suggest that VLLM-based pilots may dramatically reduce operator workload while improving safety and mission flexibility in constrained indoor environments.",
      "authors": [
        "Bessie Dominguez-Dager",
        "Sergio Suescun-Ferrandiz",
        "Felix Escalona",
        "Francisco Gomez-Donoso",
        "Miguel Cazorla"
      ],
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "published": "2026-02-05T11:23:11Z",
      "updated": "2026-02-05T11:23:11Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05552v1",
      "abs_url": "http://arxiv.org/abs/2602.05552v1",
      "summary": "VLN-Pilot利用大型视觉语言模型实现室内无人机自主导航，无需人工遥控。",
      "key_contributions": [
        "提出VLN-Pilot框架，利用VLLM控制室内无人机",
        "实现基于自然语言指令的无人机自主导航",
        "在逼真的室内模拟环境中验证了框架的有效性"
      ],
      "methodology": "利用VLLM理解自然语言指令，结合视觉信息进行路径规划，控制无人机自主飞行，规避障碍。",
      "tags": [
        "VLLM",
        "无人机",
        "自主导航",
        "Vision-Language Navigation"
      ],
      "assigned_category": "multimodal",
      "relevance_score": 9,
      "relevance_reason": "论文核心是利用VLLM进行视觉和语言结合的无人机控制，属于多模态学习关键领域。",
      "analyzed_at": "2026-02-06T06:52:23.881201",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05550v1",
      "title": "ArkTS-CodeSearch: A Open-Source ArkTS Dataset for Code Retrieval",
      "abstract": "ArkTS is a core programming language in the OpenHarmony ecosystem, yet research on ArkTS code intelligence is hindered by the lack of public datasets and evaluation benchmarks. This paper presents a large-scale ArkTS dataset constructed from open-source repositories, targeting code retrieval and code evaluation tasks. We design a single-search task, where natural language comments are used to retrieve corresponding ArkTS functions. ArkTS repositories are crawled from GitHub and Gitee, and comment-function pairs are extracted using tree-sitter-arkts, followed by cross-platform deduplication and statistical analysis of ArkTS function types. We further evaluate all existing open-source code embedding models on the single-search task and perform fine-tuning using both ArkTS and TypeScript training datasets, resulting in a high-performing model for ArkTS code understanding. This work establishes the first systematic benchmark for ArkTS code retrieval. Both the dataset and our fine-tuned model will be released publicly and are available at https://huggingface.co/hreyulog/embedinggemma_arkts and https://huggingface.co/datasets/hreyulog/arkts-code-docstring,establishing the first systematic benchmark for ArkTS code retrieval.",
      "authors": [
        "Yulong He",
        "Artem Ermakov",
        "Sergey Kovalchuk",
        "Artem Aliev",
        "Dmitry Shalymov"
      ],
      "categories": [
        "cs.SE",
        "cs.CL"
      ],
      "primary_category": "cs.SE",
      "published": "2026-02-05T11:15:34Z",
      "updated": "2026-02-05T11:15:34Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05550v1",
      "abs_url": "http://arxiv.org/abs/2602.05550v1",
      "summary": "构建了大规模ArkTS代码检索数据集与基准，并进行了模型微调，提升了ArkTS代码理解能力。",
      "key_contributions": [
        "构建了大规模开源ArkTS代码检索数据集",
        "设计了基于自然语言注释的代码检索任务",
        "对现有代码嵌入模型进行了微调，提高了ArkTS代码理解性能"
      ],
      "methodology": "从GitHub和Gitee抓取ArkTS代码仓库，使用tree-sitter-arkts提取注释-函数对，进行去重和统计分析，并微调代码嵌入模型。",
      "tags": [
        "ArkTS",
        "代码检索",
        "数据集",
        "代码嵌入",
        "代码智能"
      ],
      "assigned_category": "memory",
      "relevance_score": 8,
      "relevance_reason": "该论文核心是构建数据集用于代码检索，属于检索增强生成的相关领域。",
      "analyzed_at": "2026-02-06T06:52:26.053431",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05548v1",
      "title": "Unveiling Implicit Advantage Symmetry: Why GRPO Struggles with Exploration and Difficulty Adaptation",
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR), particularly GRPO, has become the standard for eliciting LLM reasoning. However, its efficiency in exploration and difficulty adaptation remains an open challenge. In this work, we argue that these bottlenecks stem from an implicit advantage symmetry inherent in Group Relative Advantage Estimation (GRAE). This symmetry induces two critical limitations: (i) at the group level, strict symmetry in weights between correct and incorrect trajectories leaves unsampled action logits unchanged, thereby hindering exploration of novel correct solution. (ii) at the sample level, the algorithm implicitly prioritizes medium-difficulty samples, remaining agnostic to the non-stationary demands of difficulty focus. Through controlled experiments, we reveal that this symmetric property is sub-optimal, yielding two pivotal insights: (i) asymmetrically suppressing the advantages of correct trajectories encourages essential exploration. (ii) learning efficiency is maximized by a curriculum-like transition-prioritizing simpler samples initially before gradually shifting to complex ones. Motivated by these findings, we propose Asymmetric GRAE (A-GRAE), which dynamically modulates exploration incentives and sample-difficulty focus. Experiments across seven benchmarks demonstrate that A-GRAE consistently improves GRPO and its variants across both LLMs and MLLMs.",
      "authors": [
        "Zhiqi Yu",
        "Zhangquan Chen",
        "Mengting Liu",
        "Heye Zhang",
        "Liangqiong Qu"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "published": "2026-02-05T11:07:14Z",
      "updated": "2026-02-05T11:07:14Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05548v1",
      "abs_url": "http://arxiv.org/abs/2602.05548v1",
      "summary": "论文揭示GRPO在探索和难度适应上的局限性，并提出改进算法A-GRAE。",
      "key_contributions": [
        "发现了Group Relative Advantage Estimation (GRAE) 中的隐含优势对称性问题",
        "提出Asymmetric GRAE (A-GRAE) 算法，动态调整探索激励和样本难度焦点",
        "通过实验证明A-GRAE在多个基准测试中优于 GRPO 及其变体"
      ],
      "methodology": "通过理论分析和控制实验揭示GRPO的局限性，并在此基础上设计改进算法A-GRAE，进行实验验证。",
      "tags": [
        "强化学习",
        "语言模型",
        "推理",
        "探索",
        "难度适应"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 8,
      "relevance_reason": "论文关注LLM推理的效率问题，属于该领域的重要方面。",
      "analyzed_at": "2026-02-06T06:52:27.673142",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05544v1",
      "title": "Reasoning-guided Collaborative Filtering with Language Models for Explainable Recommendation",
      "abstract": "Large Language Models (LLMs) exhibit potential for explainable recommendation systems but overlook collaborative signals, while prevailing methods treat recommendation and explanation as separate tasks, resulting in a memory footprint. We present RGCF-XRec, a hybrid framework that introduces reasoning-guided collaborative filtering (CF) knowledge into a language model to deliver explainable sequential recommendations in a single step. Theoretical grounding and empirical findings reveal that RGCF-XRec offers three key merits over leading CF-aware LLM-based methods: (1) reasoning-guided augmentation of CF knowledge through contextual prompting to discover latent preferences and interpretable reasoning paths; (2) an efficient scoring mechanism based on four dimensions: coherence, completeness, relevance, and consistency to mitigate noisy CF reasoning traces and retain high-quality explanations; (3) a unified representation learning network that encodes collaborative and semantic signals, enabling a structured prompt to condition the LLM for explainable sequential recommendation. RGCF-XRec demonstrates consistent improvements across Amazon datasets, Sports, Toys, and Beauty, comprising 642,503 user-item interactions. It improves HR@10 by 7.38\\% in Sports and 4.59\\% in Toys, along with ROUGE-L by 8.02\\% and 3.49\\%, respectively. It reduces the cold warm performance gap, achieving overall gains of 14.5\\% in cold-start and 11.9\\% in warm start scenarios, and enhances zero-shot HR@5 by 18.54\\% in Beauty and 23.16\\% in Toys, highlighting effective generalization and robustness. Moreover, RGCF-XRec achieves training efficiency with a lightweight LLaMA 3.2-3B backbone, ensuring scalability for real-world applications.",
      "authors": [
        "Fahad Anwaar",
        "Adil Mehmood Khan",
        "Muhammad Khalid",
        "Usman Zia",
        "Kezhi Wang"
      ],
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "published": "2026-02-05T11:05:09Z",
      "updated": "2026-02-05T11:05:09Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05544v1",
      "abs_url": "http://arxiv.org/abs/2602.05544v1",
      "summary": "提出RGCF-XRec，利用语言模型和协同过滤知识，实现可解释的序列推荐，提升效果和效率。",
      "key_contributions": [
        "提出 reasoning-guided CF 知识增强方法",
        "高效的四维度评分机制过滤噪声",
        "统一表示学习网络编码协同和语义信号"
      ],
      "methodology": "构建混合框架RGCF-XRec，通过上下文提示增强CF知识，利用四维度评分机制和统一表示网络，实现可解释的序列推荐。",
      "tags": [
        "推荐系统",
        "可解释性",
        "语言模型",
        "协同过滤"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 8,
      "relevance_reason": "论文使用语言模型进行推理并生成可解释的推荐，与LLM推理密切相关。",
      "analyzed_at": "2026-02-06T06:52:29.418783",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05524v1",
      "title": "AI Agent Systems for Supply Chains: Structured Decision Prompts and Memory Retrieval",
      "abstract": "This study investigates large language model (LLM) -based multi-agent systems (MASs) as a promising approach to inventory management, which is a key component of supply chain management. Although these systems have gained considerable attention for their potential to address the challenges associated with typical inventory management methods, key uncertainties regarding their effectiveness persist. Specifically, it is unclear whether LLM-based MASs can consistently derive optimal ordering policies and adapt to diverse supply chain scenarios. To address these questions, we examine an LLM-based MAS with a fixed-ordering strategy prompt that encodes the stepwise processes of the problem setting and a safe-stock strategy commonly used in inventory management. Our empirical results demonstrate that, even without detailed prompt adjustments, an LLM-based MAS can determine optimal ordering decisions in a restricted scenario. To enhance adaptability, we propose a novel agent called AIM-RM, which leverages similar historical experiences through similarity matching. Our results show that AIM-RM outperforms benchmark methods across various supply chain scenarios, highlighting its robustness and adaptability.",
      "authors": [
        "Konosuke Yoshizato",
        "Kazuma Shimizu",
        "Ryota Higa",
        "Takanobu Otsuka"
      ],
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "published": "2026-02-05T10:35:00Z",
      "updated": "2026-02-05T10:35:00Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05524v1",
      "abs_url": "http://arxiv.org/abs/2602.05524v1",
      "summary": "研究基于LLM的多智能体系统在供应链库存管理中的应用，并提出AIM-RM智能体。",
      "key_contributions": [
        "验证LLM-based MAS在特定场景下能做出最优订购决策",
        "提出AIM-RM智能体，通过相似性匹配利用历史经验",
        "实验证明AIM-RM在多种供应链场景下优于基准方法"
      ],
      "methodology": "构建LLM-based MAS，设计固定订购策略prompt和AIM-RM智能体，通过实验评估其在库存管理中的表现。",
      "tags": [
        "LLM",
        "Multi-Agent System",
        "Supply Chain Management",
        "Inventory Management",
        "Memory Retrieval"
      ],
      "assigned_category": "agent",
      "relevance_score": 9,
      "relevance_reason": "核心研究多智能体系统在特定领域的应用，并提出了新的智能体。",
      "analyzed_at": "2026-02-06T06:52:31.303396",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05523v1",
      "title": "Capture the Flags: Family-Based Evaluation of Agentic LLMs via Semantics-Preserving Transformations",
      "abstract": "Agentic large language models (LLMs) are increasingly evaluated on cybersecurity tasks using capture-the-flag (CTF) benchmarks. However, existing pointwise benchmarks have limited ability to shed light on the robustness and generalisation abilities of agents across alternative versions of the source code. We introduce CTF challenge families, whereby a single CTF is used as the basis for generating a family of semantically-equivalent challenges via semantics-preserving program transformations. This enables controlled evaluation of agent robustness to source code transformations while keeping the underlying exploit strategy fixed. We introduce a new tool, Evolve-CTF, that generates CTF families from Python challenges using a range of transformations. Using Evolve-CTF to derive families from Cybench and Intercode challenges, we evaluate 13 agentic LLM configurations with tool access. We find that models are remarkably robust to intrusive renaming and code insertion-based transformations, but that composed transformations and deeper obfuscation affect performance by requiring more sophisticated use of tools. We also find that enabling explicit reasoning has little effect on solution success rates across challenge families. Our work contributes a valuable technique and tool for future LLM evaluations, and a large dataset characterising the capabilities of current state-of-the-art models in this domain.",
      "authors": [
        "Shahin Honarvar",
        "Amber Gorzynski",
        "James Lee-Jones",
        "Harry Coppock",
        "Marek Rei",
        "Joseph Ryan",
        "Alastair F. Donaldson"
      ],
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "published": "2026-02-05T10:30:57Z",
      "updated": "2026-02-05T10:30:57Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05523v1",
      "abs_url": "http://arxiv.org/abs/2602.05523v1",
      "summary": "提出Evolve-CTF工具，通过语义保持转换生成CTF挑战家族，评估Agentic LLM的鲁棒性。",
      "key_contributions": [
        "提出CTF挑战家族的概念",
        "开发了Evolve-CTF工具",
        "评估了13个Agentic LLM在CTF挑战家族上的表现"
      ],
      "methodology": "使用Evolve-CTF对CTF挑战进行语义保持转换，生成挑战家族，并使用这些家族评估Agentic LLM。",
      "tags": [
        "Agentic LLM",
        "CTF",
        "代码变换",
        "鲁棒性评估",
        "工具使用"
      ],
      "assigned_category": "agent",
      "relevance_score": 9,
      "relevance_reason": "论文核心关注Agentic LLM在网络安全任务中的鲁棒性，属于agent领域的核心问题。",
      "analyzed_at": "2026-02-06T06:52:32.893125",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05515v1",
      "title": "A Unified Multimodal Framework for Dataset Construction and Model-Based Diagnosis of Ameloblastoma",
      "abstract": "Artificial intelligence (AI)-enabled diagnostics in maxillofacial pathology require structured, high-quality multimodal datasets. However, existing resources provide limited ameloblastoma coverage and lack the format consistency needed for direct model training. We present a newly curated multimodal dataset specifically focused on ameloblastoma, integrating annotated radiological, histopathological, and intraoral clinical images with structured data derived from case reports. Natural language processing techniques were employed to extract clinically relevant features from textual reports, while image data underwent domain specific preprocessing and augmentation. Using this dataset, a multimodal deep learning model was developed to classify ameloblastoma variants, assess behavioral patterns such as recurrence risk, and support surgical planning. The model is designed to accept clinical inputs such as presenting complaint, age, and gender during deployment to enhance personalized inference. Quantitative evaluation demonstrated substantial improvements; variant classification accuracy increased from 46.2 percent to 65.9 percent, and abnormal tissue detection F1-score improved from 43.0 percent to 90.3 percent. Benchmarked against resources like MultiCaRe, this work advances patient-specific decision support by providing both a robust dataset and an adaptable multimodal AI framework.",
      "authors": [
        "Ajo Babu George",
        "Anna Mariam John",
        "Athul Anoop",
        "Balu Bhasuran"
      ],
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "published": "2026-02-05T10:15:34Z",
      "updated": "2026-02-05T10:15:34Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05515v1",
      "abs_url": "http://arxiv.org/abs/2602.05515v1",
      "summary": "构建多模态数据集，开发AI模型辅助成釉细胞瘤诊断与治疗决策。",
      "key_contributions": [
        "构建了成釉细胞瘤多模态数据集",
        "开发了基于多模态数据的深度学习模型",
        "提高了成釉细胞瘤的分类和检测精度"
      ],
      "methodology": "采用NLP提取临床特征，进行图像预处理和增强。构建深度学习模型进行多模态数据融合和预测。",
      "tags": [
        "多模态学习",
        "医学图像处理",
        "深度学习",
        "成釉细胞瘤"
      ],
      "assigned_category": "multimodal",
      "relevance_score": 9,
      "relevance_reason": "论文核心在于构建多模态数据集并利用其进行诊断建模。",
      "analyzed_at": "2026-02-06T06:52:34.503633",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05513v1",
      "title": "DECO: Decoupled Multimodal Diffusion Transformer for Bimanual Dexterous Manipulation with a Plugin Tactile Adapter",
      "abstract": "Overview of the Proposed DECO Framework.} DECO is a DiT-based policy that decouples multimodal conditioning. Image and action tokens interact via joint self attention, while proprioceptive states and optional conditions are injected through adaptive layer normalization. Tactile signals are injected via cross attention, while a lightweight LoRA-based adapter is used to efficiently fine-tune the pretrained policy. DECO is also accompanied by DECO-50, a bimanual dexterous manipulation dataset with tactile sensing, consisting of 4 scenarios and 28 sub-tasks, covering more than 50 hours of data, approximately 5 million frames, and 8,000 successful trajectories.",
      "authors": [
        "Xukun Li",
        "Yu Sun",
        "Lei Zhang",
        "Bosheng Huang",
        "Yibo Peng",
        "Yuan Meng",
        "Haojun Jiang",
        "Shaoxuan Xie",
        "Guacai Yao",
        "Alois Knoll",
        "Zhenshan Bing",
        "Xinlong Wang",
        "Zhenguo Sun"
      ],
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "published": "2026-02-05T10:13:34Z",
      "updated": "2026-02-05T10:13:34Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05513v1",
      "abs_url": "http://arxiv.org/abs/2602.05513v1",
      "summary": "DECO提出了一种解耦多模态扩散Transformer，用于灵巧双臂操作。",
      "key_contributions": [
        "提出了DECO框架，用于解耦多模态条件",
        "引入触觉适配器，增强感知能力",
        "构建了DECO-50触觉感知双臂操作数据集"
      ],
      "methodology": "DECO基于DiT架构，解耦图像、动作和状态等模态，通过自注意力、跨注意力和自适应归一化融合信息。",
      "tags": [
        "多模态学习",
        "双臂操作",
        "触觉感知",
        "Diffusion Transformer"
      ],
      "assigned_category": "multimodal",
      "relevance_score": 9,
      "relevance_reason": "论文核心关注多模态信息融合和机器人灵巧操作问题。",
      "analyzed_at": "2026-02-06T06:52:36.091763",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05512v1",
      "title": "A Human-in-the-Loop, LLM-Centered Architecture for Knowledge-Graph Question Answering",
      "abstract": "Large Language Models (LLMs) excel at language understanding but remain limited in knowledge-intensive domains due to hallucinations, outdated information, and limited explainability. Text-based retrieval-augmented generation (RAG) helps ground model outputs in external sources but struggles with multi-hop reasoning. Knowledge Graphs (KGs), in contrast, support precise, explainable querying, yet require a knowledge of query languages. This work introduces an interactive framework in which LLMs generate and explain Cypher graph queries and users iteratively refine them through natural language. Applied to real-world KGs, the framework improves accessibility to complex datasets while preserving factual accuracy and semantic rigor and provides insight into how model performance varies across domains. Our core quantitative evaluation is a 90-query benchmark on a synthetic movie KG that measures query explanation quality and fault detection across multiple LLMs, complemented by two smaller real-life query-generation experiments on a Hyena KG and the MaRDI (Mathematical Research Data Initiative) KG.",
      "authors": [
        "Larissa Pusch",
        "Alexandre Courtiol",
        "Tim Conrad"
      ],
      "categories": [
        "cs.CL",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "published": "2026-02-05T10:10:19Z",
      "updated": "2026-02-05T10:10:19Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05512v1",
      "abs_url": "http://arxiv.org/abs/2602.05512v1",
      "summary": "提出了一种人机协作的LLM知识图谱问答框架，提升知识图谱的可访问性和准确性。",
      "key_contributions": [
        "提出人机协作的问答框架",
        "利用LLM生成和解释Cypher查询",
        "在多个知识图谱上验证了框架有效性"
      ],
      "methodology": "构建交互式框架，LLM生成Cypher查询，用户通过自然语言迭代优化，最终获得准确答案。",
      "tags": [
        "知识图谱",
        "LLM",
        "人机交互",
        "问答系统",
        "Cypher"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 9,
      "relevance_reason": "论文核心在于利用LLM提升知识图谱的推理能力，并加入了人工干预进行优化。",
      "analyzed_at": "2026-02-06T06:52:38.090032",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05499v1",
      "title": "SDFP: Speculative Decoding with FIT-Pruned Models for Training-Free and Plug-and-Play LLM Acceleration",
      "abstract": "Large language models (LLMs) underpin interactive multimedia applications such as captioning, retrieval, recommendation, and creative content generation, yet their autoregressive decoding incurs substantial latency. Speculative decoding reduces latency using a lightweight draft model, but deployment is often limited by the cost and complexity of acquiring, tuning, and maintaining an effective draft model. Recent approaches usually require auxiliary training or specialization, and even training-free methods incur costly search or optimization. We propose SDFP, a fully training-free and plug-and-play framework that builds the draft model via Fisher Information Trace (FIT)-based layer pruning of a given LLM. Using layer sensitivity as a proxy for output perturbation, SDFP removes low-impact layers to obtain a compact draft while preserving compatibility with the original model for standard speculative verification. SDFP needs no additional training, hyperparameter tuning, or separately maintained drafts, enabling rapid, deployment-friendly draft construction. Across benchmarks, SDFP delivers 1.32x-1.5x decoding speedup without altering the target model's output distribution, supporting low-latency multimedia applications.",
      "authors": [
        "Hanyu Wei",
        "Zunhai Su",
        "Peng Lu",
        "Chao Li",
        "Spandan Tiwari",
        "Ashish Sirasao",
        "Yuhan Dong"
      ],
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "published": "2026-02-05T10:02:00Z",
      "updated": "2026-02-05T10:02:00Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05499v1",
      "abs_url": "http://arxiv.org/abs/2602.05499v1",
      "summary": "SDFP提出了一种无需训练、即插即用的LLM加速框架，通过FIT剪枝构建draft模型。",
      "key_contributions": [
        "提出基于Fisher信息迹(FIT)的层剪枝方法",
        "构建无需训练的轻量级draft模型",
        "实现1.32x-1.5x的推理加速"
      ],
      "methodology": "利用FIT评估层敏感度，剪枝低影响层构建draft模型，与原模型进行投机解码验证，无需额外训练。",
      "tags": [
        "大语言模型",
        "投机解码",
        "模型剪枝",
        "模型加速"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 8,
      "relevance_reason": "该论文致力于提升LLM的推理速度，属于推理优化范畴。",
      "analyzed_at": "2026-02-06T06:52:39.940036",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05496v1",
      "title": "XEmoGPT: An Explainable Multimodal Emotion Recognition Framework with Cue-Level Perception and Reasoning",
      "abstract": "Explainable Multimodal Emotion Recognition plays a crucial role in applications such as human-computer interaction and social media analytics. However, current approaches struggle with cue-level perception and reasoning due to two main challenges: 1) general-purpose modality encoders are pretrained to capture global structures and general semantics rather than fine-grained emotional cues, resulting in limited sensitivity to emotional signals; and 2) available datasets usually involve a trade-off between annotation quality and scale, which leads to insufficient supervision for emotional cues and ultimately limits cue-level reasoning. Moreover, existing evaluation metrics are inadequate for assessing cue-level reasoning performance. To address these challenges, we propose eXplainable Emotion GPT (XEmoGPT), a novel EMER framework capable of both perceiving and reasoning over emotional cues. It incorporates two specialized modules: the Video Emotional Cue Bridge (VECB) and the Audio Emotional Cue Bridge (AECB), which enhance the video and audio encoders through carefully designed tasks for fine-grained emotional cue perception. To further support cue-level reasoning, we construct a large-scale dataset, EmoCue, designed to teach XEmoGPT how to reason over multimodal emotional cues. In addition, we introduce EmoCue-360, an automated metric that extracts and matches emotional cues using semantic similarity, and release EmoCue-Eval, a benchmark of 400 expert-annotated samples covering diverse emotional scenarios. Experimental results show that XEmoGPT achieves strong performance in both emotional cue perception and reasoning.",
      "authors": [
        "Hanwen Zhang",
        "Yao Liu",
        "Peiyuan Jiang",
        "Lang Junjie",
        "Xie Jun",
        "Yihui He",
        "Yajiao Deng",
        "Siyu Du",
        "Qiao Liu"
      ],
      "categories": [
        "cs.MM",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.MM",
      "published": "2026-02-05T09:58:41Z",
      "updated": "2026-02-05T09:58:41Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05496v1",
      "abs_url": "http://arxiv.org/abs/2602.05496v1",
      "summary": "XEmoGPT提出了一种可解释的多模态情感识别框架，提升了情感线索感知和推理能力。",
      "key_contributions": [
        "提出XEmoGPT框架，增强情感线索感知和推理",
        "构建大规模情感线索数据集EmoCue，促进线索级推理",
        "引入EmoCue-360指标和EmoCue-Eval基准"
      ],
      "methodology": "通过VECB和AECB模块增强视频和音频编码器，利用EmoCue数据集训练，提升模型对情感线索的理解和推理能力。",
      "tags": [
        "情感识别",
        "多模态学习",
        "可解释性",
        "情感线索"
      ],
      "assigned_category": "multimodal",
      "relevance_score": 9,
      "relevance_reason": "论文核心关注多模态情感识别，是该领域的关键研究。",
      "analyzed_at": "2026-02-06T06:52:41.562049",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05495v1",
      "title": "Transport and Merge: Cross-Architecture Merging for Large Language Models",
      "abstract": "Large language models (LLMs) achieve strong capabilities by scaling model capacity and training data, yet many real-world deployments rely on smaller models trained or adapted from low-resource data. This gap motivates the need for mechanisms to transfer knowledge from large, high-resource models to smaller, low-resource targets. While model merging provides an effective transfer mechanism, most existing approaches assume architecture-compatible models and therefore cannot directly transfer knowledge from large high-resource LLMs to heterogeneous low-resource targets. In this work, we propose a cross-architecture merging framework based on optimal transport (OT) that aligns activations to infer cross-neuron correspondences between heterogeneous models. The resulting transport plans are then used to guide direct weight-space fusion, enabling effective high-resource to low-resource transfer using only a small set of inputs. Extensive experiments across low-resource languages and specialized domains demonstrate consistent improvements over target models.",
      "authors": [
        "Chenhang Cui",
        "Binyun Yang",
        "Fei Shen",
        "Yuxin Chen",
        "Jingnan Zheng",
        "Xiang Wang",
        "An Zhang",
        "Tat-Seng Chua"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "published": "2026-02-05T09:57:57Z",
      "updated": "2026-02-05T09:57:57Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05495v1",
      "abs_url": "http://arxiv.org/abs/2602.05495v1",
      "summary": "提出了基于最优传输的跨架构模型融合框架，实现大模型知识向小模型的有效迁移。",
      "key_contributions": [
        "提出了一种基于最优传输的跨架构模型融合方法",
        "实现了大模型到异构小模型的知识迁移",
        "在低资源语言和特定领域验证了方法的有效性"
      ],
      "methodology": "利用最优传输对齐异构模型的激活，推断神经元对应关系，指导权重空间融合。",
      "tags": [
        "模型融合",
        "知识迁移",
        "最优传输",
        "跨架构",
        "大语言模型"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 7,
      "relevance_reason": "模型融合可增强模型推理能力，对特定任务推理性能有提升。",
      "analyzed_at": "2026-02-06T06:52:43.071186",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05493v1",
      "title": "LinguistAgent: A Reflective Multi-Model Platform for Automated Linguistic Annotation",
      "abstract": "Data annotation remains a significant bottleneck in the Humanities and Social Sciences, particularly for complex semantic tasks such as metaphor identification. While Large Language Models (LLMs) show promise, a significant gap remains between the theoretical capability of LLMs and their practical utility for researchers. This paper introduces LinguistAgent, an integrated, user-friendly platform that leverages a reflective multi-model architecture to automate linguistic annotation. The system implements a dual-agent workflow, comprising an Annotator and a Reviewer, to simulate a professional peer-review process. LinguistAgent supports comparative experiments across three paradigms: Prompt Engineering (Zero/Few-shot), Retrieval-Augmented Generation, and Fine-tuning. We demonstrate LinguistAgent's efficacy using the task of metaphor identification as an example, providing real-time token-level evaluation (Precision, Recall, and $F_1$ score) against human gold standards. The application and codes are released on https://github.com/Bingru-Li/LinguistAgent.",
      "authors": [
        "Bingru Li"
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.CL",
      "published": "2026-02-05T09:55:19Z",
      "updated": "2026-02-05T09:55:19Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05493v1",
      "abs_url": "http://arxiv.org/abs/2602.05493v1",
      "summary": "LinguistAgent是一个自动化语言标注平台，通过多模型架构和双Agent机制，提升复杂语义任务的标注效率。",
      "key_contributions": [
        "提出了一个基于反射式多模型架构的自动化语言标注平台LinguistAgent",
        "实现了双Agent（Annotator和Reviewer）工作流，模拟同行评审过程",
        "支持Prompt Engineering、RAG和Fine-tuning三种标注范式的比较实验"
      ],
      "methodology": "采用了双Agent架构，模拟专家评审流程，结合Prompt工程、RAG和微调等技术，在隐喻识别任务上进行评估。",
      "tags": [
        "自动化标注",
        "LLM",
        "语言学",
        "多Agent",
        "RAG"
      ],
      "assigned_category": "agent",
      "relevance_score": 8,
      "relevance_reason": "该论文的核心在于构建一个多Agent系统来完成标注任务，因此与AI Agent类别高度相关。",
      "analyzed_at": "2026-02-06T06:52:45.146807",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05480v1",
      "title": "SOMA-1M: A Large-Scale SAR-Optical Multi-resolution Alignment Dataset for Multi-Task Remote Sensing",
      "abstract": "Synthetic Aperture Radar (SAR) and optical imagery provide complementary strengths that constitute the critical foundation for transcending single-modality constraints and facilitating cross-modal collaborative processing and intelligent interpretation. However, existing benchmark datasets often suffer from limitations such as single spatial resolution, insufficient data scale, and low alignment accuracy, making them inadequate for supporting the training and generalization of multi-scale foundation models. To address these challenges, we introduce SOMA-1M (SAR-Optical Multi-resolution Alignment), a pixel-level precisely aligned dataset containing over 1.3 million pairs of georeferenced images with a specification of 512 x 512 pixels. This dataset integrates imagery from Sentinel-1, PIESAT-1, Capella Space, and Google Earth, achieving global multi-scale coverage from 0.5 m to 10 m. It encompasses 12 typical land cover categories, effectively ensuring scene diversity and complexity. To address multimodal projection deformation and massive data registration, we designed a rigorous coarse-to-fine image matching framework ensuring pixel-level alignment. Based on this dataset, we established comprehensive evaluation benchmarks for four hierarchical vision tasks, including image matching, image fusion, SAR-assisted cloud removal, and cross-modal translation, involving over 30 mainstream algorithms. Experimental results demonstrate that supervised training on SOMA-1M significantly enhances performance across all tasks. Notably, multimodal remote sensing image (MRSI) matching performance achieves current state-of-the-art (SOTA) levels. SOMA-1M serves as a foundational resource for robust multimodal algorithms and remote sensing foundation models. The dataset will be released publicly at: https://github.com/PeihaoWu/SOMA-1M.",
      "authors": [
        "Peihao Wu",
        "Yongxiang Yao",
        "Yi Wan",
        "Wenfei Zhang",
        "Ruipeng Zhao",
        "Jiayuan Li",
        "Yongjun Zhang"
      ],
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2026-02-05T09:39:49Z",
      "updated": "2026-02-05T09:39:49Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05480v1",
      "abs_url": "http://arxiv.org/abs/2602.05480v1",
      "summary": "SOMA-1M是一个大规模、多分辨率、像素级对齐的SAR-光学遥感数据集，促进多模态遥感算法研究。",
      "key_contributions": [
        "构建大规模多分辨率SAR-光学对齐数据集SOMA-1M",
        "提出严格的粗到细图像匹配框架，保证像素级对齐",
        "建立四个层次视觉任务的综合评估基准"
      ],
      "methodology": "设计了严格的从粗到细图像匹配框架，确保像素级对齐，并在此基础上建立了评估基准。",
      "tags": [
        "遥感",
        "多模态",
        "数据集",
        "图像匹配"
      ],
      "assigned_category": "multimodal",
      "relevance_score": 9,
      "relevance_reason": "论文核心内容为构建多模态遥感数据集，直接研究多模态学习领域的关键问题。",
      "analyzed_at": "2026-02-06T06:52:47.015124",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05474v1",
      "title": "LMMRec: LLM-driven Motivation-aware Multimodal Recommendation",
      "abstract": "Motivation-based recommendation systems uncover user behavior drivers. Motivation modeling, crucial for decision-making and content preference, explains recommendation generation. Existing methods often treat motivation as latent variables from interaction data, neglecting heterogeneous information like review text. In multimodal motivation fusion, two challenges arise: 1) achieving stable cross-modal alignment amid noise, and 2) identifying features reflecting the same underlying motivation across modalities. To address these, we propose LLM-driven Motivation-aware Multimodal Recommendation (LMMRec), a model-agnostic framework leveraging large language models for deep semantic priors and motivation understanding. LMMRec uses chain-of-thought prompting to extract fine-grained user and item motivations from text. A dual-encoder architecture models textual and interaction-based motivations for cross-modal alignment, while Motivation Coordination Strategy and Interaction-Text Correspondence Method mitigate noise and semantic drift through contrastive learning and momentum updates. Experiments on three datasets show LMMRec achieves up to a 4.98\\% performance improvement.",
      "authors": [
        "Yicheng Di",
        "Zhanjie Zhang",
        "Yun Wangc",
        "Jinren Liue",
        "Jiaqi Yanf",
        "Jiyu Wei",
        "Xiangyu Chend",
        "Yuan Liu"
      ],
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "published": "2026-02-05T09:22:17Z",
      "updated": "2026-02-05T09:22:17Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05474v1",
      "abs_url": "http://arxiv.org/abs/2602.05474v1",
      "summary": "LMMRec利用LLM提取动机，融合多模态信息，提升推荐系统性能。",
      "key_contributions": [
        "提出LMMRec框架，利用LLM理解用户和物品动机",
        "采用双编码器结构和对比学习，实现跨模态对齐",
        "设计动机协调策略和交互-文本对应方法，降低噪声和语义漂移"
      ],
      "methodology": "使用LLM提取文本动机，通过双编码器建模文本和交互动机，利用对比学习进行跨模态对齐和噪声抑制。",
      "tags": [
        "推荐系统",
        "多模态学习",
        "大型语言模型",
        "动机建模",
        "对比学习"
      ],
      "assigned_category": "multimodal",
      "relevance_score": 9,
      "relevance_reason": "论文核心关注多模态学习在推荐系统中的应用，并利用LLM进行语义理解。",
      "analyzed_at": "2026-02-06T06:52:51.826395",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05467v1",
      "title": "MerNav: A Highly Generalizable Memory-Execute-Review Framework for Zero-Shot Object Goal Navigation",
      "abstract": "Visual Language Navigation (VLN) is one of the fundamental capabilities for embodied intelligence and a critical challenge that urgently needs to be addressed. However, existing methods are still unsatisfactory in terms of both success rate (SR) and generalization: Supervised Fine-Tuning (SFT) approaches typically achieve higher SR, while Training-Free (TF) approaches often generalize better, but it is difficult to obtain both simultaneously. To this end, we propose a Memory-Execute-Review framework. It consists of three parts: a hierarchical memory module for providing information support, an execute module for routine decision-making and actions, and a review module for handling abnormal situations and correcting behavior. We validated the effectiveness of this framework on the Object Goal Navigation task. Across 4 datasets, our average SR achieved absolute improvements of 7% and 5% compared to all baseline methods under TF and Zero-Shot (ZS) settings, respectively. On the most commonly used HM3D_v0.1 and the more challenging open vocabulary dataset HM3D_OVON, the SR improved by 8% and 6%, under ZS settings. Furthermore, on the MP3D and HM3D_OVON datasets, our method not only outperformed all TF methods but also surpassed all SFT methods, achieving comprehensive leadership in both SR (5% and 2%) and generalization.",
      "authors": [
        "Dekang Qi",
        "Shuang Zeng",
        "Xinyuan Chang",
        "Feng Xiong",
        "Shichao Xie",
        "Xiaolong Wu",
        "Mu Xu"
      ],
      "categories": [
        "cs.CV",
        "cs.CL",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "published": "2026-02-05T09:15:34Z",
      "updated": "2026-02-05T09:15:34Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05467v1",
      "abs_url": "http://arxiv.org/abs/2602.05467v1",
      "summary": "提出MerNav框架，利用记忆、执行和回顾模块，提升零样本目标导航的成功率和泛化性。",
      "key_contributions": [
        "提出Memory-Execute-Review (MerNav) 框架",
        "在四个数据集上验证了框架的有效性，显著提升了零样本设定下的成功率",
        "在部分数据集上超越了监督微调方法，实现了成功率和泛化性的双重领先"
      ],
      "methodology": "构建分层记忆模块提供信息支持，执行模块进行决策，回顾模块处理异常并纠正行为。",
      "tags": [
        "Visual Language Navigation",
        "Zero-Shot Learning",
        "Object Goal Navigation"
      ],
      "assigned_category": "agent",
      "relevance_score": 8,
      "relevance_reason": "该论文研究AI Agent在复杂环境中的导航问题，与Agent领域高度相关。",
      "analyzed_at": "2026-02-06T06:52:53.679193",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05464v1",
      "title": "Refine and Purify: Orthogonal Basis Optimization with Null-Space Denoising for Conditional Representation Learning",
      "abstract": "Conditional representation learning aims to extract criterion-specific features for customized tasks. Recent studies project universal features onto the conditional feature subspace spanned by an LLM-generated text basis to obtain conditional representations. However, such methods face two key limitations: sensitivity to subspace basis and vulnerability to inter-subspace interference. To address these challenges, we propose OD-CRL, a novel framework integrating Adaptive Orthogonal Basis Optimization (AOBO) and Null-Space Denoising Projection (NSDP). Specifically, AOBO constructs orthogonal semantic bases via singular value decomposition with a curvature-based truncation. NSDP suppresses non-target semantic interference by projecting embeddings onto the null space of irrelevant subspaces. Extensive experiments conducted across customized clustering, customized classification, and customized retrieval tasks demonstrate that OD-CRL achieves a new state-of-the-art performance with superior generalization.",
      "authors": [
        "Jiaquan Wang",
        "Yan Lyu",
        "Chen Li",
        "Yuheng Jia"
      ],
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "published": "2026-02-05T09:14:44Z",
      "updated": "2026-02-05T09:14:44Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05464v1",
      "abs_url": "http://arxiv.org/abs/2602.05464v1",
      "summary": "提出OD-CRL框架，优化条件表示学习中的基向量并抑制干扰，提升任务性能。",
      "key_contributions": [
        "提出Adaptive Orthogonal Basis Optimization (AOBO)",
        "提出Null-Space Denoising Projection (NSDP)",
        "OD-CRL在定制化任务上达到SOTA"
      ],
      "methodology": "利用AOBO构建正交语义基向量，并通过NSDP将embeddings投影到无关子空间的零空间，抑制非目标语义干扰。",
      "tags": [
        "条件表示学习",
        "正交基优化",
        "零空间去噪"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 6,
      "relevance_reason": "涉及LLM生成语义空间用于特定任务，具有一定推理成分。",
      "analyzed_at": "2026-02-06T06:52:55.314044",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05456v1",
      "title": "Ontology-Driven Robotic Specification Synthesis",
      "abstract": "This paper addresses robotic system engineering for safety- and mission-critical applications by bridging the gap between high-level objectives and formal, executable specifications. The proposed method, Robotic System Task to Model Transformation Methodology (RSTM2) is an ontology-driven, hierarchical approach using stochastic timed Petri nets with resources, enabling Monte Carlo simulations at mission, system, and subsystem levels. A hypothetical case study demonstrates how the RSTM2 method supports architectural trades, resource allocation, and performance analysis under uncertainty. Ontological concepts further enable explainable AI-based assistants, facilitating fully autonomous specification synthesis. The methodology offers particular benefits to complex multi-robot systems, such as the NASA CADRE mission, representing decentralized, resource-aware, and adaptive autonomous systems of the future.",
      "authors": [
        "Maksym Figat",
        "Ryan M. Mackey",
        "Michel D. Ingham"
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "published": "2026-02-05T08:59:23Z",
      "updated": "2026-02-05T08:59:23Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05456v1",
      "abs_url": "http://arxiv.org/abs/2602.05456v1",
      "summary": "基于本体的机器人系统规范综合方法，用于安全关键应用，支持多机器人系统。",
      "key_contributions": [
        "提出RSTM2方法，连接高层目标和形式化规范",
        "利用随机时间Petri网进行多层级蒙特卡洛仿真",
        "使用本体实现可解释AI辅助，促进自主规范综合"
      ],
      "methodology": "RSTM2方法是一种基于本体的层次化方法，使用随机时间Petri网进行多层级仿真，支持资源分配和性能分析。",
      "tags": [
        "机器人",
        "本体",
        "形式化方法",
        "Petri网",
        "多机器人系统"
      ],
      "assigned_category": "agent",
      "relevance_score": 8,
      "relevance_reason": "该论文关注自主机器人系统的规范合成，与Agent的自主规划和执行高度相关。",
      "analyzed_at": "2026-02-06T06:53:23.098258",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05448v1",
      "title": "BLITZRANK: Principled Zero-shot Ranking Agents with Tournament Graphs",
      "abstract": "Large language models have emerged as powerful zero-shot rerankers for retrieval-augmented generation, offering strong generalization without task-specific training. However, existing LLM reranking methods either rely on heuristics that fail to fully exploit the information revealed by each ranking decision or are inefficient when they do. We introduce a tournament graph framework that provides a principled foundation for $k$-wise reranking. Our key observation is that each $k$-document comparison reveals a complete tournament of $\\binom{k}{2}$ pairwise preferences. These tournaments are aggregated into a global preference graph, whose transitive closure yields many additional orderings without further model invocations. We formalize when a candidate's rank is certifiably determined and design a query schedule that greedily maximizes information gain towards identifying the top-$m$ items. Our framework also gracefully handles non-transitive preferences - cycles induced by LLM judgments - by collapsing them into equivalence classes that yield principled tiered rankings. Empirically, across 14 benchmarks and 5 LLMs, our method achieves Pareto dominance over existing methods: matching or exceeding accuracy while requiring 25-40% fewer tokens than comparable approaches, and 7$\\times$ fewer than pairwise methods at near-identical quality.",
      "authors": [
        "Sheshansh Agrawal",
        "Thien Hang Nguyen",
        "Douwe Kiela"
      ],
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "published": "2026-02-05T08:41:00Z",
      "updated": "2026-02-05T08:41:00Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05448v1",
      "abs_url": "http://arxiv.org/abs/2602.05448v1",
      "summary": "提出了基于tournament graph的LLM zero-shot reranking框架，提高了效率和准确率。",
      "key_contributions": [
        "提出tournament graph reranking框架",
        "设计信息增益最大化查询策略",
        "处理非传递偏好，实现分层排序"
      ],
      "methodology": "将k-wise reranking转化为tournament graph，利用传递闭包扩展排序信息，并贪婪地选择查询。",
      "tags": [
        "LLM",
        "Reranking",
        "Tournament Graph",
        "Zero-shot"
      ],
      "assigned_category": "memory",
      "relevance_score": 8,
      "relevance_reason": "虽然不是直接记忆增强，但优化了RAG流程中的排序环节，高度相关。",
      "analyzed_at": "2026-02-06T06:53:25.043482",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05447v1",
      "title": "Structured Context Engineering for File-Native Agentic Systems: Evaluating Schema Accuracy, Format Effectiveness, and Multi-File Navigation at Scale",
      "abstract": "Large Language Model agents increasingly operate external systems through programmatic interfaces, yet practitioners lack empirical guidance on how to structure the context these agents consume. Using SQL generation as a proxy for programmatic agent operations, we present a systematic study of context engineering for structured data, comprising 9,649 experiments across 11 models, 4 formats (YAML, Markdown, JSON, Token-Oriented Object Notation [TOON]), and schemas ranging from 10 to 10,000 tables.   Our findings challenge common assumptions. First, architecture choice is model-dependent: file-based context retrieval improves accuracy for frontier-tier models (Claude, GPT, Gemini; +2.7%, p=0.029) but shows mixed results for open source models (aggregate -7.7%, p<0.001), with deficits varying substantially by model. Second, format does not significantly affect aggregate accuracy (chi-squared=2.45, p=0.484), though individual models, particularly open source, exhibit format-specific sensitivities. Third, model capability is the dominant factor, with a 21 percentage point accuracy gap between frontier and open source tiers that dwarfs any format or architecture effect. Fourth, file-native agents scale to 10,000 tables through domain-partitioned schemas while maintaining high navigation accuracy. Fifth, file size does not predict runtime efficiency: compact formats can consume significantly more tokens at scale due to format-unfamiliar search patterns.   These findings provide practitioners with evidence-based guidance for deploying LLM agents on structured systems, demonstrating that architectural decisions should be tailored to model capability rather than assuming universal best practices.",
      "authors": [
        "Damon McMillan"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "published": "2026-02-05T08:39:05Z",
      "updated": "2026-02-05T08:39:05Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05447v1",
      "abs_url": "http://arxiv.org/abs/2602.05447v1",
      "summary": "研究LLM Agent在处理结构化数据时，上下文工程的不同架构和格式的影响。",
      "key_contributions": [
        "评估了文件结构对LLM处理结构化数据的性能影响",
        "分析了不同数据格式（YAML, Markdown, JSON, TOON）的影响",
        "揭示模型能力是影响性能的主要因素，架构选择应依赖模型能力"
      ],
      "methodology": "通过SQL生成任务，对比不同模型在不同架构和格式下的表现，进行大规模实验分析。",
      "tags": [
        "LLM Agent",
        "上下文工程",
        "结构化数据",
        "SQL生成"
      ],
      "assigned_category": "agent",
      "relevance_score": 9,
      "relevance_reason": "直接研究Agent如何利用上下文处理结构化数据，核心相关。",
      "analyzed_at": "2026-02-06T06:53:26.584603",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05441v1",
      "title": "Benchmarking Affordance Generalization with BusyBox",
      "abstract": "Vision-Language-Action (VLA) models have been attracting the attention of researchers and practitioners thanks to their promise of generalization. Although single-task policies still offer competitive performance, VLAs are increasingly able to handle commands and environments unseen in their training set. While generalization in vision and language space is undoubtedly important for robust versatile behaviors, a key meta-skill VLAs need to possess is affordance generalization -- the ability to manipulate new objects with familiar physical features.   In this work, we present BusyBox, a physical benchmark for systematic semi-automatic evaluation of VLAs' affordance generalization. BusyBox consists of 6 modules with switches, sliders, wires, buttons, a display, and a dial. The modules can be swapped and rotated to create a multitude of BusyBox variations with different visual appearances but the same set of affordances. We empirically demonstrate that generalization across BusyBox variants is highly challenging even for strong open-weights VLAs such as $π_{0.5}$ and GR00T-N1.6. To encourage the research community to evaluate their own VLAs on BusyBox and to propose new affordance generalization experiments, we have designed BusyBox to be easy to build in most robotics labs. We release the full set of CAD files for 3D-printing its parts as well as a bill of materials for (optionally) assembling its electronics. We also publish a dataset of language-annotated demonstrations that we collected using the common bimanual Mobile Aloha robot on the canonical BusyBox configuration. All of the released materials are available at https://microsoft.github.io/BusyBox.",
      "authors": [
        "Dean Fortier",
        "Timothy Adamson",
        "Tess Hellebrekers",
        "Teresa LaScala",
        "Kofi Ennin",
        "Michael Murray",
        "Andrey Kolobov",
        "Galen Mullins"
      ],
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "published": "2026-02-05T08:31:27Z",
      "updated": "2026-02-05T08:31:27Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05441v1",
      "abs_url": "http://arxiv.org/abs/2602.05441v1",
      "summary": "提出了BusyBox，一个评估VLA模型在操作具有熟悉物理特征的新物体时泛化能力的物理基准。",
      "key_contributions": [
        "提出了BusyBox基准，用于评估VLA模型的affordance generalization能力",
        "BusyBox由可互换和旋转的模块组成，可创建具有不同外观但相同 affordance 的变体",
        "发布了 CAD 文件、材料清单和语言注释演示数据集，方便复现和研究"
      ],
      "methodology": "通过物理基准测试，系统性地评估VLA模型在不同BusyBox变体上的操作能力，衡量其affordance generalization性能。",
      "tags": [
        "Vision-Language-Action",
        "Affordance Generalization",
        "Robotics"
      ],
      "assigned_category": "multimodal",
      "relevance_score": 9,
      "relevance_reason": "论文核心关注VLA模型操作新物体的能力，属于多模态学习领域的重要研究方向。",
      "analyzed_at": "2026-02-06T06:53:28.651815",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05437v1",
      "title": "Once Correct, Still Wrong: Counterfactual Hallucination in Multilingual Vision-Language Models",
      "abstract": "Vision-language models (VLMs) can achieve high accuracy while still accepting culturally plausible but visually incorrect interpretations. Existing hallucination benchmarks rarely test this failure mode, particularly outside Western contexts and English. We introduce M2CQA, a culturally grounded multimodal benchmark built from images spanning 17 MENA countries, paired with contrastive true and counterfactual statements in English, Arabic, and its dialects. To isolate hallucination beyond raw accuracy, we propose the CounterFactual Hallucination Rate (CFHR), which measures counterfactual acceptance conditioned on correctly answering the true statement. Evaluating state-of-the-art VLMs under multiple prompting strategies, we find that CFHR rises sharply in Arabic, especially in dialects, even when true-statement accuracy remains high. Moreover, reasoning-first prompting consistently increases counterfactual hallucination, while answering before justifying improves robustness. We will make the experimental resources and dataset publicly available for the community.",
      "authors": [
        "Basel Mousi",
        "Fahim Dalvi",
        "Shammur Chowdhury",
        "Firoj Alam",
        "Nadir Durrani"
      ],
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "published": "2026-02-05T08:26:44Z",
      "updated": "2026-02-05T08:26:44Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05437v1",
      "abs_url": "http://arxiv.org/abs/2602.05437v1",
      "summary": "论文揭示了多语言视觉-语言模型在非西方文化背景下的反事实幻觉问题，并提出了新的评估基准。",
      "key_contributions": [
        "提出了M2CQA基准测试，用于评估中东北非文化背景下的多语言视觉-语言模型的反事实幻觉",
        "提出了CounterFactual Hallucination Rate (CFHR)指标，用于衡量模型在正确回答真实语句后接受反事实语句的可能性",
        "揭示了现有视觉-语言模型在阿拉伯语（特别是方言）中存在显著的反事实幻觉问题，尤其是在reasoning-first prompting下"
      ],
      "methodology": "构建包含17个中东北非国家图像和对比性语句的数据集，评估模型在不同prompt策略下的准确率和CFHR。",
      "tags": [
        "Multimodal",
        "Vision-Language Models",
        "Hallucination",
        "Arabic"
      ],
      "assigned_category": "multimodal",
      "relevance_score": 9,
      "relevance_reason": "论文核心关注多模态学习中的幻觉问题，与主题直接相关。",
      "analyzed_at": "2026-02-06T06:53:31.199882",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05430v1",
      "title": "Day-Ahead Electricity Price Forecasting for Volatile Markets Using Foundation Models with Regularization Strategy",
      "abstract": "Electricity price forecasting (EPF) is essential for energy markets stakeholders (e.g. grid operators, energy traders, policymakers) but remains challenging due to the inherent volatility and nonlinearity of price signals. Traditional statistical and deep learning (DL) models often struggle to capture complex temporal dependencies and integrate heterogeneous data effectively. While time series foundation models (TSFMs) have shown strong performance in general time series forecasting tasks, such as traffic forecasting and weather forecasting. However, their effectiveness in day-ahead EPF, particularly in volatile markets, remains underexplored. This paper presents a spike regularization strategy and evaluates a wide range of TSFMs, including Tiny Time Mixers (TTMs), MOIRAI, MOMENT, and TimesFM, against traditional statistical and DL models such as Autoregressive Integrated Moving Average (ARIMA), Long-short Term Memory (LSTM), and Convolutional Neural Network - LSTM (CNN-LSTM) using half-hourly wholesale market data with volatile trends in Singapore. Exogenous factors (e.g. weather and calendar variables) are also incorporated into models where applicable. Results demonstrate that TSFMs consistently outperform traditional approaches, achieving up to 37.4% improvement in MAPE across various evaluation settings. The findings offer practical guidance for improving forecast accuracy and decision-making in volatile electricity markets.",
      "authors": [
        "Kritchanat Ponyuenyong",
        "Pengyu Tu",
        "Jia Wei Tan",
        "Wei Soon Cheong",
        "Jamie Ng Suat Ling",
        "Lianlian Jiang"
      ],
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "published": "2026-02-05T08:20:50Z",
      "updated": "2026-02-05T08:20:50Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05430v1",
      "abs_url": "http://arxiv.org/abs/2602.05430v1",
      "summary": "论文提出一种基于Spike正则化的时间序列基础模型，用于波动市场中电力价格预测，效果显著。",
      "key_contributions": [
        "评估时间序列基础模型在波动电力市场中的有效性",
        "提出一种spike正则化策略",
        "对比了多种TSFM与传统模型的性能"
      ],
      "methodology": "采用Spike正则化策略，并对比TTMs, MOIRAI, MOMENT, TimesFM等TSFM与ARIMA, LSTM, CNN-LSTM等模型。",
      "tags": [
        "电力价格预测",
        "时间序列基础模型",
        "正则化",
        "波动市场"
      ],
      "assigned_category": "agent",
      "relevance_score": 5,
      "relevance_reason": "虽然使用了基础模型，但主要聚焦在电力预测应用，与agent有一定联系。",
      "analyzed_at": "2026-02-06T06:53:32.904970",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05429v1",
      "title": "M$^2$-Miner: Multi-Agent Enhanced MCTS for Mobile GUI Agent Data Mining",
      "abstract": "Graphical User Interface (GUI) agent is pivotal to advancing intelligent human-computer interaction paradigms. Constructing powerful GUI agents necessitates the large-scale annotation of high-quality user-behavior trajectory data (i.e., intent-trajectory pairs) for training. However, manual annotation methods and current GUI agent data mining approaches typically face three critical challenges: high construction cost, poor data quality, and low data richness. To address these issues, we propose M$^2$-Miner, the first low-cost and automated mobile GUI agent data-mining framework based on Monte Carlo Tree Search (MCTS). For better data mining efficiency and quality, we present a collaborative multi-agent framework, comprising InferAgent, OrchestraAgent, and JudgeAgent for guidance, acceleration, and evaluation. To further enhance the efficiency of mining and enrich intent diversity, we design an intent recycling strategy to extract extra valuable interaction trajectories. Additionally, a progressive model-in-the-loop training strategy is introduced to improve the success rate of data mining. Extensive experiments have demonstrated that the GUI agent fine-tuned using our mined data achieves state-of-the-art performance on several commonly used mobile GUI benchmarks. Our work will be released to facilitate the community research.",
      "authors": [
        "Rui Lv",
        "Juncheng Mo",
        "Tianyi Chu",
        "Chen Rao",
        "Hongyi Jing",
        "Jiajie Teng",
        "Jiafu Chen",
        "Shiqi Zhang",
        "Liangzi Ding",
        "Shuo Fang",
        "Huaizhong Lin",
        "Ziqiang Dang",
        "Chenguang Ma",
        "Lei Zhao"
      ],
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "published": "2026-02-05T08:19:39Z",
      "updated": "2026-02-05T08:19:39Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05429v1",
      "abs_url": "http://arxiv.org/abs/2602.05429v1",
      "summary": "提出了基于多智能体增强蒙特卡洛树搜索的移动GUI代理数据挖掘框架M$^2$-Miner。",
      "key_contributions": [
        "提出了低成本自动化的GUI代理数据挖掘框架M$^2$-Miner",
        "设计了协同多智能体框架，提升数据挖掘效率和质量",
        "引入意图循环利用策略和渐进式模型在环训练策略"
      ],
      "methodology": "利用蒙特卡洛树搜索(MCTS)和多智能体协同框架（InferAgent, OrchestraAgent, JudgeAgent）进行GUI代理数据挖掘，并加入意图循环和模型在环训练。",
      "tags": [
        "GUI Agent",
        "Data Mining",
        "MCTS",
        "Multi-Agent",
        "Human-Computer Interaction"
      ],
      "assigned_category": "agent",
      "relevance_score": 9,
      "relevance_reason": "论文核心在于构建智能体进行GUI数据挖掘，属于Agent领域的核心研究内容。",
      "analyzed_at": "2026-02-06T06:53:34.901540",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05419v1",
      "title": "Grammatical Error Correction Evaluation by Optimally Transporting Edit Representation",
      "abstract": "Automatic evaluation in grammatical error correction (GEC) is crucial for selecting the best-performing systems. Currently, reference-based metrics are a popular choice, which basically measure the similarity between hypothesis and reference sentences. However, similarity measures based on embeddings, such as BERTScore, are often ineffective, since many words in the source sentences remain unchanged in both the hypothesis and the reference. This study focuses on edits specifically designed for GEC, i.e., ERRANT, and computes similarity measured over the edits from the source sentence. To this end, we propose edit vector, a representation for an edit, and introduce a new metric, UOT-ERRANT, which transports these edit vectors from hypothesis to reference using unbalanced optimal transport. Experiments with SEEDA meta-evaluation show that UOT-ERRANT improves evaluation performance, particularly in the +Fluency domain where many edits occur. Moreover, our method is highly interpretable because the transport plan can be interpreted as a soft edit alignment, making UOT-ERRANT a useful metric for both system ranking and analyzing GEC systems. Our code is available from https://github.com/gotutiyan/uot-errant.",
      "authors": [
        "Takumi Goto",
        "Yusuke Sakai",
        "Taro Watanabe"
      ],
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "published": "2026-02-05T08:05:42Z",
      "updated": "2026-02-05T08:05:42Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05419v1",
      "abs_url": "http://arxiv.org/abs/2602.05419v1",
      "summary": "提出了一种基于非平衡最优传输的语法纠错评估指标UOT-ERRANT，提高了评估性能和可解释性。",
      "key_contributions": [
        "提出edit vector，一种用于表示编辑操作的向量。",
        "引入基于非平衡最优传输的GEC评估指标UOT-ERRANT。",
        "实验证明UOT-ERRANT在Fluency方面表现更优，并具有良好的可解释性。"
      ],
      "methodology": "通过计算hypothesis和reference之间的edit vector距离，并使用非平衡最优传输来对齐编辑操作，从而评估GEC系统的性能。",
      "tags": [
        "Grammatical Error Correction",
        "Evaluation Metric",
        "Optimal Transport",
        "NLP"
      ],
      "assigned_category": "agent_tuning",
      "relevance_score": 5,
      "relevance_reason": "在agent的tool use中，语法纠错可以作为一种工具，因此具有一定相关性。",
      "analyzed_at": "2026-02-06T06:53:36.918736",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05413v1",
      "title": "SciDef: Automating Definition Extraction from Academic Literature with Large Language Models",
      "abstract": "Definitions are the foundation for any scientific work, but with a significant increase in publication numbers, gathering definitions relevant to any keyword has become challenging. We therefore introduce SciDef, an LLM-based pipeline for automated definition extraction. We test SciDef on DefExtra & DefSim, novel datasets of human-extracted definitions and definition-pairs' similarity, respectively. Evaluating 16 language models across prompting strategies, we demonstrate that multi-step and DSPy-optimized prompting improve extraction performance. To evaluate extraction, we test various metrics and show that an NLI-based method yields the most reliable results. We show that LLMs are largely able to extract definitions from scientific literature (86.4% of definitions from our test-set); yet future work should focus not just on finding definitions, but on identifying relevant ones, as models tend to over-generate them.   Code & datasets are available at https://github.com/Media-Bias-Group/SciDef.",
      "authors": [
        "Filip Kučera",
        "Christoph Mandl",
        "Isao Echizen",
        "Radu Timofte",
        "Timo Spinde"
      ],
      "categories": [
        "cs.IR",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "published": "2026-02-05T07:52:08Z",
      "updated": "2026-02-05T07:52:08Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05413v1",
      "abs_url": "http://arxiv.org/abs/2602.05413v1",
      "summary": "SciDef提出一个基于LLM的pipeline，用于从学术文献中自动提取定义，并评估了不同prompting策略和指标。",
      "key_contributions": [
        "提出了SciDef：一个基于LLM的定义提取pipeline",
        "构建了DefExtra & DefSim数据集用于评估",
        "评估了不同prompting策略和NLI-based指标"
      ],
      "methodology": "利用LLM和多步prompting方法进行定义提取，使用NLI-based方法进行结果评估，并构建新数据集进行验证。",
      "tags": [
        "定义提取",
        "LLM",
        "自然语言处理",
        "知识获取"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 8,
      "relevance_reason": "该论文使用LLM进行知识抽取，并优化prompting策略，与LLM推理相关。",
      "analyzed_at": "2026-02-06T06:53:38.562187",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05407v1",
      "title": "H-AdminSim: A Multi-Agent Simulator for Realistic Hospital Administrative Workflows with FHIR Integration",
      "abstract": "Hospital administration departments handle a wide range of operational tasks and, in large hospitals, process over 10,000 requests per day, driving growing interest in LLM-based automation. However, prior work has focused primarily on patient--physician interactions or isolated administrative subtasks, failing to capture the complexity of real administrative workflows. To address this gap, we propose H-AdminSim, a comprehensive end-to-end simulation framework that combines realistic data generation with multi-agent-based simulation of hospital administrative workflows. These tasks are quantitatively evaluated using detailed rubrics, enabling systematic comparison of LLMs. Through FHIR integration, H-AdminSim provides a unified and interoperable environment for testing administrative workflows across heterogeneous hospital settings, serving as a standardized testbed for assessing the feasibility and performance of LLM-driven administrative automation.",
      "authors": [
        "Jun-Min Lee",
        "Meong Hi Son",
        "Edward Choi"
      ],
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "published": "2026-02-05T07:44:56Z",
      "updated": "2026-02-05T07:44:56Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05407v1",
      "abs_url": "http://arxiv.org/abs/2602.05407v1",
      "summary": "H-AdminSim是一个用于模拟医院行政工作流程的多智能体仿真框架，集成FHIR标准。",
      "key_contributions": [
        "提出H-AdminSim仿真框架，模拟医院行政工作流程",
        "结合现实数据生成和多智能体仿真",
        "通过FHIR集成，实现跨异构医院环境的互操作性"
      ],
      "methodology": "构建基于多智能体的仿真环境，生成真实数据，并使用FHIR标准集成异构医院环境。",
      "tags": [
        "multi-agent",
        "simulation",
        "FHIR",
        "healthcare",
        "LLM"
      ],
      "assigned_category": "agent",
      "relevance_score": 9,
      "relevance_reason": "论文核心是多智能体仿真，用于评估LLM在医院行政自动化中的应用。",
      "analyzed_at": "2026-02-06T06:53:40.073972",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05392v1",
      "title": "Beyond Length: Context-Aware Expansion and Independence as Developmentally Sensitive Evaluation in Child Utterances",
      "abstract": "Evaluating the quality of children's utterances in adult-child dialogue remains challenging due to insufficient context-sensitive metrics. Common proxies such as Mean Length of Utterance (MLU), lexical diversity (vocd-D), and readability indices (Flesch-Kincaid Grade Level, Gunning Fog Index) are dominated by length and ignore conversational context, missing aspects of response quality such as reasoning depth, topic maintenance, and discourse planning. We introduce an LLM-as-a-judge framework that first classifies the Previous Adult Utterance Type and then scores the child's response along two axes: Expansion (contextual elaboration and inferential depth) and Independence (the child's contribution to advancing the discourse). These axes reflect fundamental dimensions in child language development, where Expansion captures elaboration, clause combining, and causal and contrastive connectives. Independence captures initiative, topic control, decreasing reliance on adult scaffolding through growing self-regulation, and audience design. We establish developmental validity by showing age-related patterns and demonstrate predictive value by improving age estimation over common baselines. We further confirm semantic sensitivity by detecting differences tied to discourse relations. Our metrics align with human judgments, enabling large-scale evaluation. This shifts child utterance assessment from simply measuring length to evaluating how meaningfully the child's speech contributes to and advances the conversation within its context.",
      "authors": [
        "Jiyun Chun",
        "Eric Fosler-Lussier",
        "Michael White",
        "Andrew Perrault"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "published": "2026-02-05T07:19:04Z",
      "updated": "2026-02-05T07:19:04Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05392v1",
      "abs_url": "http://arxiv.org/abs/2602.05392v1",
      "summary": "提出一种上下文感知的儿童语言评估框架，关注扩展性和独立性，优于传统长度指标。",
      "key_contributions": [
        "提出Expansion和Independence两个评估儿童语言的新维度",
        "开发基于LLM的评估框架，自动评估儿童语言",
        "验证了该框架的有效性，与人类判断一致，并具有预测价值"
      ],
      "methodology": "利用LLM分类成人语句类型，然后根据Expansion和Independence两个维度对儿童的回应进行评分，并进行实验验证。",
      "tags": [
        "儿童语言",
        "语言评估",
        "LLM",
        "上下文感知"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 8,
      "relevance_reason": "使用LLM进行语言评估，关注儿童语句的推理深度和独立性。",
      "analyzed_at": "2026-02-06T06:53:41.824644",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05387v1",
      "title": "Parallel Swin Transformer-Enhanced 3D MRI-to-CT Synthesis for MRI-Only Radiotherapy Planning",
      "abstract": "MRI provides superior soft tissue contrast without ionizing radiation; however, the absence of electron density information limits its direct use for dose calculation. As a result, current radiotherapy workflows rely on combined MRI and CT acquisitions, increasing registration uncertainty and procedural complexity. Synthetic CT generation enables MRI only planning but remains challenging due to nonlinear MRI-CT relationships and anatomical variability. We propose Parallel Swin Transformer-Enhanced Med2Transformer, a 3D architecture that integrates convolutional encoding with dual Swin Transformer branches to model both local anatomical detail and long-range contextual dependencies. Multi-scale shifted window attention with hierarchical feature aggregation improves anatomical fidelity. Experiments on public and clinical datasets demonstrate higher image similarity and improved geometric accuracy compared with baseline methods. Dosimetric evaluation shows clinically acceptable performance, with a mean target dose error of 1.69%. Code is available at: https://github.com/mobaidoctor/med2transformer.",
      "authors": [
        "Zolnamar Dorjsembe",
        "Hung-Yi Chen",
        "Furen Xiao",
        "Hsing-Kuo Pao"
      ],
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "published": "2026-02-05T07:13:54Z",
      "updated": "2026-02-05T07:13:54Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05387v1",
      "abs_url": "http://arxiv.org/abs/2602.05387v1",
      "summary": "提出一种基于并行Swin Transformer的3D MRI合成CT方法，用于MRI引导的放疗计划。",
      "key_contributions": [
        "提出并行Swin Transformer增强的Med2Transformer架构",
        "利用双Swin Transformer分支建模局部细节和长程依赖",
        "在公开和临床数据集上验证了方法的有效性"
      ],
      "methodology": "结合卷积编码和双Swin Transformer分支，通过多尺度移位窗口注意力进行特征聚合，提升解剖结构保真度。",
      "tags": [
        "MRI",
        "CT",
        "Swin Transformer",
        "医学图像合成",
        "放疗计划"
      ],
      "assigned_category": "multimodal",
      "relevance_score": 7,
      "relevance_reason": "涉及图像合成，可以视作一种模态转换问题，具有一定相关性。",
      "analyzed_at": "2026-02-06T06:53:43.619851",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05386v1",
      "title": "Spider-Sense: Intrinsic Risk Sensing for Efficient Agent Defense with Hierarchical Adaptive Screening",
      "abstract": "As large language models (LLMs) evolve into autonomous agents, their real-world applicability has expanded significantly, accompanied by new security challenges. Most existing agent defense mechanisms adopt a mandatory checking paradigm, in which security validation is forcibly triggered at predefined stages of the agent lifecycle. In this work, we argue that effective agent security should be intrinsic and selective rather than architecturally decoupled and mandatory. We propose Spider-Sense framework, an event-driven defense framework based on Intrinsic Risk Sensing (IRS), which allows agents to maintain latent vigilance and trigger defenses only upon risk perception. Once triggered, the Spider-Sense invokes a hierarchical defence mechanism that trades off efficiency and precision: it resolves known patterns via lightweight similarity matching while escalating ambiguous cases to deep internal reasoning, thereby eliminating reliance on external models. To facilitate rigorous evaluation, we introduce S$^2$Bench, a lifecycle-aware benchmark featuring realistic tool execution and multi-stage attacks. Extensive experiments demonstrate that Spider-Sense achieves competitive or superior defense performance, attaining the lowest Attack Success Rate (ASR) and False Positive Rate (FPR), with only a marginal latency overhead of 8.3\\%.",
      "authors": [
        "Zhenxiong Yu",
        "Zhi Yang",
        "Zhiheng Jin",
        "Shuhe Wang",
        "Heng Zhang",
        "Yanlin Fei",
        "Lingfeng Zeng",
        "Fangqi Lou",
        "Shuo Zhang",
        "Tu Hu",
        "Jingping Liu",
        "Rongze Chen",
        "Xingyu Zhu",
        "Kunyi Wang",
        "Chaofa Yuan",
        "Xin Guo",
        "Zhaowei Liu",
        "Feipeng Zhang",
        "Jie Huang",
        "Huacan Wang",
        "Ronghao Chen",
        "Liwen Zhang"
      ],
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "published": "2026-02-05T07:11:05Z",
      "updated": "2026-02-05T07:11:05Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05386v1",
      "abs_url": "http://arxiv.org/abs/2602.05386v1",
      "summary": "提出Spider-Sense框架，通过内在风险感知和分层防御机制，提升智能体的安全性和效率。",
      "key_contributions": [
        "提出Spider-Sense框架，实现内在风险感知",
        "设计分层防御机制，平衡效率和精度",
        "构建S$^2$Bench基准测试，促进agent安全评估"
      ],
      "methodology": "采用内在风险感知触发防御，结合轻量级相似度匹配和深度内部推理的分层防御机制。",
      "tags": [
        "AI Agent Security",
        "Intrinsic Risk Sensing",
        "Hierarchical Defense",
        "Benchmark"
      ],
      "assigned_category": "agent",
      "relevance_score": 9,
      "relevance_reason": "论文核心关注LLM智能体的安全防御，属于agent领域的核心研究方向。",
      "analyzed_at": "2026-02-06T06:53:45.443787",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05384v1",
      "title": "Dolphin-v2: Universal Document Parsing via Scalable Anchor Prompting",
      "abstract": "Document parsing has garnered widespread attention as vision-language models (VLMs) advance OCR capabilities. However, the field remains fragmented across dozens of specialized models with varying strengths, forcing users to navigate complex model selection and limiting system scalability. Moreover, existing two-stage approaches depend on axis-aligned bounding boxes for layout detection, failing to handle distorted or photographed documents effectively. To this end, we present Dolphin-v2, a two-stage document image parsing model that substantially improves upon the original Dolphin. In the first stage, Dolphin-v2 jointly performs document type classification (digital-born versus photographed) alongside layout analysis. For digital-born documents, it conducts finer-grained element detection with reading order prediction. In the second stage, we employ a hybrid parsing strategy: photographed documents are parsed holistically as complete pages to handle geometric distortions, while digital-born documents undergo element-wise parallel parsing guided by the detected layout anchors, enabling efficient content extraction. Compared with the original Dolphin, Dolphin-v2 introduces several crucial enhancements: (1) robust parsing of photographed documents via holistic page-level understanding, (2) finer-grained element detection (21 categories) with semantic attribute extraction such as author information and document metadata, and (3) code block recognition with indentation preservation, which existing systems typically lack. Comprehensive evaluations are conducted on DocPTBench, OmniDocBench, and our self-constructed RealDoc-160 benchmark. The results demonstrate substantial improvements: +14.78 points overall on the challenging OmniDocBench and 91% error reduction on photographed documents, while maintaining efficient inference through parallel processing.",
      "authors": [
        "Hao Feng",
        "Wei Shi",
        "Ke Zhang",
        "Xiang Fei",
        "Lei Liao",
        "Dingkang Yang",
        "Yongkun Du",
        "Xuecheng Wu",
        "Jingqun Tang",
        "Yang Liu",
        "Hong Chen",
        "Can Huang"
      ],
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2026-02-05T07:09:57Z",
      "updated": "2026-02-05T07:09:57Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05384v1",
      "abs_url": "http://arxiv.org/abs/2602.05384v1",
      "summary": "Dolphin-v2通过可扩展的Anchor Prompting实现了通用文档解析，提升了多种文档的解析性能。",
      "key_contributions": [
        "针对拍摄文档的鲁棒解析",
        "更细粒度的元素检测与语义属性提取",
        "代码块识别与缩进保持"
      ],
      "methodology": "两阶段模型，第一阶段进行文档类型分类和布局分析；第二阶段采用混合解析策略，根据文档类型进行整体或元素级解析。",
      "tags": [
        "文档解析",
        "视觉语言模型",
        "布局分析"
      ],
      "assigned_category": "multimodal",
      "relevance_score": 9,
      "relevance_reason": "论文核心关注多模态文档解析，属于该领域关键问题。",
      "analyzed_at": "2026-02-06T06:53:46.932624",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.05382v1",
      "title": "VRIQ: Benchmarking and Analyzing Visual-Reasoning IQ of VLMs",
      "abstract": "Recent progress in Vision Language Models (VLMs) has raised the question of whether they can reliably perform nonverbal reasoning. To this end, we introduce VRIQ (Visual Reasoning IQ), a novel benchmark designed to assess and analyze the visual reasoning ability of VLMs. We evaluate models on two sets of tasks: abstract puzzle-style and natural-image reasoning tasks. We find that on abstract puzzles, performance remains near random with an average accuracy of around 28%, while natural tasks yield better but still weak results with 45% accuracy. We also find that tool-augmented reasoning demonstrates only modest improvements. To uncover the source of this weakness, we introduce diagnostic probes targeting perception and reasoning. Our analysis demonstrates that around 56% of failures arise from perception alone, 43% from both perception and reasoning, and only a mere 1% from reasoning alone. This motivates us to design fine-grained diagnostic probe questions targeting specific perception categories (e.g., shape, count, position, 3D/depth), revealing that certain categories cause more failures than others. Our benchmark and analysis establish that current VLMs, even with visual reasoning tools, remain unreliable abstract reasoners, mostly due to perception limitations, and offer a principled basis for improving visual reasoning in multimodal systems.",
      "authors": [
        "Tina Khezresmaeilzadeh",
        "Jike Zhong",
        "Konstantinos Psounis"
      ],
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "published": "2026-02-05T07:07:27Z",
      "updated": "2026-02-05T07:07:27Z",
      "pdf_url": "https://arxiv.org/pdf/2602.05382v1",
      "abs_url": "http://arxiv.org/abs/2602.05382v1",
      "summary": "VRIQ基准测试VLMs的视觉推理能力，发现感知是主要瓶颈。",
      "key_contributions": [
        "提出VRIQ基准测试，评估VLMs的视觉推理能力",
        "分析了VLMs在视觉推理上的弱点，发现感知是主要瓶颈",
        "设计了细粒度的诊断探针，揭示了特定感知类别的失败原因"
      ],
      "methodology": "构建抽象谜题和自然图像推理任务，评估VLMs性能，并设计诊断探针分析失败原因。",
      "tags": [
        "视觉推理",
        "VLM",
        "基准测试",
        "感知",
        "诊断探针"
      ],
      "assigned_category": "multimodal",
      "relevance_score": 9,
      "relevance_reason": "论文直接评估VLMs的视觉推理能力，属于多模态学习核心问题。",
      "analyzed_at": "2026-02-06T06:53:48.716274",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    }
  ],
  "fetch_time": "2026-02-06T06:53:48.716444"
}