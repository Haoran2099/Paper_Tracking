{
  "date": "2026-02-10",
  "papers": [
    {
      "arxiv_id": "2602.09018v1",
      "title": "Robustness Is a Function, Not a Number: A Factorized Comprehensive Study of OOD Robustness in Vision-Based Driving",
      "abstract": "Out of distribution (OOD) robustness in autonomous driving is often reduced to a single number, hiding what breaks a policy. We decompose environments along five axes: scene (rural/urban), season, weather, time (day/night), and agent mix; and measure performance under controlled $k$-factor perturbations ($k \\in \\{0,1,2,3\\}$). Using closed loop control in VISTA, we benchmark FC, CNN, and ViT policies, train compact ViT heads on frozen foundation-model (FM) features, and vary ID support in scale, diversity, and temporal context. (1) ViT policies are markedly more OOD-robust than comparably sized CNN/FC, and FM features yield state-of-the-art success at a latency cost. (2) Naive temporal inputs (multi-frame) do not beat the best single-frame baseline. (3) The largest single factor drops are rural $\\rightarrow$ urban and day $\\rightarrow$ night ($\\sim 31\\%$ each); actor swaps $\\sim 10\\%$, moderate rain $\\sim 7\\%$; season shifts can be drastic, and combining a time flip with other changes further degrades performance. (4) FM-feature policies stay above $85\\%$ under three simultaneous changes; non-FM single-frame policies take a large first-shift hit, and all no-FM models fall below $50\\%$ by three changes. (5) Interactions are non-additive: some pairings partially offset, whereas season-time combinations are especially harmful. (6) Training on winter/snow is most robust to single-factor shifts, while a rural+summer baseline gives the best overall OOD performance. (7) Scaling traces/views improves robustness ($+11.8$ points from $5$ to $14$ traces), yet targeted exposure to hard conditions can substitute for scale. (8) Using multiple ID environments broadens coverage and strengthens weak cases (urban OOD $60.6\\% \\rightarrow 70.1\\%$) with a small ID drop; single-ID preserves peak performance but in a narrow domain. These results yield actionable design rules for OOD-robust driving policies.",
      "authors": [
        "Amir Mallak",
        "Alaa Maalouf"
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "published": "2026-02-09T18:59:03Z",
      "updated": "2026-02-09T18:59:03Z",
      "pdf_url": "https://arxiv.org/pdf/2602.09018v1",
      "abs_url": "http://arxiv.org/abs/2602.09018v1",
      "summary": "论文研究视觉自动驾驶中OOD鲁棒性，并提出可行的设计规则。",
      "key_contributions": [
        "分解环境因素，系统评估OOD鲁棒性。",
        "对比FC、CNN、ViT等模型，发现ViT更鲁棒。",
        "提出针对OOD鲁棒性的模型训练和优化策略。"
      ],
      "methodology": "在VISTA模拟器中，通过控制多因素扰动，评估不同模型在闭环控制下的性能，并分析各因素的影响。",
      "tags": [
        "自动驾驶",
        "OOD鲁棒性",
        "视觉感知",
        "Transformer",
        "迁移学习"
      ],
      "assigned_category": "agent",
      "relevance_score": 7,
      "relevance_reason": "涉及Agent的OOD鲁棒性，但主要集中在视觉感知方面。",
      "analyzed_at": "2026-02-10T07:02:30.777044",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.09014v1",
      "title": "ArcFlow: Unleashing 2-Step Text-to-Image Generation via High-Precision Non-Linear Flow Distillation",
      "abstract": "Diffusion models have achieved remarkable generation quality, but they suffer from significant inference cost due to their reliance on multiple sequential denoising steps, motivating recent efforts to distill this inference process into a few-step regime. However, existing distillation methods typically approximate the teacher trajectory by using linear shortcuts, which makes it difficult to match its constantly changing tangent directions as velocities evolve across timesteps, thereby leading to quality degradation. To address this limitation, we propose ArcFlow, a few-step distillation framework that explicitly employs non-linear flow trajectories to approximate pre-trained teacher trajectories. Concretely, ArcFlow parameterizes the velocity field underlying the inference trajectory as a mixture of continuous momentum processes. This enables ArcFlow to capture velocity evolution and extrapolate coherent velocities to form a continuous non-linear trajectory within each denoising step. Importantly, this parameterization admits an analytical integration of this non-linear trajectory, which circumvents numerical discretization errors and results in high-precision approximation of the teacher trajectory. To train this parameterization into a few-step generator, we implement ArcFlow via trajectory distillation on pre-trained teacher models using lightweight adapters. This strategy ensures fast, stable convergence while preserving generative diversity and quality. Built on large-scale models (Qwen-Image-20B and FLUX.1-dev), ArcFlow only fine-tunes on less than 5% of original parameters and achieves a 40x speedup with 2 NFEs over the original multi-step teachers without significant quality degradation. Experiments on benchmarks show the effectiveness of ArcFlow both qualitatively and quantitatively.",
      "authors": [
        "Zihan Yang",
        "Shuyuan Tu",
        "Licheng Zhang",
        "Qi Dai",
        "Yu-Gang Jiang",
        "Zuxuan Wu"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2026-02-09T18:56:14Z",
      "updated": "2026-02-09T18:56:14Z",
      "pdf_url": "https://arxiv.org/pdf/2602.09014v1",
      "abs_url": "http://arxiv.org/abs/2602.09014v1",
      "summary": "ArcFlow通过非线性流蒸馏加速文本到图像的生成，实现高质量快速生成。",
      "key_contributions": [
        "提出ArcFlow框架，利用非线性流近似教师轨迹",
        "使用连续动量过程混合来参数化速度场",
        "通过轨迹蒸馏和轻量级适配器实现快速稳定收敛"
      ],
      "methodology": "使用非线性流轨迹近似预训练教师轨迹，通过轨迹蒸馏微调轻量级适配器。",
      "tags": [
        "文本到图像生成",
        "扩散模型",
        "模型蒸馏",
        "非线性流"
      ],
      "assigned_category": "multimodal",
      "relevance_score": 9,
      "relevance_reason": "论文核心在于优化文本到图像生成，属于多模态学习的关键问题。",
      "analyzed_at": "2026-02-10T07:02:32.913565",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.09009v1",
      "title": "ANCRe: Adaptive Neural Connection Reassignment for Efficient Depth Scaling",
      "abstract": "Scaling network depth has been a central driver behind the success of modern foundation models, yet recent investigations suggest that deep layers are often underutilized. This paper revisits the default mechanism for deepening neural networks, namely residual connections, from an optimization perspective. Rigorous analysis proves that the layout of residual connections can fundamentally shape convergence behavior, and even induces an exponential gap in convergence rates. Prompted by this insight, we introduce adaptive neural connection reassignment (ANCRe), a principled and lightweight framework that parameterizes and learns residual connectivities from the data. ANCRe adaptively reassigns residual connections with negligible computational and memory overhead ($<1\\%$), while enabling more effective utilization of network depth. Extensive numerical tests across pre-training of large language models, diffusion models, and deep ResNets demonstrate consistently accelerated convergence, boosted performance, and enhanced depth efficiency over conventional residual connections.",
      "authors": [
        "Yilang Zhang",
        "Bingcong Li",
        "Niao He",
        "Georgios B. Giannakis"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "published": "2026-02-09T18:54:18Z",
      "updated": "2026-02-09T18:54:18Z",
      "pdf_url": "https://arxiv.org/pdf/2602.09009v1",
      "abs_url": "http://arxiv.org/abs/2602.09009v1",
      "summary": "提出自适应神经连接重分配(ANCRe)框架，优化残差连接，提升深度网络的效率。",
      "key_contributions": [
        "提出ANCRe框架，自适应学习残差连接",
        "证明残差连接布局影响收敛速度",
        "在多种模型上验证了ANCRe的有效性"
      ],
      "methodology": "通过参数化和学习残差连接性，ANCRe框架自适应地重新分配残差连接，开销极小。",
      "tags": [
        "深度学习",
        "残差连接",
        "优化",
        "神经网络",
        "深度缩放"
      ],
      "assigned_category": "agent_tuning",
      "relevance_score": 6,
      "relevance_reason": "涉及网络结构优化，可能影响agent的性能。",
      "analyzed_at": "2026-02-10T07:02:37.841102",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.09007v1",
      "title": "GEBench: Benchmarking Image Generation Models as GUI Environments",
      "abstract": "Recent advancements in image generation models have enabled the prediction of future Graphical User Interface (GUI) states based on user instructions. However, existing benchmarks primarily focus on general domain visual fidelity, leaving the evaluation of state transitions and temporal coherence in GUI-specific contexts underexplored. To address this gap, we introduce GEBench, a comprehensive benchmark for evaluating dynamic interaction and temporal coherence in GUI generation. GEBench comprises 700 carefully curated samples spanning five task categories, covering both single-step interactions and multi-step trajectories across real-world and fictional scenarios, as well as grounding point localization. To support systematic evaluation, we propose GE-Score, a novel five-dimensional metric that assesses Goal Achievement, Interaction Logic, Content Consistency, UI Plausibility, and Visual Quality. Extensive evaluations on current models indicate that while they perform well on single-step transitions, they struggle significantly with maintaining temporal coherence and spatial grounding over longer interaction sequences. Our findings identify icon interpretation, text rendering, and localization precision as critical bottlenecks. This work provides a foundation for systematic assessment and suggests promising directions for future research toward building high-fidelity generative GUI environments. The code is available at: https://github.com/stepfun-ai/GEBench.",
      "authors": [
        "Haodong Li",
        "Jingwei Wu",
        "Quan Sun",
        "Guopeng Li",
        "Juanxi Tian",
        "Huanyu Zhang",
        "Yanlin Lai",
        "Ruichuan An",
        "Hongbo Peng",
        "Yuhong Dai",
        "Chenxi Li",
        "Chunmei Qing",
        "Jia Wang",
        "Ziyang Meng",
        "Zheng Ge",
        "Xiangyu Zhang",
        "Daxin Jiang"
      ],
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "published": "2026-02-09T18:52:02Z",
      "updated": "2026-02-09T18:52:02Z",
      "pdf_url": "https://arxiv.org/pdf/2602.09007v1",
      "abs_url": "http://arxiv.org/abs/2602.09007v1",
      "summary": "GEBench提出了一个评估GUI图像生成模型在动态交互和时间一致性方面的基准。",
      "key_contributions": [
        "提出了GEBench基准数据集",
        "提出了GE-Score评估指标",
        "评估了现有模型在GUI生成任务上的表现"
      ],
      "methodology": "构建包含多种GUI交互场景的数据集，并设计五维评估指标来衡量生成模型的效果。",
      "tags": [
        "GUI generation",
        "image generation",
        "benchmark",
        "evaluation"
      ],
      "assigned_category": "multimodal",
      "relevance_score": 8,
      "relevance_reason": "涉及图像生成，是多模态学习的重要应用场景。",
      "analyzed_at": "2026-02-10T07:02:39.277696",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.09000v1",
      "title": "iGRPO: Self-Feedback-Driven LLM Reasoning",
      "abstract": "Large Language Models (LLMs) have shown promise in solving complex mathematical problems, yet they still fall short of producing accurate and consistent solutions. Reinforcement Learning (RL) is a framework for aligning these models with task-specific rewards, improving overall quality and reliability. Group Relative Policy Optimization (GRPO) is an efficient, value-function-free alternative to Proximal Policy Optimization (PPO) that leverages group-relative reward normalization. We introduce Iterative Group Relative Policy Optimization (iGRPO), a two-stage extension of GRPO that adds dynamic self-conditioning through model-generated drafts. In Stage 1, iGRPO samples multiple exploratory drafts and selects the highest-reward draft using the same scalar reward signal used for optimization. In Stage 2, it appends this best draft to the original prompt and applies a GRPO-style update on draft-conditioned refinements, training the policy to improve beyond its strongest prior attempt. Under matched rollout budgets, iGRPO consistently outperforms GRPO across base models (e.g., Nemotron-H-8B-Base-8K and DeepSeek-R1 Distilled), validating its effectiveness on diverse reasoning benchmarks. Moreover, applying iGRPO to OpenReasoning-Nemotron-7B trained on AceReason-Math achieves new state-of-the-art results of 85.62\\% and 79.64\\% on AIME24 and AIME25, respectively. Ablations further show that the refinement wrapper generalizes beyond GRPO variants, benefits from a generative judge, and alters learning dynamics by delaying entropy collapse. These results underscore the potential of iterative, self-feedback-based RL for advancing verifiable mathematical reasoning.",
      "authors": [
        "Ali Hatamizadeh",
        "Shrimai Prabhumoye",
        "Igor Gitman",
        "Ximing Lu",
        "Seungju Han",
        "Wei Ping",
        "Yejin Choi",
        "Jan Kautz"
      ],
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "published": "2026-02-09T18:45:11Z",
      "updated": "2026-02-09T18:45:11Z",
      "pdf_url": "https://arxiv.org/pdf/2602.09000v1",
      "abs_url": "http://arxiv.org/abs/2602.09000v1",
      "summary": "提出iGRPO，一种基于自反馈的LLM推理优化方法，并在数学推理任务上取得了SOTA结果。",
      "key_contributions": [
        "提出了一种新的基于自反馈的强化学习方法iGRPO",
        "iGRPO在数学推理任务上优于GRPO",
        "在AIME24和AIME25上取得了新的SOTA结果"
      ],
      "methodology": "通过两阶段迭代优化，首先生成多个草稿并选择最佳草稿，然后基于最佳草稿进行GRPO风格的更新。",
      "tags": [
        "LLM",
        "Reasoning",
        "Reinforcement Learning",
        "Self-Feedback"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 9,
      "relevance_reason": "论文专注于提升LLM的推理能力，并提出了新的优化方法。",
      "analyzed_at": "2026-02-10T07:02:42.979012",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.08990v1",
      "title": "InternAgent-1.5: A Unified Agentic Framework for Long-Horizon Autonomous Scientific Discovery",
      "abstract": "We introduce InternAgent-1.5, a unified system designed for end-to-end scientific discovery across computational and empirical domains. The system is built on a structured architecture composed of three coordinated subsystems for generation, verification, and evolution. These subsystems are supported by foundational capabilities for deep research, solution optimization, and long horizon memory. The architecture allows InternAgent-1.5 to operate continuously across extended discovery cycles while maintaining coherent and improving behavior. It also enables the system to coordinate computational modeling and laboratory experimentation within a single unified system. We evaluate InternAgent-1.5 on scientific reasoning benchmarks such as GAIA, HLE, GPQA, and FrontierScience, and the system achieves leading performance that demonstrates strong foundational capabilities. Beyond these benchmarks, we further assess two categories of discovery tasks. In algorithm discovery tasks, InternAgent-1.5 autonomously designs competitive methods for core machine learning problems. In empirical discovery tasks, it executes complete computational or wet lab experiments and produces scientific findings in earth, life, biological, and physical domains. Overall, these results show that InternAgent-1.5 provides a general and scalable framework for autonomous scientific discovery.",
      "authors": [
        "Shiyang Feng",
        "Runmin Ma",
        "Xiangchao Yan",
        "Yue Fan",
        "Yusong Hu",
        "Songtao Huang",
        "Shuaiyu Zhang",
        "Zongsheng Cao",
        "Tianshuo Peng",
        "Jiakang Yuan",
        "Zijie Guo",
        "Zhijie Zhong",
        "Shangheng Du",
        "Weida Wang",
        "Jinxin Shi",
        "Yuhao Zhou",
        "Xiaohan He",
        "Zhiyin Yu",
        "Fangchen Yu",
        "Qihao Zheng",
        "Jiamin Wu",
        "Mianxin Liu",
        "Chi Zhang",
        "Shaowei Hou",
        "Shuya Li",
        "Yankai Jiang",
        "Wenjie Lou",
        "Lilong Wang",
        "Zifu Wang",
        "Jiong Wang",
        "Wanghan Xu",
        "Yue Deng",
        "Dongrui Liu",
        "Yiheng Wang",
        "Wenlong Zhang",
        "Fenghua Ling",
        "Shufei Zhang",
        "Xiaosong Wang",
        "Shuangjia Zheng",
        "Xun Huang",
        "Siqi Sun",
        "Shuyue Hu",
        "Peng Ye",
        "Chunfeng Song",
        "Bin Wang",
        "Conghui He",
        "Yihao Liu",
        "Xin Li",
        "Qibin Hou",
        "Tao Chen",
        "Xiangyu Yue",
        "Bin Wang",
        "Liang He",
        "Dahua Lin",
        "Bowen Zhou",
        "Bo Zhang",
        "Lei Bai"
      ],
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "published": "2026-02-09T18:36:06Z",
      "updated": "2026-02-09T18:36:06Z",
      "pdf_url": "https://arxiv.org/pdf/2602.08990v1",
      "abs_url": "http://arxiv.org/abs/2602.08990v1",
      "summary": "InternAgent-1.5是一个用于端到端自主科学发现的统一智能体框架。",
      "key_contributions": [
        "提出一个用于科学发现的统一系统InternAgent-1.5",
        "设计生成、验证和演化的三子系统架构",
        "在多种科学推理基准和发现任务上验证了系统性能"
      ],
      "methodology": "构建三子系统架构，利用深度研究、优化和记忆能力，在计算和实验环境中进行持续自主科学发现。",
      "tags": [
        "AI Agents",
        "Scientific Discovery",
        "Autonomous Systems"
      ],
      "assigned_category": "agent",
      "relevance_score": 10,
      "relevance_reason": "论文核心在于构建自主智能体进行科学发现，与Agent主题高度相关。",
      "analyzed_at": "2026-02-10T07:02:45.221599",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.08965v1",
      "title": "Learning to Coordinate via Quantum Entanglement in Multi-Agent Reinforcement Learning",
      "abstract": "The inability to communicate poses a major challenge to coordination in multi-agent reinforcement learning (MARL). Prior work has explored correlating local policies via shared randomness, sometimes in the form of a correlation device, as a mechanism to assist in decentralized decision-making. In contrast, this work introduces the first framework for training MARL agents to exploit shared quantum entanglement as a coordination resource, which permits a larger class of communication-free correlated policies than shared randomness alone. This is motivated by well-known results in quantum physics which posit that, for certain single-round cooperative games with no communication, shared quantum entanglement enables strategies that outperform those that only use shared randomness. In such cases, we say that there is quantum advantage. Our framework is based on a novel differentiable policy parameterization that enables optimization over quantum measurements, together with a novel policy architecture that decomposes joint policies into a quantum coordinator and decentralized local actors. To illustrate the effectiveness of our proposed method, we first show that we can learn, purely from experience, strategies that attain quantum advantage in single-round games that are treated as black box oracles. We then demonstrate how our machinery can learn policies with quantum advantage in an illustrative multi-agent sequential decision-making problem formulated as a decentralized partially observable Markov decision process (Dec-POMDP).",
      "authors": [
        "John Gardiner",
        "Orlando Romero",
        "Brendan Tivnan",
        "Nicolò Dal Fabbro",
        "George J. Pappas"
      ],
      "categories": [
        "cs.MA",
        "cs.LG"
      ],
      "primary_category": "cs.MA",
      "published": "2026-02-09T18:01:40Z",
      "updated": "2026-02-09T18:01:40Z",
      "pdf_url": "https://arxiv.org/pdf/2602.08965v1",
      "abs_url": "http://arxiv.org/abs/2602.08965v1",
      "summary": "提出一种利用量子纠缠增强多智能体强化学习协调能力的新框架。",
      "key_contributions": [
        "提出基于量子纠缠的多智能体强化学习框架",
        "设计可微的量子测量策略参数化方法",
        "提出分解联合策略的量子协调器架构"
      ],
      "methodology": "设计新型策略参数化和架构，通过黑盒游戏和Dec-POMDP验证量子优势。",
      "tags": [
        "多智能体强化学习",
        "量子纠缠",
        "协调",
        "量子优势"
      ],
      "assigned_category": "agent",
      "relevance_score": 8,
      "relevance_reason": "该论文研究多智能体协调问题，属于AI Agent领域的重要分支。",
      "analyzed_at": "2026-02-10T07:02:47.660972",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.08949v1",
      "title": "Digital Twin and Agentic AI for Wild Fire Disaster Management: Intelligent Virtual Situation Room",
      "abstract": "According to the United Nations, wildfire frequency and intensity are projected to increase by approximately 14% by 2030 and 30% by 2050 due to global warming, posing critical threats to life, infrastructure, and ecosystems. Conventional disaster management frameworks rely on static simulations and passive data acquisition, hindering their ability to adapt to arbitrarily evolving wildfire episodes in real-time. To address these limitations, we introduce the Intelligent Virtual Situation Room (IVSR), a bidirectional Digital Twin (DT) platform augmented by autonomous AI agents. The IVSR continuously ingests multisource sensor imagery, weather data, and 3D forest models to create a live virtual replica of the fire environment. A similarity engine powered by AI aligns emerging conditions with a precomputed Disaster Simulation Library, retrieving and calibrating intervention tactics under the watchful eyes of experts. Authorized action-ranging from UAV redeployment to crew reallocation-is cycled back through standardized procedures to the physical layer, completing the loop between response and analysis. We validate IVSR through detailed case-study simulations provided by an industrial partner, demonstrating capabilities in localized incident detection, privacy-preserving playback, collider-based fire-spread projection, and site-specific ML retraining. Our results indicate marked reductions in detection-to-intervention latency and more effective resource coordination versus traditional systems. By uniting real-time bidirectional DTs with agentic AI, IVSR offers a scalable, semi-automated decision-support paradigm for proactive, adaptive wildfire disaster management.",
      "authors": [
        "Mohammad Morsali",
        "Siavash H. Khajavi"
      ],
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "published": "2026-02-09T17:44:52Z",
      "updated": "2026-02-09T17:44:52Z",
      "pdf_url": "https://arxiv.org/pdf/2602.08949v1",
      "abs_url": "http://arxiv.org/abs/2602.08949v1",
      "summary": "提出了结合数字孪生和智能体AI的智能虚拟情境室，用于野火灾害管理。",
      "key_contributions": [
        "构建了双向数字孪生平台IVSR",
        "利用AI智能体实现半自动化决策支持",
        "验证了IVSR在野火管理中的有效性"
      ],
      "methodology": "构建实时数字孪生环境，结合AI智能体进行态势感知和资源调配，并通过案例研究验证。",
      "tags": [
        "数字孪生",
        "AI Agent",
        "野火管理",
        "灾害响应",
        "智能决策"
      ],
      "assigned_category": "agent",
      "relevance_score": 9,
      "relevance_reason": "核心涉及AI Agent的构建和应用，解决实际灾害管理问题。",
      "analyzed_at": "2026-02-10T07:02:52.516833",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.08945v1",
      "title": "GitSearch: Enhancing Community Notes Generation with Gap-Informed Targeted Search",
      "abstract": "Community-based moderation offers a scalable alternative to centralized fact-checking, yet it faces significant structural challenges, and existing AI-based methods fail in \"cold start\" scenarios. To tackle these challenges, we introduce GitSearch (Gap-Informed Targeted Search), a framework that treats human-perceived quality gaps, such as missing context, etc., as first-class signals. GitSearch has a three-stage pipeline: identifying information deficits, executing real-time targeted web-retrieval to resolve them, and synthesizing platform-compliant notes. To facilitate evaluation, we present PolBench, a benchmark of 78,698 U.S. political tweets with their associated Community Notes. We find GitSearch achieves 99% coverage, almost doubling coverage over the state-of-the-art. GitSearch surpasses human-authored helpful notes with a 69% win rate and superior helpfulness scores (3.87 vs. 3.36), demonstrating retrieval effectiveness that balanced the trade-off between scale and quality.",
      "authors": [
        "Sahajpreet Singh",
        "Kokil Jaidka",
        "Min-Yen Kan"
      ],
      "categories": [
        "cs.CL",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "published": "2026-02-09T17:42:32Z",
      "updated": "2026-02-09T17:42:32Z",
      "pdf_url": "https://arxiv.org/pdf/2602.08945v1",
      "abs_url": "http://arxiv.org/abs/2602.08945v1",
      "summary": "GitSearch通过识别信息缺失并检索相关信息，提升社区笔记的生成效果。",
      "key_contributions": [
        "提出GitSearch框架，优化社区笔记生成",
        "构建PolBench基准数据集",
        "实验证明GitSearch优于现有方法和人工标注"
      ],
      "methodology": "GitSearch包含信息缺失识别、实时目标网络检索和平台兼容笔记合成三个阶段。",
      "tags": [
        "社区笔记",
        "信息检索",
        "信息缺失",
        "自动生成"
      ],
      "assigned_category": "memory",
      "relevance_score": 8,
      "relevance_reason": "通过检索增强生成，与RAG密切相关。",
      "analyzed_at": "2026-02-10T07:02:54.445192",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.08939v1",
      "title": "CausalT5K: Diagnosing and Informing Refusal for Trustworthy Causal Reasoning of Skepticism, Sycophancy, Detection-Correction, and Rung Collapse",
      "abstract": "LLM failures in causal reasoning, including sycophancy, rung collapse, and miscalibrated refusal, are well-documented, yet progress on remediation is slow because no benchmark enables systematic diagnosis. We introduce CausalT5K, a diagnostic benchmark of over 5,000 cases across 10 domains that tests three critical capabilities: (1) detecting rung collapse, where models answer interventional queries with associational evidence; (2) resisting sycophantic drift under adversarial pressure; and (3) generating Wise Refusals that specify missing information when evidence is underdetermined. Unlike synthetic benchmarks, CausalT5K embeds causal traps in realistic narratives and decomposes performance into Utility (sensitivity) and Safety (specificity), revealing failure modes invisible to aggregate accuracy. Developed through a rigorous human-machine collaborative pipeline involving 40 domain experts, iterative cross-validation cycles, and composite verification via rule-based, LLM, and human scoring, CausalT5K implements Pearl's Ladder of Causation as research infrastructure. Preliminary experiments reveal a Four-Quadrant Control Landscape where static audit policies universally fail, a finding that demonstrates CausalT5K's value for advancing trustworthy reasoning systems. Repository: https://github.com/genglongling/CausalT5kBench",
      "authors": [
        "Longling Geng",
        "Andy Ouyang",
        "Theodore Wu",
        "Daphne Barretto",
        "Matthew John Hayes",
        "Rachael Cooper",
        "Yuqiao Zeng",
        "Sameer Vijay",
        "Gia Ancone",
        "Ankit Rai",
        "Matthew Wolfman",
        "Patrick Flanagan",
        "Edward Y. Chang"
      ],
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "published": "2026-02-09T17:36:56Z",
      "updated": "2026-02-09T17:36:56Z",
      "pdf_url": "https://arxiv.org/pdf/2602.08939v1",
      "abs_url": "http://arxiv.org/abs/2602.08939v1",
      "summary": "CausalT5K是一个诊断LLM因果推理缺陷的基准测试，旨在提升模型的可信赖性。",
      "key_contributions": [
        "构建了一个包含5000多个案例的因果推理诊断基准CausalT5K",
        "提出了三个关键的因果推理能力：检测阶梯崩塌、抵御逢迎和生成明智拒绝",
        "揭示了静态审计策略在因果推理上的局限性"
      ],
      "methodology": "采用人机协作流程，结合领域专家、交叉验证和多种评分方式构建基准，并利用该基准评估LLM的因果推理能力。",
      "tags": [
        "因果推理",
        "LLM",
        "基准测试",
        "可信赖性"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 9,
      "relevance_reason": "论文核心关注LLM的因果推理能力和诊断，与reasoning类别直接相关。",
      "analyzed_at": "2026-02-10T07:02:56.900649",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.08916v1",
      "title": "AMS-HD: Hyperdimensional Computing for Real-Time and Energy-Efficient Acute Mountain Sickness Detection",
      "abstract": "Altitude sickness is a potentially life-threatening condition that impacts many individuals traveling to elevated altitudes. Timely detection is critical as symptoms can escalate rapidly. Early recognition enables simple interventions such as descent, oxygen, or medication, and prompt treatment can save lives by significantly lowering the risk of severe complications. Although conventional machine learning (ML) techniques have been applied to identify altitude sickness using physiological signals, such as heart rate, oxygen saturation, respiration rate, blood pressure, and body temperature, they often struggle to balance predictive performance with low hardware demands. In contrast, hyperdimensional computing (HDC) remains under-explored for this task with limited biomedical features, where it may offer a compelling alternative to existing classification models. Its vector symbolic framework is inherently suited to hardware-efficient design, making it a strong candidate for low-power systems like wearables. Leveraging lightweight computation and efficient streamlined memory usage, HDC enables real-time detection of altitude sickness from physiological parameters collected by wearable devices, achieving accuracy comparable to that of traditional ML models. We present AMS-HD, a novel system that integrates tailored feature extraction and Hadamard HV encoding to enhance both the precision and efficiency of HDC-based detection. This framework is well-positioned for deployment in wearable health monitoring platforms, enabling continuous, on-the-go tracking of acute altitude sickness.",
      "authors": [
        "Abu Masum",
        "Mehran Moghadam",
        "M. Hassan Najafi",
        "Bige Unluturk",
        "Ulkuhan Guler",
        "Sercan Aygun"
      ],
      "categories": [
        "cs.SC",
        "cs.ET",
        "cs.LG"
      ],
      "primary_category": "cs.SC",
      "published": "2026-02-09T17:16:13Z",
      "updated": "2026-02-09T17:16:13Z",
      "pdf_url": "https://arxiv.org/pdf/2602.08916v1",
      "abs_url": "http://arxiv.org/abs/2602.08916v1",
      "summary": "针对高原反应，提出了一种基于超维计算（HDC）的实时、节能检测系统AMS-HD。",
      "key_contributions": [
        "提出基于超维计算（HDC）的高原反应检测系统AMS-HD",
        "针对性地进行特征提取和Hadamard HV编码，提升检测精度和效率",
        "验证了HDC在可穿戴设备上实现实时高原反应检测的可行性"
      ],
      "methodology": "结合生理信号特征，利用超维计算构建分类模型，通过特征提取和Hadamard HV编码优化模型性能。",
      "tags": [
        "超维计算",
        "高原反应检测",
        "可穿戴设备",
        "生理信号处理"
      ],
      "assigned_category": "agent",
      "relevance_score": 5,
      "relevance_reason": "该研究涉及利用可穿戴设备收集数据并进行健康状态检测，与AI agent具有一定联系。",
      "analyzed_at": "2026-02-10T07:02:58.832090",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.08914v1",
      "title": "Gesturing Toward Abstraction: Multimodal Convention Formation in Collaborative Physical Tasks",
      "abstract": "A quintessential feature of human intelligence is the ability to create ad hoc conventions over time to achieve shared goals efficiently. We investigate how communication strategies evolve through repeated collaboration as people coordinate on shared procedural abstractions. To this end, we conducted an online unimodal study (n = 98) using natural language to probe abstraction hierarchies. In a follow-up lab study (n = 40), we examined how multimodal communication (speech and gestures) changed during physical collaboration. Pairs used augmented reality to isolate their partner's hand and voice; one participant viewed a 3D virtual tower and sent instructions to the other, who built the physical tower. Participants became faster and more accurate by establishing linguistic and gestural abstractions and using cross-modal redundancy to emphasize key changes from previous interactions. Based on these findings, we extend probabilistic models of convention formation to multimodal settings, capturing shifts in modality preferences. Our findings and model provide building blocks for designing convention-aware intelligent agents situated in the physical world.",
      "authors": [
        "Kiyosu Maeda",
        "William P. McCarthy",
        "Ching-Yi Tsai",
        "Jeffrey Mu",
        "Haoliang Wang",
        "Robert D. Hawkins",
        "Judith E. Fan",
        "Parastoo Abtahi"
      ],
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "published": "2026-02-09T17:13:34Z",
      "updated": "2026-02-09T17:13:34Z",
      "pdf_url": "https://arxiv.org/pdf/2602.08914v1",
      "abs_url": "http://arxiv.org/abs/2602.08914v1",
      "summary": "研究人机协作中语言和手势如何演化为高效的共享抽象，并构建多模态协同模型。",
      "key_contributions": [
        "揭示了物理协作中语言和手势抽象的形成机制",
        "提出了多模态环境下的概率性约定形成模型",
        "为设计具有约定意识的智能体提供基础"
      ],
      "methodology": "通过线上语言实验和线下增强现实协作实验，观察参与者如何通过语言和手势建立共享抽象。",
      "tags": [
        "人机协作",
        "多模态沟通",
        "约定形成",
        "增强现实"
      ],
      "assigned_category": "multimodal",
      "relevance_score": 8,
      "relevance_reason": "论文核心关注多模态信息（语言和手势）在协作中的应用，与多模态学习高度相关。",
      "analyzed_at": "2026-02-10T07:03:00.778322",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.08905v1",
      "title": "Efficient and Stable Reinforcement Learning for Diffusion Language Models",
      "abstract": "Reinforcement Learning (RL) is crucial for unlocking the complex reasoning capabilities of Diffusion-based Large Language Models (dLLMs). However, applying RL to dLLMs faces unique challenges in efficiency and stability. To address these challenges, we propose Spatio-Temporal Pruning (STP), a framework designed to simultaneously improve the efficiency and stability of RL for dLLMs. STP compresses the redundancy in the generative process through: (1) \\textit{spatial pruning}, which constrains the exploration space using static priors; and (2) \\textit{temporal pruning}, which bypasses redundant late-stage refinement steps. Our theoretical analysis demonstrates that STP strictly reduces the variance of the log-likelihood estimation, thereby ensuring more stable policy updates. Extensive experiments demonstrate that STP surpasses state-of-the-art baselines in both efficiency and accuracy. Our code is available at https://github.com/Lolo1222/STP.",
      "authors": [
        "Jiawei Liu",
        "Xiting Wang",
        "Yuanyuan Zhong",
        "Defu Lian",
        "Yu Yang"
      ],
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "published": "2026-02-09T17:04:23Z",
      "updated": "2026-02-09T17:04:23Z",
      "pdf_url": "https://arxiv.org/pdf/2602.08905v1",
      "abs_url": "http://arxiv.org/abs/2602.08905v1",
      "summary": "提出Spatio-Temporal Pruning(STP)框架，提升基于扩散模型的LLM的强化学习效率和稳定性。",
      "key_contributions": [
        "提出Spatio-Temporal Pruning (STP) 框架",
        "通过空间剪枝和时间剪枝压缩生成过程中的冗余",
        "理论分析证明STP降低了log-likelihood估计的方差，确保了更稳定的策略更新"
      ],
      "methodology": "通过空间剪枝约束探索空间，利用时间剪枝跳过冗余的后期优化步骤，从而提升效率和稳定性。",
      "tags": [
        "Reinforcement Learning",
        "Diffusion Language Models",
        "Pruning"
      ],
      "assigned_category": "agent_tuning",
      "relevance_score": 8,
      "relevance_reason": "论文旨在提升 dLLM 的强化学习效率和稳定性，与 Agent Tuning & Optimization 高度相关。",
      "analyzed_at": "2026-02-10T07:03:02.573661",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.08885v1",
      "title": "Breaking the Simplification Bottleneck in Amortized Neural Symbolic Regression",
      "abstract": "Symbolic regression (SR) aims to discover interpretable analytical expressions that accurately describe observed data. Amortized SR promises to be much more efficient than the predominant genetic programming SR methods, but currently struggles to scale to realistic scientific complexity. We find that a key obstacle is the lack of a fast reduction of equivalent expressions to a concise normalized form. Amortized SR has addressed this by general-purpose Computer Algebra Systems (CAS) like SymPy, but the high computational cost severely limits training and inference speed. We propose SimpliPy, a rule-based simplification engine achieving a 100-fold speed-up over SymPy at comparable quality. This enables substantial improvements in amortized SR, including scalability to much larger training sets, more efficient use of the per-expression token budget, and systematic training set decontamination with respect to equivalent test expressions. We demonstrate these advantages in our Flash-ANSR framework, which achieves much better accuracy than amortized baselines (NeSymReS, E2E) on the FastSRB benchmark. Moreover, it performs on par with state-of-the-art direct optimization (PySR) while recovering more concise instead of more complex expressions with increasing inference budget.",
      "authors": [
        "Paul Saegert",
        "Ullrich Köthe"
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SC"
      ],
      "primary_category": "cs.LG",
      "published": "2026-02-09T16:47:00Z",
      "updated": "2026-02-09T16:47:00Z",
      "pdf_url": "https://arxiv.org/pdf/2602.08885v1",
      "abs_url": "http://arxiv.org/abs/2602.08885v1",
      "summary": "论文提出SimpliPy加速符号回归简化，提升了Amortized SR的效率和准确性。",
      "key_contributions": [
        "设计了快速的规则化表达式简化引擎SimpliPy",
        "提出了Flash-ANSR框架，显著提升了Amortized SR的性能",
        "实现了训练集去污染，避免测试集表达式的等价性问题"
      ],
      "methodology": "通过规则化引擎SimpliPy加速表达式简化，集成到Flash-ANSR框架中，优化Amortized SR的训练和推理。",
      "tags": [
        "符号回归",
        "表达式简化",
        "机器学习",
        "AI加速",
        "神经网络"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 7,
      "relevance_reason": "符号回归本质上是寻找符合观测数据的逻辑表达式的过程，涉及推理。",
      "analyzed_at": "2026-02-10T07:03:04.752324",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.08872v1",
      "title": "Large Language Models for Geolocation Extraction in Humanitarian Crisis Response",
      "abstract": "Humanitarian crises demand timely and accurate geographic information to inform effective response efforts. Yet, automated systems that extract locations from text often reproduce existing geographic and socioeconomic biases, leading to uneven visibility of crisis-affected regions. This paper investigates whether Large Language Models (LLMs) can address these geographic disparities in extracting location information from humanitarian documents. We introduce a two-step framework that combines few-shot LLM-based named entity recognition with an agent-based geocoding module that leverages context to resolve ambiguous toponyms. We benchmark our approach against state-of-the-art pretrained and rule-based systems using both accuracy and fairness metrics across geographic and socioeconomic dimensions. Our evaluation uses an extended version of the HumSet dataset with refined literal toponym annotations. Results show that LLM-based methods substantially improve both the precision and fairness of geolocation extraction from humanitarian texts, particularly for underrepresented regions. By bridging advances in LLM reasoning with principles of responsible and inclusive AI, this work contributes to more equitable geospatial data systems for humanitarian response, advancing the goal of leaving no place behind in crisis analytics.",
      "authors": [
        "G. Cafferata",
        "T. Demarco",
        "K. Kalimeri",
        "Y. Mejova",
        "M. G. Beiró"
      ],
      "categories": [
        "cs.CL",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "published": "2026-02-09T16:34:25Z",
      "updated": "2026-02-09T16:34:25Z",
      "pdf_url": "https://arxiv.org/pdf/2602.08872v1",
      "abs_url": "http://arxiv.org/abs/2602.08872v1",
      "summary": "论文利用LLM提升人道主义危机响应中地理位置提取的精度和公平性。",
      "key_contributions": [
        "提出了基于LLM的两步地理位置提取框架",
        "改进了人道主义文本中地理位置提取的精度和公平性",
        "使用了扩展的HumSet数据集并提出了更精细的地名标注方法"
      ],
      "methodology": "采用基于少量样本学习的LLM进行命名实体识别，结合基于上下文的智能地理编码模块解决地名歧义。",
      "tags": [
        "LLM",
        "Geolocation Extraction",
        "Humanitarian Crisis Response",
        "Fairness",
        "Named Entity Recognition",
        "Geocoding"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 8,
      "relevance_reason": "论文利用LLM进行推理和信息提取，解决实际问题，具有较高相关性。",
      "analyzed_at": "2026-02-10T07:03:07.781156",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.08868v1",
      "title": "AnomSeer: Reinforcing Multimodal LLMs to Reason for Time-Series Anomaly Detection",
      "abstract": "Time-series anomaly detection (TSAD) with multimodal large language models (MLLMs) is an emerging area, yet a persistent challenge remains: MLLMs rely on coarse time-series heuristics but struggle with multi-dimensional, detailed reasoning, which is vital for understanding complex time-series data. We present AnomSeer to address this by reinforcing the model to ground its reasoning in precise, structural details of time series, unifying anomaly classification, localization, and explanation. At its core, an expert chain-of-thought trace is generated to provide a verifiable, fine-grained reasoning from classical analyses (e.g., statistical measures, frequency transforms). Building on this, we propose a novel time-series grounded policy optimization (TimerPO) that incorporates two additional components beyond standard reinforcement learning: a time-series grounded advantage based on optimal transport and an orthogonal projection to ensure this auxiliary granular signal does not interfere with the primary detection objective. Across diverse anomaly scenarios, AnomSeer, with Qwen2.5-VL-3B/7B-Instruct, outperforms larger commercial baselines (e.g., GPT-4o) in classification and localization accuracy, particularly on point- and frequency-driven exceptions. Moreover, it produces plausible time-series reasoning traces that support its conclusions.",
      "authors": [
        "Junru Zhang",
        "Lang Feng",
        "Haoran Shi",
        "Xu Guo",
        "Han Yu",
        "Yabo Dong",
        "Duanqing Xu"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "published": "2026-02-09T16:30:13Z",
      "updated": "2026-02-09T16:30:13Z",
      "pdf_url": "https://arxiv.org/pdf/2602.08868v1",
      "abs_url": "http://arxiv.org/abs/2602.08868v1",
      "summary": "AnomSeer通过强化MLLM对时序数据结构细节的推理，提升了异常检测、定位和解释的精度。",
      "key_contributions": [
        "提出AnomSeer框架，用于增强MLLM的时序异常检测能力",
        "引入专家CoT生成精细化推理过程",
        "提出基于最优传输的时间序列优势函数TimerPO"
      ],
      "methodology": "构建专家CoT进行分析，使用TimerPO优化策略，通过时间序列优势函数和正交投影增强推理，实现异常检测。",
      "tags": [
        "时序异常检测",
        "多模态学习",
        "大语言模型",
        "强化学习"
      ],
      "assigned_category": "multimodal",
      "relevance_score": 9,
      "relevance_reason": "核心关注MLLM在时序数据异常检测的应用，属于该领域关键研究。",
      "analyzed_at": "2026-02-10T07:03:10.094834",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.08861v1",
      "title": "TiFRe: Text-guided Video Frame Reduction for Efficient Video Multi-modal Large Language Models",
      "abstract": "With the rapid development of Large Language Models (LLMs), Video Multi-Modal Large Language Models (Video MLLMs) have achieved remarkable performance in video-language tasks such as video understanding and question answering. However, Video MLLMs face high computational costs, particularly in processing numerous video frames as input, which leads to significant attention computation overhead. A straightforward approach to reduce computational costs is to decrease the number of input video frames. However, simply selecting key frames at a fixed frame rate (FPS) often overlooks valuable information in non-key frames, resulting in notable performance degradation. To address this, we propose Text-guided Video Frame Reduction (TiFRe), a framework that reduces input frames while preserving essential video information. TiFRe uses a Text-guided Frame Sampling (TFS) strategy to select key frames based on user input, which is processed by an LLM to generate a CLIP-style prompt. Pre-trained CLIP encoders calculate the semantic similarity between the prompt and each frame, selecting the most relevant frames as key frames. To preserve video semantics, TiFRe employs a Frame Matching and Merging (FMM) mechanism, which integrates non-key frame information into the selected key frames, minimizing information loss. Experiments show that TiFRe effectively reduces computational costs while improving performance on video-language tasks.",
      "authors": [
        "Xiangtian Zheng",
        "Zishuo Wang",
        "Yuxin Peng"
      ],
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2026-02-09T16:24:53Z",
      "updated": "2026-02-09T16:24:53Z",
      "pdf_url": "https://arxiv.org/pdf/2602.08861v1",
      "abs_url": "http://arxiv.org/abs/2602.08861v1",
      "summary": "TiFRe通过文本引导的帧采样和帧匹配融合，在减少计算成本的同时提升视频语言任务性能。",
      "key_contributions": [
        "提出了文本引导的帧采样(TFS)策略，利用LLM和CLIP选择关键帧",
        "提出了帧匹配和融合(FMM)机制，将非关键帧信息融入关键帧",
        "实验证明TiFRe能有效降低计算成本并提升视频语言任务性能"
      ],
      "methodology": "利用LLM生成CLIP风格提示，计算与帧的语义相似度选择关键帧，并将非关键帧信息融合到关键帧中。",
      "tags": [
        "Video MLLM",
        "Frame Reduction",
        "Text-guided",
        "CLIP"
      ],
      "assigned_category": "multimodal",
      "relevance_score": 9,
      "relevance_reason": "该论文直接针对Video MLLM的计算效率问题，提出了新的帧采样和融合方法，核心相关。",
      "analyzed_at": "2026-02-10T07:03:12.638209",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.08848v1",
      "title": "Deciding the Satisfiability of Combined Qualitative Constraint Networks",
      "abstract": "Among the various forms of reasoning studied in the context of artificial intelligence, qualitative reasoning makes it possible to infer new knowledge in the context of imprecise, incomplete information without numerical values. In this paper, we propose a formal framework unifying several forms of extensions and combinations of qualitative formalisms, including multi-scale reasoning, temporal sequences, and loose integrations. This framework makes it possible to reason in the context of each of these combinations and extensions, but also to study in a unified way the satisfiability decision and its complexity. In particular, we establish two complementary theorems guaranteeing that the satisfiability decision is polynomial, and we use them to recover the known results of the size-topology combination. We also generalize the main definition of qualitative formalism to include qualitative formalisms excluded from the definitions of the literature, important in the context of combinations.",
      "authors": [
        "Quentin Cohen-Solal",
        "Alexandre Niveau",
        "Maroua Bouzid"
      ],
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "published": "2026-02-09T16:14:58Z",
      "updated": "2026-02-09T16:14:58Z",
      "pdf_url": "https://arxiv.org/pdf/2602.08848v1",
      "abs_url": "http://arxiv.org/abs/2602.08848v1",
      "summary": "该论文提出一个统一的定性推理框架，研究了组合定性约束网络的可满足性判定及其复杂性。",
      "key_contributions": [
        "统一了多种定性形式的扩展和组合",
        "建立了可满足性判定的多项式定理",
        "推广了定性形式主义的定义"
      ],
      "methodology": "构建形式化框架，通过理论分析和证明，研究组合定性约束网络的可满足性问题。",
      "tags": [
        "定性推理",
        "约束网络",
        "可满足性",
        "复杂性",
        "人工智能"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 8,
      "relevance_reason": "论文核心研究推理问题，具有很高的相关性。",
      "analyzed_at": "2026-02-10T07:03:14.559815",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.08835v1",
      "title": "Learning the Value Systems of Societies with Preference-based Multi-objective Reinforcement Learning",
      "abstract": "Value-aware AI should recognise human values and adapt to the value systems (value-based preferences) of different users. This requires operationalization of values, which can be prone to misspecification. The social nature of values demands their representation to adhere to multiple users while value systems are diverse, yet exhibit patterns among groups. In sequential decision making, efforts have been made towards personalization for different goals or values from demonstrations of diverse agents. However, these approaches demand manually designed features or lack value-based interpretability and/or adaptability to diverse user preferences.   We propose algorithms for learning models of value alignment and value systems for a society of agents in Markov Decision Processes (MDPs), based on clustering and preference-based multi-objective reinforcement learning (PbMORL). We jointly learn socially-derived value alignment models (groundings) and a set of value systems that concisely represent different groups of users (clusters) in a society. Each cluster consists of a value system representing the value-based preferences of its members and an approximately Pareto-optimal policy that reflects behaviours aligned with this value system. We evaluate our method against a state-of-the-art PbMORL algorithm and baselines on two MDPs with human values.",
      "authors": [
        "Andrés Holgado-Sánchez",
        "Peter Vamplew",
        "Richard Dazeley",
        "Sascha Ossowski",
        "Holger Billhardt"
      ],
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "published": "2026-02-09T16:06:36Z",
      "updated": "2026-02-09T16:06:36Z",
      "pdf_url": "https://arxiv.org/pdf/2602.08835v1",
      "abs_url": "http://arxiv.org/abs/2602.08835v1",
      "summary": "提出基于偏好的多目标强化学习方法，用于学习社会群体的价值体系。",
      "key_contributions": [
        "提出了学习价值对齐模型和社会价值体系的算法",
        "结合聚类和基于偏好的多目标强化学习",
        "学习不同用户群体的价值系统和行为策略"
      ],
      "methodology": "基于聚类和偏好驱动的多目标强化学习，学习价值对齐模型和代表用户群体的价值系统。",
      "tags": [
        "强化学习",
        "多目标优化",
        "价值对齐",
        "用户偏好",
        "聚类"
      ],
      "assigned_category": "agent_tuning",
      "relevance_score": 8,
      "relevance_reason": "论文关注学习用户偏好和价值，并用于优化智能体的行为，高度相关。",
      "analyzed_at": "2026-02-10T07:03:17.047823",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.08822v1",
      "title": "Any-to-All MRI Synthesis: A Unified Foundation Model for Nasopharyngeal Carcinoma and Its Downstream Applications",
      "abstract": "Magnetic resonance imaging (MRI) is essential for nasopharyngeal carcinoma (NPC) radiotherapy (RT), but practical constraints, such as patient discomfort, long scan times, and high costs often lead to incomplete modalities in clinical practice, compromising RT planning accuracy. Traditional MRI synthesis methods are modality-specific, limited in anatomical adaptability, and lack clinical interpretability-failing to meet NPC's RT needs. Here, we developed a unified foundation model integrating contrastive visual representation learning and vision-language alignment (VLA) to enable any-to-all MRI synthesis. The model uses a contrastive encoder for modality-invariant representations and a CLIP-based text-informed decoder for semantically consistent synthesis, supporting any-to-all MRI synthesis via one unified foundation model. Trained on 40,825 images from 13 institutions, it achieves consistently high performance (average SSIM 0.90, PSNR 27) across 26 internal/external validation sites (15,748 images), with superior synthesis fidelity and robustness to noise and domain shifts. Meanwhile, its unified representation enhances downstream RT-relevant tasks (e.g., segmentation). This work advances digital medicine solutions for NPC care by leveraging foundation models to bridge technical synthesis and clinical utility.",
      "authors": [
        "Yao Pu",
        "Yiming Shi",
        "Zhenxi Zhang",
        "Peixin Yu",
        "Yitao Zhuang",
        "Xiang Wang",
        "Hongzhao Chen",
        "Jing Cai",
        "Ge Ren"
      ],
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2026-02-09T15:56:53Z",
      "updated": "2026-02-09T15:56:53Z",
      "pdf_url": "https://arxiv.org/pdf/2602.08822v1",
      "abs_url": "http://arxiv.org/abs/2602.08822v1",
      "summary": "开发了一种用于鼻咽癌MRI图像合成的统一基础模型，提升RT规划准确性。",
      "key_contributions": [
        "提出了一种基于对比视觉表征学习和VLA的统一基础模型。",
        "实现了任意模态到任意模态的MRI合成。",
        "通过统一表示增强了下游RT相关任务（如分割）的性能。"
      ],
      "methodology": "利用对比编码器学习模态不变的表示，使用CLIP风格的文本引导解码器进行语义一致的合成。",
      "tags": [
        "MRI合成",
        "鼻咽癌",
        "基础模型",
        "对比学习",
        "视觉语言对齐"
      ],
      "assigned_category": "multimodal",
      "relevance_score": 8,
      "relevance_reason": "涉及图像和文本的多模态学习，利用文本信息引导图像生成。",
      "analyzed_at": "2026-02-10T07:03:19.782320",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.08820v1",
      "title": "Omni-Video 2: Scaling MLLM-Conditioned Diffusion for Unified Video Generation and Editing",
      "abstract": "We present Omni-Video 2, a scalable and computationally efficient model that connects pretrained multimodal large-language models (MLLMs) with video diffusion models for unified video generation and editing. Our key idea is to exploit the understanding and reasoning capabilities of MLLMs to produce explicit target captions to interpret user instructions. In this way, the rich contextual representations from the understanding model are directly used to guide the generative process, thereby improving performance on complex and compositional editing. Moreover, a lightweight adapter is developed to inject multimodal conditional tokens into pretrained text-to-video diffusion models, allowing maximum reuse of their powerful generative priors in a parameter-efficient manner. Benefiting from these designs, we scale up Omni-Video 2 to a 14B video diffusion model on meticulously curated training data with quality, supporting high quality text-to-video generation and various video editing tasks such as object removal, addition, background change, complex motion editing, \\emph{etc.} We evaluate the performance of Omni-Video 2 on the FiVE benchmark for fine-grained video editing and the VBench benchmark for text-to-video generation. The results demonstrate its superior ability to follow complex compositional instructions in video editing, while also achieving competitive or superior quality in video generation tasks.",
      "authors": [
        "Hao Yang",
        "Zhiyu Tan",
        "Jia Gong",
        "Luozheng Qin",
        "Hesen Chen",
        "Xiaomeng Yang",
        "Yuqing Sun",
        "Yuetan Lin",
        "Mengping Yang",
        "Hao Li"
      ],
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2026-02-09T15:56:05Z",
      "updated": "2026-02-09T15:56:05Z",
      "pdf_url": "https://arxiv.org/pdf/2602.08820v1",
      "abs_url": "http://arxiv.org/abs/2602.08820v1",
      "summary": "Omni-Video 2利用MLLM理解用户指令，指导视频扩散模型实现统一的视频生成与编辑。",
      "key_contributions": [
        "提出基于MLLM的视频编辑框架",
        "设计轻量级适配器以复用预训练扩散模型",
        "高质量大规模视频生成和编辑"
      ],
      "methodology": "利用MLLM生成目标字幕指导扩散模型，并开发轻量级适配器注入多模态条件token。",
      "tags": [
        "视频生成",
        "视频编辑",
        "多模态学习",
        "扩散模型",
        "MLLM"
      ],
      "assigned_category": "multimodal",
      "relevance_score": 9,
      "relevance_reason": "论文核心在于利用MLLM进行视频生成和编辑，属于多模态学习的关键研究方向。",
      "analyzed_at": "2026-02-10T07:03:21.489234",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.08819v1",
      "title": "Bayesian Preference Learning for Test-Time Steerable Reward Models",
      "abstract": "Reward models are central to aligning language models with human preferences via reinforcement learning (RL). As RL is increasingly applied to settings such as verifiable rewards and multi-objective alignment, RMs are expected to encode more complex and multifaceted preference distributions. However, classifier RMs remain static once trained, limiting their adaptability at test time. We propose Variational In-Context Reward Modeling (ICRM), a novel Bayesian reward modeling objective that enables test-time steerability via in-context preference demonstrations. ICRM casts reward modeling as amortized variational inference over a latent preference probability under the Bradley-Terry model using a conjugate Beta prior. We show that ICRM adapt to unseen preference distributions at test time for both single and multi-objective settings. With more in-context demonstrations, ICRM gains 34% accuracy on SafeRLHF and 9% accuracy on RM-Bench in the single-objective setting, while widening the Pareto frontier with a 4% gain in hypervolume on helpfulness and refusal benchmarks. We further study the practical applicability of ICRM for RL training, showing that it can effectively encode verifiable rewards by outperforming a conventional RM in math reasoning. Finally, we provide theoretical guarantees that the variational objective admits a global interior optimum with finite confidence, and we analyze how KL regularization mitigates reward over-optimization.",
      "authors": [
        "Jiwoo Hong",
        "Shao Tang",
        "Zhipeng Wang"
      ],
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "published": "2026-02-09T15:55:56Z",
      "updated": "2026-02-09T15:55:56Z",
      "pdf_url": "https://arxiv.org/pdf/2602.08819v1",
      "abs_url": "http://arxiv.org/abs/2602.08819v1",
      "summary": "提出Variational In-Context Reward Modeling (ICRM)，提升奖励模型测试时可控性和泛化能力。",
      "key_contributions": [
        "提出了一种新的贝叶斯奖励建模目标ICRM。",
        "ICRM通过上下文演示实现测试时可控性。",
        "证明ICRM在单目标和多目标设置下优于传统方法。"
      ],
      "methodology": "ICRM将奖励建模视为在Bradley-Terry模型下，使用共轭Beta先验进行潜在偏好概率的摊销变分推断。",
      "tags": [
        "Reward Modeling",
        "Bayesian Inference",
        "In-Context Learning",
        "Reinforcement Learning"
      ],
      "assigned_category": "agent_tuning",
      "relevance_score": 9,
      "relevance_reason": "论文直接针对奖励模型的可控性进行优化，是Agent Tuning的核心问题。",
      "analyzed_at": "2026-02-10T07:03:23.507155",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.08818v1",
      "title": "FlexMoRE: A Flexible Mixture of Rank-heterogeneous Experts for Efficient Federatedly-trained Large Language Models",
      "abstract": "Recent advances in mixture-of-experts architectures have shown that individual experts models can be trained federatedly, i.e., in isolation from other experts by using a common base model to facilitate coordination. However, we hypothesize that full-sized experts may not be necessary for all domains and that instead low-rank adapters may be sufficient. Here, we introduce FlexMoRE, a Flexible Mixture of Rank-heterogenous Experts, which may be either full-sized experts or adapters of a suitable rank. We systematically investigate the trade-off between expert rank and downstream task performance by evaluating $6$ experts with ranks $2^0$ to $2^{14}$ resulting in experiments covering 150 mixtures (96 with 2 experts, 54 with 7 experts) that are evaluated across $120$ tasks. For our experiments, we build on FlexOlmo and turn its pre-trained experts into low-rank versions. Our regression analysis from expert rank to downstream task performance reveals that the best-performing rank is substantially higher for reasoning-heavy benchmarks than for knowledge-heavy benchmarks. These findings on rank sensitivity come with direct implications for memory efficiency: Using optimal ranks, FlexMoRE yields improved downstream task performance (average score $47.18$) compared to the baseline FlexOlmo-style mixture of full-sized experts (average score $45.46$) at less than one third the parameters ($10.75$B for FlexMoRE vs. $33.27$B for FlexOlmo). All code will be made available.",
      "authors": [
        "Annemette Brok Pirchert",
        "Jacob Nielsen",
        "Mogens Henrik From",
        "Lukas Galke Poech",
        "Peter Schneider-Kamp"
      ],
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "published": "2026-02-09T15:54:29Z",
      "updated": "2026-02-09T15:54:29Z",
      "pdf_url": "https://arxiv.org/pdf/2602.08818v1",
      "abs_url": "http://arxiv.org/abs/2602.08818v1",
      "summary": "FlexMoRE提出了一种灵活的混合专家模型，通过异构秩专家提升联邦训练大语言模型的效率和性能。",
      "key_contributions": [
        "提出FlexMoRE，一种灵活的混合秩异构专家模型。",
        "系统性地研究了专家秩与下游任务性能之间的权衡。",
        "实验表明，最优秩的选择与任务类型（推理或知识密集型）相关，并能显著提升效率。"
      ],
      "methodology": "通过在FlexOlmo基础上，将预训练专家转化为低秩版本，并进行大量混合专家模型的下游任务性能评估和回归分析。",
      "tags": [
        "混合专家",
        "联邦学习",
        "低秩分解",
        "大语言模型",
        "模型压缩"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 8,
      "relevance_reason": "论文通过优化专家秩，提升了LLM在推理任务上的效率和性能，与reasoning领域高度相关。",
      "analyzed_at": "2026-02-10T07:03:25.703611",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.08817v1",
      "title": "Kirin: Improving ANN efficiency with SNN Hybridization",
      "abstract": "Artificial neural networks (ANNs), particularly large language models (LLMs), demonstrate powerful inference capabilities but consume substantial energy. Conversely, spiking neural networks (SNNs) exhibit exceptional energy efficiency due to their binary and event-driven characteristics, thus motivating the study of ANN-to-SNN conversion. In this process, quantization plays a pivotal role, mapping LLMs' floating-point parameters to discrete SNN parameters via the temporal dimension of the time window. However, several challenges remain in the conversion process: (i) converting high bit-width quantization values into binary spikes requires longer time windows, increasing system latency; and (ii) the inherent trade-off between the information loss of single-spike schemes and the energy costs of multi-spike ones in SNN. To address these challenges, we propose Kirin, a integer and spike hybrid based SNN to achieve accuracy lossless ANN-to-SNN conversion with time and energy efficiency. Specifically, we first propose a Spike Matrix Hybridization strategy that encoding low bit-width parameters that leading to small time window size into binary spikes while preserving the rest in integer format, thereby reducing the overall latency of SNN execution. Second, we introduce a silence threshold mechanism to regulate the timing of single-spike firing, ensuring the output is mathematically equivalent to the LLM's output and preserves accuracy. Experimental results demonstrate that Kirin, under a W4A4\\&8 quantization setting, achieves near-FP16 accuracy while reducing energy consumption by up to 84.66\\% and shortening time steps by 93.75\\%.",
      "authors": [
        "Chenyu Wang",
        "Zhanglu Yan",
        "Zhi Zhou",
        "Xu Chen",
        "Weng-Fai Wong"
      ],
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "published": "2026-02-09T15:53:26Z",
      "updated": "2026-02-09T15:53:26Z",
      "pdf_url": "https://arxiv.org/pdf/2602.08817v1",
      "abs_url": "http://arxiv.org/abs/2602.08817v1",
      "summary": "Kirin提出了一种整数和脉冲混合的SNN，实现了ANN到SNN的无损精度转换，并提高了时间和能源效率。",
      "key_contributions": [
        "提出了 Spike Matrix Hybridization 策略，降低延迟",
        "引入了 Silence Threshold 机制，保持精度",
        "实验结果表明在精度接近FP16的情况下，能源消耗降低84.66%，时间步长缩短93.75%"
      ],
      "methodology": "通过混合整数和脉冲，并引入沉默阈值机制，优化ANN到SNN的转换过程，实现精度保持和效率提升。",
      "tags": [
        "SNN",
        "ANN",
        "量化",
        "混合神经网络",
        "低功耗"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 7,
      "relevance_reason": "涉及神经网络推理优化，聚焦于效率提升。",
      "analyzed_at": "2026-02-10T07:03:27.629915",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.08809v1",
      "title": "Efficient Deep Learning for Biometrics: Overview, Challenges and Trends in Ear of Frugal AI",
      "abstract": "Recent advances in deep learning, whether on discriminative or generative tasks have been beneficial for various applications, among which security and defense. However, their increasing computational demands during training and deployment translates directly into high energy consumption. As a consequence, this induces a heavy carbon footprint which hinders their widespread use and scalability, but also a limitation when deployed on resource-constrained edge devices for real-time use. In this paper, we briefly survey efficient deep learning methods for biometric applications. Specifically, we tackle the challenges one might incur when training and deploying deep learning approaches, and provide a taxonomy of the various efficient deep learning families. Additionally, we discuss complementary metrics for evaluating the efficiency of these models such as memory, computation, latency, throughput, and advocate for universal and reproducible metrics for better comparison. Last, we give future research directions to consider.",
      "authors": [
        "Karim Haroun",
        "Aya Zitouni",
        "Aicha Zenakhri",
        "Meriem Amel Guessoum",
        "Larbi Boubchir"
      ],
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "published": "2026-02-09T15:48:34Z",
      "updated": "2026-02-09T15:48:34Z",
      "pdf_url": "https://arxiv.org/pdf/2602.08809v1",
      "abs_url": "http://arxiv.org/abs/2602.08809v1",
      "summary": "综述了生物识别中高效深度学习方法，讨论了训练和部署挑战，提出了评估指标和未来研究方向。",
      "key_contributions": [
        "综述了生物识别领域的高效深度学习方法",
        "提出了训练和部署深度学习模型的挑战和解决方法",
        "讨论了评估模型效率的指标，并提出了未来研究方向"
      ],
      "methodology": "本文采用文献综述的方法，对生物识别中高效深度学习的现有方法进行了整理和归纳。",
      "tags": [
        "深度学习",
        "生物识别",
        "高效学习",
        "边缘计算"
      ],
      "assigned_category": "agent_tuning",
      "relevance_score": 6,
      "relevance_reason": "聚焦模型效率，但生物识别应用非核心Agent相关。",
      "analyzed_at": "2026-02-10T07:03:29.872633",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.08782v1",
      "title": "Amortising Inference and Meta-Learning Priors in Neural Networks",
      "abstract": "One of the core facets of Bayesianism is in the updating of prior beliefs in light of new evidence$\\text{ -- }$so how can we maintain a Bayesian approach if we have no prior beliefs in the first place? This is one of the central challenges in the field of Bayesian deep learning, where it is not clear how to represent beliefs about a prediction task by prior distributions over model parameters. Bridging the fields of Bayesian deep learning and probabilistic meta-learning, we introduce a way to $\\textit{learn}$ a weights prior from a collection of datasets by introducing a way to perform per-dataset amortised variational inference. The model we develop can be viewed as a neural process whose latent variable is the set of weights of a BNN and whose decoder is the neural network parameterised by a sample of the latent variable itself. This unique model allows us to study the behaviour of Bayesian neural networks under well-specified priors, use Bayesian neural networks as flexible generative models, and perform desirable but previously elusive feats in neural processes such as within-task minibatching or meta-learning under extreme data-starvation.",
      "authors": [
        "Tommy Rochussen",
        "Vincent Fortuin"
      ],
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "published": "2026-02-09T15:24:07Z",
      "updated": "2026-02-09T15:24:07Z",
      "pdf_url": "https://arxiv.org/pdf/2602.08782v1",
      "abs_url": "http://arxiv.org/abs/2602.08782v1",
      "summary": "该论文提出了一种学习神经网络权重先验的方法，结合了贝叶斯深度学习和概率元学习。",
      "key_contributions": [
        "提出了一种学习权重先验的方法",
        "实现了数据集级别的摊销变分推断",
        "利用贝叶斯神经网络作为灵活的生成模型"
      ],
      "methodology": "通过引入数据集级别的摊销变分推断，学习BNN权重的先验，并将模型视为神经过程。",
      "tags": [
        "贝叶斯深度学习",
        "元学习",
        "变分推断",
        "神经网络"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 6,
      "relevance_reason": "涉及到模型参数的先验信息和推理，与reasoning领域有一定关联。",
      "analyzed_at": "2026-02-10T07:03:33.956834",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.08742v1",
      "title": "Welfarist Formulations for Diverse Similarity Search",
      "abstract": "Nearest Neighbor Search (NNS) is a fundamental problem in data structures with wide-ranging applications, such as web search, recommendation systems, and, more recently, retrieval-augmented generations (RAG). In such recent applications, in addition to the relevance (similarity) of the returned neighbors, diversity among the neighbors is a central requirement. In this paper, we develop principled welfare-based formulations in NNS for realizing diversity across attributes. Our formulations are based on welfare functions -- from mathematical economics -- that satisfy central diversity (fairness) and relevance (economic efficiency) axioms. With a particular focus on Nash social welfare, we note that our welfare-based formulations provide objective functions that adaptively balance relevance and diversity in a query-dependent manner. Notably, such a balance was not present in the prior constraint-based approach, which forced a fixed level of diversity and optimized for relevance. In addition, our formulation provides a parametric way to control the trade-off between relevance and diversity, providing practitioners with flexibility to tailor search results to task-specific requirements. We develop efficient nearest neighbor algorithms with provable guarantees for the welfare-based objectives. Notably, our algorithm can be applied on top of any standard ANN method (i.e., use standard ANN method as a subroutine) to efficiently find neighbors that approximately maximize our welfare-based objectives. Experimental results demonstrate that our approach is practical and substantially improves diversity while maintaining high relevance of the retrieved neighbors.",
      "authors": [
        "Siddharth Barman",
        "Nirjhar Das",
        "Shivam Gupta",
        "Kirankumar Shiragur"
      ],
      "categories": [
        "cs.DS",
        "cs.CG",
        "cs.GT",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.DS",
      "published": "2026-02-09T14:42:28Z",
      "updated": "2026-02-09T14:42:28Z",
      "pdf_url": "https://arxiv.org/pdf/2602.08742v1",
      "abs_url": "http://arxiv.org/abs/2602.08742v1",
      "summary": "提出了基于福利函数的近邻搜索算法，提升检索结果的多样性，并兼顾相关性。",
      "key_contributions": [
        "提出了基于福利函数的多样性近邻搜索目标函数",
        "设计了高效的算法，可与现有ANN方法结合",
        "实验证明算法有效提升多样性并保持相关性"
      ],
      "methodology": "基于福利函数（特别是Nash社会福利）构建目标函数，并设计算法优化该目标函数。",
      "tags": [
        "近邻搜索",
        "多样性",
        "福利函数",
        "推荐系统",
        "RAG"
      ],
      "assigned_category": "memory",
      "relevance_score": 9,
      "relevance_reason": "直接解决RAG中检索结果多样性的关键问题，高度相关。",
      "analyzed_at": "2026-02-10T07:03:41.088394",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.08722v1",
      "title": "QUOKA: Query-Oriented KV Selection For Efficient LLM Prefill",
      "abstract": "We present QUOKA: Query-oriented KV selection for efficient attention, a training-free and hardware agnostic sparse attention algorithm for accelerating transformer inference under chunked prefill. While many queries focus on a smaller group of keys in the attention operator, we observe that queries with low cosine similarity with respect to the mean query interact more strongly with more keys and have the greatest contribution to final attention logits. By prioritizing these low cosine similarity queries, the behavior of full attention during the prefill stage can be closely approximated. QUOKA leverages this observation, accelerating attention by (1) first retaining a small set of representative queries and (2) then subselectin the keys most aligned with those queries. Through experiments on Needle-In-A-Haystack, LongBench, RULER, and Math500, we show that, while realizing a 3x reduction in time-to-first-token, 5x speedup in attention on Nvidia GPUs and up to nearly a 7x speedup on Intel Xeon CPUs, QUOKA achieves near-baseline accuracy, utilizing 88% fewer key-value pairs per attention evaluation.",
      "authors": [
        "Dalton Jones",
        "Junyoung Park",
        "Matthew Morse",
        "Mingu Lee",
        "Chris Lott",
        "Harper Langston"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "published": "2026-02-09T14:32:26Z",
      "updated": "2026-02-09T14:32:26Z",
      "pdf_url": "https://arxiv.org/pdf/2602.08722v1",
      "abs_url": "http://arxiv.org/abs/2602.08722v1",
      "summary": "QUOKA是一种面向查询的KV选择算法，通过减少KV对数量加速LLM推理，同时保持精度。",
      "key_contributions": [
        "提出了一种新的稀疏注意力算法QUOKA",
        "基于查询与平均查询的余弦相似度进行KV选择",
        "实现了在多种硬件平台上的加速"
      ],
      "methodology": "QUOKA首先保留少量代表性查询，然后选择与这些查询最相关的keys。利用低余弦相似度查询进行更充分的注意力计算。",
      "tags": [
        "LLM",
        "Sparse Attention",
        "Inference Optimization",
        "KV Selection",
        "Acceleration"
      ],
      "assigned_category": "memory",
      "relevance_score": 8,
      "relevance_reason": "KV选择优化影响LLM memory访问效率，与RAG等应用密切相关。",
      "analyzed_at": "2026-02-10T07:03:45.658522",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.08717v1",
      "title": "Zero-shot System for Automatic Body Region Detection for Volumetric CT and MR Images",
      "abstract": "Reliable identification of anatomical body regions is a prerequisite for many automated medical imaging workflows, yet existing solutions remain heavily dependent on unreliable DICOM metadata. Current solutions mainly use supervised learning, which limits their applicability in many real-world scenarios. In this work, we investigate whether body region detection in volumetric CT and MR images can be achieved in a fully zero-shot manner by using knowledge embedded in large pre-trained foundation models. We propose and systematically evaluate three training-free pipelines: (1) a segmentation-driven rule-based system leveraging pre-trained multi-organ segmentation models, (2) a Multimodal Large Language Model (MLLM) guided by radiologist-defined rules, and (3) a segmentation-aware MLLM that combines visual input with explicit anatomical evidence. All methods are evaluated on 887 heterogeneous CT and MR scans with manually verified anatomical region labels. The segmentation-driven rule-based approach achieves the strongest and most consistent performance, with weighted F1-scores of 0.947 (CT) and 0.914 (MR), demonstrating robustness across modalities and atypical scan coverage. The MLLM performs competitively in visually distinctive regions, while the segmentation-aware MLLM reveals fundamental limitations.",
      "authors": [
        "Farnaz Khun Jush",
        "Grit Werner",
        "Mark Klemens",
        "Matthias Lenga"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2026-02-09T14:26:24Z",
      "updated": "2026-02-09T14:26:24Z",
      "pdf_url": "https://arxiv.org/pdf/2602.08717v1",
      "abs_url": "http://arxiv.org/abs/2602.08717v1",
      "summary": "提出基于预训练模型的零样本方法，用于CT和MR图像的自动身体区域检测。",
      "key_contributions": [
        "提出三种零样本身体区域检测流程。",
        "评估了分割驱动的规则系统、MLLM和分割感知MLLM。",
        "实验证明分割驱动的规则系统性能最佳。"
      ],
      "methodology": "利用预训练的分割模型和MLLM，结合规则或解剖学知识，实现无需训练的身体区域检测。",
      "tags": [
        "零样本学习",
        "医学图像分析",
        "身体区域检测",
        "多模态学习"
      ],
      "assigned_category": "multimodal",
      "relevance_score": 7,
      "relevance_reason": "使用了MLLM进行图像处理，但主要解决医学图像问题。",
      "analyzed_at": "2026-02-10T07:03:47.582864",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.08716v1",
      "title": "PERSPECTRA: A Scalable and Configurable Pluralist Benchmark of Perspectives from Arguments",
      "abstract": "Pluralism, the capacity to engage with diverse perspectives without collapsing them into a single viewpoint, is critical for developing large language models that faithfully reflect human heterogeneity. Yet this characteristic has not been carefully examined in the LLM research community and remains absent from most alignment studies. Debate-oriented sources provide a natural entry point for pluralism research. Previous work builds on online debate sources but remains constrained by costly human validation. Other debate-rich platforms such as Reddit and Kialo also offer promising material: Reddit provides linguistic diversity and scale but lacks clear argumentative structure, while Kialo supplies explicit pro/con graphs but remains overly concise and detached from natural discourse. We introduce PERSPECTRA, a pluralist benchmark that integrates the structural clarity of Kialo debate graphs with the linguistic diversity of real Reddit discussions. Using a controlled retrieval-and-expansion pipeline, we construct 3,810 enriched arguments spanning 762 pro/con stances on 100 controversial topics. Each opinion is expanded to multiple naturalistic variants, enabling robust evaluation of pluralism. We initialise three tasks with PERSPECTRA: opinion counting (identifying distinct viewpoints), opinion matching (aligning supporting stances and discourse to source opinions), and polarity check (inferring aggregate stance in mixed discourse). Experiments with state-of-the-art open-source and proprietary LLMs, highlight systematic failures, such as overestimating the number of viewpoints and misclassifying concessive structures, underscoring the difficulty of pluralism-aware understanding and reasoning. By combining diversity with structure, PERSPECTRA establishes the first scalable, configurable benchmark for evaluating how well models represent, distinguish, and reason over multiple perspectives.",
      "authors": [
        "Shangrui Nie",
        "Kian Omoomi",
        "Lucie Flek",
        "Zhixue Zhao",
        "Charles Welch"
      ],
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "published": "2026-02-09T14:25:07Z",
      "updated": "2026-02-09T14:25:07Z",
      "pdf_url": "https://arxiv.org/pdf/2602.08716v1",
      "abs_url": "http://arxiv.org/abs/2602.08716v1",
      "summary": "PERSPECTRA是一个评估LLM处理多元观点的可扩展、可配置的基准。",
      "key_contributions": [
        "构建了包含丰富论据的多元观点基准数据集PERSPECTRA",
        "提出了意见计数、意见匹配和极性检查三个任务",
        "评估了现有LLM在理解和推理多元观点方面的不足"
      ],
      "methodology": "结合Kialo辩论图的结构清晰性和Reddit讨论的语言多样性，使用受控的检索和扩展流程构建数据集。",
      "tags": [
        "LLM",
        "多元观点",
        "基准",
        "评估"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 8,
      "relevance_reason": "论文核心在于评估LLM的推理能力，特别是处理复杂和矛盾观点的能力。",
      "analyzed_at": "2026-02-10T07:03:49.706725",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.08708v1",
      "title": "Intermediate Results on the Complexity of STRIPS$_{1}^{1}$",
      "abstract": "This paper is based on Bylander's results on the computational complexity of propositional STRIPS planning. He showed that when only ground literals are permitted, determining plan existence is PSPACE-complete even if operators are limited to two preconditions and two postconditions. While NP-hardness is settled, it is unknown whether propositional STRIPS with operators that only have one precondition and one effect is NP-complete. We shed light on the question whether this small solution hypothesis for STRIPS$^1_1$ is true, calling a SAT solver for small instances, introducing the literal graph, and mapping it to Petri nets.",
      "authors": [
        "Stefan Edelkamp",
        "Jiří Fink",
        "Petr Gregor",
        "Anders Jonsson",
        "Bernhard Nebel"
      ],
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "published": "2026-02-09T14:21:10Z",
      "updated": "2026-02-09T14:21:10Z",
      "pdf_url": "https://arxiv.org/pdf/2602.08708v1",
      "abs_url": "http://arxiv.org/abs/2602.08708v1",
      "summary": "研究STRIPS规划问题，探索只有一个前置条件和一个效果的STRIPS问题的复杂性。",
      "key_contributions": [
        "使用SAT求解器解决小规模实例",
        "引入字面量图",
        "将字面量图映射到Petri网"
      ],
      "methodology": "结合SAT求解、图论和Petri网等多种方法，研究STRIPS规划的复杂性。",
      "tags": [
        "STRIPS",
        "规划",
        "复杂性",
        "SAT求解",
        "Petri网"
      ],
      "assigned_category": "agent",
      "relevance_score": 7,
      "relevance_reason": "规划问题与agent的规划能力密切相关，值得关注。",
      "analyzed_at": "2026-02-10T07:03:51.264058",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.08692v1",
      "title": "PBLean: Pseudo-Boolean Proof Certificates for Lean 4",
      "abstract": "We present PBLean, a method for importing VeriPB pseudo-Boolean (PB) proof certificates into Lean 4. Key to our approach is reflection: a Boolean checker function whose soundness is fully proved in Lean and executed as compiled native code. Our method scales to proofs with tens of thousands of steps that would exhaust memory under explicit proof-term construction. Our checker supports all VeriPB kernel rules, including cutting-plane derivations and proof-by-contradiction subproofs. In contrast to external verified checkers that produce verdicts, our integration yields Lean theorems that can serve as composable lemmas in larger formal developments. To derive theorems about the original combinatorial problems rather than about PB constraints alone, we support verified encodings. This closes the trust gap between solver output and problem semantics since the constraint translation and its correctness proof are both formalized in Lean. We demonstrate the approach on various combinatorial problems.",
      "authors": [
        "Stefan Szeider"
      ],
      "categories": [
        "cs.LO",
        "cs.AI"
      ],
      "primary_category": "cs.LO",
      "published": "2026-02-09T14:13:30Z",
      "updated": "2026-02-09T14:13:30Z",
      "pdf_url": "https://arxiv.org/pdf/2602.08692v1",
      "abs_url": "http://arxiv.org/abs/2602.08692v1",
      "summary": "PBLean将VeriPB的伪布尔证明导入Lean 4，通过反射实现验证和定理推导。",
      "key_contributions": [
        "实现了VeriPB证明到Lean 4的导入",
        "提出了基于反射的证明检查器，并验证其正确性",
        "支持验证编码，弥合求解器输出和问题语义之间的信任差距"
      ],
      "methodology": "使用Lean 4的形式化方法，通过反射实现伪布尔证明的验证，并支持编码验证。",
      "tags": [
        "Lean 4",
        "伪布尔证明",
        "形式化验证",
        "SAT/SMT"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 7,
      "relevance_reason": "涉及形式化验证和自动推理，与LLM的推理能力有一定关联。",
      "analyzed_at": "2026-02-10T07:03:56.765911",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.08683v1",
      "title": "OneVision-Encoder: Codec-Aligned Sparsity as a Foundational Principle for Multimodal Intelligence",
      "abstract": "Hypothesis. Artificial general intelligence is, at its core, a compression problem. Effective compression demands resonance: deep learning scales best when its architecture aligns with the fundamental structure of the data. These are the fundamental principles. Yet, modern vision architectures have strayed from these truths: visual signals are highly redundant, while discriminative information, the surprise, is sparse. Current models process dense pixel grids uniformly, wasting vast compute on static background rather than focusing on the predictive residuals that define motion and meaning. We argue that to solve visual understanding, we must align our architectures with the information-theoretic principles of video, i.e., Codecs.   Method. OneVision-Encoder encodes video by compressing predictive visual structure into semantic meaning. By adopting Codec Patchification, OV-Encoder abandons uniform computation to focus exclusively on the 3.1%-25% of regions rich in signal entropy. To unify spatial and temporal reasoning under irregular token layouts, OneVision-Encoder employs a shared 3D RoPE and is trained with a large-scale cluster discrimination objective over more than one million semantic concepts, jointly capturing object permanence and motion dynamics.   Evidence. The results validate our core hypothesis: efficiency and accuracy are not a trade-off; they are positively correlated. When integrated into LLM, it consistently outperforms strong vision backbones such as Qwen3-ViT and SigLIP2 across 16 image, video, and document understanding benchmarks, despite using substantially fewer visual tokens and pretraining data. Notably, on video understanding tasks, OV-Encoder achieves an average improvement of 4.1% over Qwen3-ViT. Codec-aligned, patch-level sparsity is a foundational principle, enabling OV-Encoder as a scalable engine for next-generation visual generalists.",
      "authors": [
        "Feilong Tang",
        "Xiang An",
        "Yunyao Yan",
        "Yin Xie",
        "Bin Qin",
        "Kaicheng Yang",
        "Yifei Shen",
        "Yuanhan Zhang",
        "Chunyuan Li",
        "Shikun Feng",
        "Changrui Chen",
        "Huajie Tan",
        "Ming Hu",
        "Manyuan Zhang",
        "Bo Li",
        "Ziyong Feng",
        "Ziwei Liu",
        "Zongyuan Ge",
        "Jiankang Deng"
      ],
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2026-02-09T14:06:17Z",
      "updated": "2026-02-09T14:06:17Z",
      "pdf_url": "https://arxiv.org/pdf/2602.08683v1",
      "abs_url": "http://arxiv.org/abs/2602.08683v1",
      "summary": "OV-Encoder通过编解码器对齐的稀疏性，在多模态理解上取得显著性能提升。",
      "key_contributions": [
        "提出Codec Patchification，聚焦关键区域",
        "引入3D RoPE，统一时空推理",
        "大规模概念聚类，提升视频理解"
      ],
      "methodology": "采用Codec Patchification选择高熵区域，利用3D RoPE进行时空推理，通过大规模聚类目标训练。",
      "tags": [
        "多模态学习",
        "视频理解",
        "稀疏性",
        "编解码器对齐"
      ],
      "assigned_category": "multimodal",
      "relevance_score": 9,
      "relevance_reason": "论文核心关注视觉信息的编解码和多模态学习，与multimodal类别高度相关。",
      "analyzed_at": "2026-02-10T07:03:59.776804",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.08681v1",
      "title": "The Theory and Practice of MAP Inference over Non-Convex Constraints",
      "abstract": "In many safety-critical settings, probabilistic ML systems have to make predictions subject to algebraic constraints, e.g., predicting the most likely trajectory that does not cross obstacles.   These real-world constraints are rarely convex, nor the densities considered are (log-)concave.   This makes computing this constrained maximum a posteriori (MAP) prediction efficiently and reliably extremely challenging.   In this paper, we first investigate under which conditions we can perform constrained MAP inference over continuous variables exactly and efficiently and devise a scalable message-passing algorithm for this tractable fragment.   Then, we devise a general constrained MAP strategy that interleaves partitioning the domain into convex feasible regions with numerical constrained optimization.   We evaluate both methods on synthetic and real-world benchmarks, showing our %   approaches outperform constraint-agnostic baselines, and scale to complex densities intractable for SoTA exact solvers.",
      "authors": [
        "Leander Kurscheidt",
        "Gabriele Masina",
        "Roberto Sebastiani",
        "Antonio Vergari"
      ],
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "published": "2026-02-09T14:05:58Z",
      "updated": "2026-02-09T14:05:58Z",
      "pdf_url": "https://arxiv.org/pdf/2602.08681v1",
      "abs_url": "http://arxiv.org/abs/2602.08681v1",
      "summary": "研究非凸约束下的MAP推断问题，提出了一种可扩展的消息传递算法和一种通用的约束MAP策略。",
      "key_contributions": [
        "研究了约束MAP推断的条件和可行性",
        "设计了可扩展的消息传递算法",
        "提出了一种基于凸可行域划分的通用约束MAP策略"
      ],
      "methodology": "首先分析条件，然后针对特定情况设计算法，最后提出通用的凸划分策略并进行数值优化。",
      "tags": [
        "MAP inference",
        "Non-convex constraints",
        "Optimization",
        "Message Passing"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 7,
      "relevance_reason": "涉及概率推理和优化，与LLM的推理能力有一定关联。",
      "analyzed_at": "2026-02-10T07:04:01.562974",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.08675v1",
      "title": "6G-Bench: An Open Benchmark for Semantic Communication and Network-Level Reasoning with Foundation Models in AI-Native 6G Networks",
      "abstract": "This paper introduces 6G-Bench, an open benchmark for evaluating semantic communication and network-level reasoning in AI-native 6G networks. 6G-Bench defines a taxonomy of 30 decision-making tasks (T1--T30) extracted from ongoing 6G and AI-agent standardization activities in 3GPP, IETF, ETSI, ITU-T, and the O-RAN Alliance, and organizes them into five standardization-aligned capability categories. Starting from 113,475 scenarios, we generate a balanced pool of 10,000 very-hard multiple-choice questions using task-conditioned prompts that enforce multi-step quantitative reasoning under uncertainty and worst-case regret minimization over multi-turn horizons. After automated filtering and expert human validation, 3,722 questions are retained as a high-confidence evaluation set, while the full pool is released to support training and fine-tuning of 6G-specialized models. Using 6G-Bench, we evaluate 22 foundation models spanning dense and mixture-of-experts architectures, short- and long-context designs (up to 1M tokens), and both open-weight and proprietary systems. Across models, deterministic single-shot accuracy (pass@1) spans a wide range from 0.22 to 0.82, highlighting substantial variation in semantic reasoning capability. Leading models achieve intent and policy reasoning accuracy in the range 0.87--0.89, while selective robustness analysis on reasoning-intensive tasks shows pass@5 values ranging from 0.20 to 0.91. To support open science and reproducibility, we release the 6G-Bench dataset on GitHub: https://github.com/maferrag/6G-Bench",
      "authors": [
        "Mohamed Amine Ferrag",
        "Abderrahmane Lakas",
        "Merouane Debbah"
      ],
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI",
      "published": "2026-02-09T13:57:37Z",
      "updated": "2026-02-09T13:57:37Z",
      "pdf_url": "https://arxiv.org/pdf/2602.08675v1",
      "abs_url": "http://arxiv.org/abs/2602.08675v1",
      "summary": "6G-Bench是一个用于评估6G网络中语义通信和网络推理的开放基准。",
      "key_contributions": [
        "定义了6G网络决策任务分类体系",
        "构建包含3722个高质量问题的评估集",
        "评估了22个大型模型在6G任务上的性能"
      ],
      "methodology": "基于3GPP等标准，提取决策任务，利用任务条件提示生成多选题，经过过滤和人工验证构建高质量评估集。",
      "tags": [
        "6G",
        "语义通信",
        "网络推理",
        "基准测试",
        "大语言模型"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 8,
      "relevance_reason": "侧重于LLM在6G网络推理能力评估，有重要参考价值。",
      "analyzed_at": "2026-02-10T07:04:03.456459",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.08603v1",
      "title": "OSCAR: Optimization-Steered Agentic Planning for Composed Image Retrieval",
      "abstract": "Composed image retrieval (CIR) requires complex reasoning over heterogeneous visual and textual constraints. Existing approaches largely fall into two paradigms: unified embedding retrieval, which suffers from single-model myopia, and heuristic agentic retrieval, which is limited by suboptimal, trial-and-error orchestration. To this end, we propose OSCAR, an optimization-steered agentic planning framework for composed image retrieval. We are the first to reformulate agentic CIR from a heuristic search process into a principled trajectory optimization problem. Instead of relying on heuristic trial-and-error exploration, OSCAR employs a novel offline-online paradigm. In the offline phase, we model CIR via atomic retrieval selection and composition as a two-stage mixed-integer programming problem, mathematically deriving optimal trajectories that maximize ground-truth coverage for training samples via rigorous boolean set operations. These trajectories are then stored in a golden library to serve as in-context demonstrations for online steering of VLM planner at online inference time. Extensive experiments on three public benchmarks and a private industrial benchmark show that OSCAR consistently outperforms SOTA baselines. Notably, it achieves superior performance using only 10% of training data, demonstrating strong generalization of planning logic rather than dataset-specific memorization.",
      "authors": [
        "Teng Wang",
        "Rong Shan",
        "Jianghao Lin",
        "Junjie Wu",
        "Tianyi Xu",
        "Jianping Zhang",
        "Wenteng Chen",
        "Changwang Zhang",
        "Zhaoxiang Wang",
        "Weinan Zhang",
        "Jun Wang"
      ],
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "published": "2026-02-09T12:44:56Z",
      "updated": "2026-02-09T12:44:56Z",
      "pdf_url": "https://arxiv.org/pdf/2602.08603v1",
      "abs_url": "http://arxiv.org/abs/2602.08603v1",
      "summary": "提出了OSCAR框架，通过优化指导的Agent规划实现组合图像检索，显著提升检索性能。",
      "key_contributions": [
        "将Agentic CIR重构为轨迹优化问题",
        "提出离线-在线范式，利用离线阶段的优化轨迹指导在线规划",
        "在多个数据集上超越SOTA，并展现出优秀的泛化能力"
      ],
      "methodology": "使用混合整数规划离线优化检索轨迹，生成黄金库，用于在线VLM规划器的上下文演示指导。",
      "tags": [
        "组合图像检索",
        "Agent规划",
        "轨迹优化",
        "视觉语言模型"
      ],
      "assigned_category": "agent_tuning",
      "relevance_score": 9,
      "relevance_reason": "论文核心在于Agent的规划和轨迹优化，属于Agent Tuning范畴。",
      "analyzed_at": "2026-02-10T07:04:06.328771",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.08597v1",
      "title": "An Attention Mechanism for Robust Multimodal Integration in a Global Workspace Architecture",
      "abstract": "Global Workspace Theory (GWT), inspired by cognitive neuroscience, posits that flexible cognition could arise via the attentional selection of a relevant subset of modalities within a multimodal integration system. This cognitive framework can inspire novel computational architectures for multimodal integration. Indeed, recent implementations of GWT have explored its multimodal representation capabilities, but the related attention mechanisms remain understudied. Here, we propose and evaluate a top-down attention mechanism to select modalities inside a global workspace. First, we demonstrate that our attention mechanism improves noise robustness of a global workspace system on two multimodal datasets of increasing complexity: Simple Shapes and MM-IMDb 1.0. Second, we highlight various cross-task and cross-modality generalization capabilities that are not shared by multimodal attention models from the literature. Comparing against existing baselines on the MM-IMDb 1.0 benchmark, we find our attention mechanism makes the global workspace competitive with the state of the art.",
      "authors": [
        "Roland Bertin-Johannet",
        "Lara Scipio",
        "Leopold Maytié",
        "Rufin VanRullen"
      ],
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "published": "2026-02-09T12:38:05Z",
      "updated": "2026-02-09T12:38:05Z",
      "pdf_url": "https://arxiv.org/pdf/2602.08597v1",
      "abs_url": "http://arxiv.org/abs/2602.08597v1",
      "summary": "论文提出一种顶向下注意力机制，增强全局工作空间架构在多模态任务中的噪声鲁棒性和泛化能力。",
      "key_contributions": [
        "提出了一种用于全局工作空间的顶向下注意力机制",
        "证明了该机制提升了多模态系统的噪声鲁棒性",
        "展示了该机制具有良好的跨任务和跨模态泛化能力"
      ],
      "methodology": "在全局工作空间架构中引入注意力机制，通过选择相关模态进行集成，并在多模态数据集上进行实验验证。",
      "tags": [
        "多模态学习",
        "注意力机制",
        "全局工作空间",
        "噪声鲁棒性"
      ],
      "assigned_category": "multimodal",
      "relevance_score": 9,
      "relevance_reason": "论文核心关注多模态数据的处理和集成，与该领域密切相关。",
      "analyzed_at": "2026-02-10T07:04:08.325776",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.08585v1",
      "title": "Predicting Future Utility: Global Combinatorial Optimization for Task-Agnostic KV Cache Eviction",
      "abstract": "Given the quadratic complexity of attention, KV cache eviction is vital to accelerate model inference. Current KV cache eviction methods typically rely on instantaneous heuristic metrics, implicitly assuming that score magnitudes are consistent proxies for importance across all heads. However, this overlooks the heterogeneity in predictive fidelity across attention heads. While certain heads prioritize the instantaneous contribution of tokens, others are dedicated to capturing long-horizon utility. In this paper, we propose that optimal budget allocation should be governed by the marginal utility in preserving long-term semantic information. Based on this insight, we propose LU-KV, a novel framework that optimizes head-level budget allocation through a convex-hull relaxation and a marginal-utility-based greedy solver to achieve near-optimal precision. Furthermore, we implement a data-driven offline profiling protocol to facilitate the practical deployment of LU-KV. Extensive evaluations on LongBench and RULER benchmarks demonstrate that LU-KV achieves an 80% reduction in KV cache size with minimal performance degradation, while simultaneously reducing inference latency and GPU memory footprint.",
      "authors": [
        "Ziyao Tang",
        "Pengkun Jiao",
        "Xinhang Chen",
        "Wei Liu",
        "Shiyong Li",
        "Jingjing Chen"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "published": "2026-02-09T12:23:38Z",
      "updated": "2026-02-09T12:23:38Z",
      "pdf_url": "https://arxiv.org/pdf/2602.08585v1",
      "abs_url": "http://arxiv.org/abs/2602.08585v1",
      "summary": "LU-KV通过优化头级别缓存分配，减少KV缓存大小，降低推理延迟和显存占用。",
      "key_contributions": [
        "提出LU-KV框架，通过凸包松弛和边际效用贪婪求解器优化头级别缓存分配",
        "引入数据驱动的离线分析协议，便于LU-KV的实际部署",
        "在LongBench和RULER基准测试上验证了LU-KV的有效性"
      ],
      "methodology": "基于长期语义信息保持的边际效用，通过凸包松弛和贪婪算法优化头级别预算分配。",
      "tags": [
        "KV Cache",
        "Attention",
        "Inference Optimization",
        "Memory Management"
      ],
      "assigned_category": "memory",
      "relevance_score": 9,
      "relevance_reason": "论文核心在于优化KV缓存，直接解决LLM内存管理的关键问题。",
      "analyzed_at": "2026-02-10T07:04:11.226840",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.08567v1",
      "title": "ValueFlow: Measuring the Propagation of Value Perturbations in Multi-Agent LLM Systems",
      "abstract": "Multi-agent large language model (LLM) systems increasingly consist of agents that observe and respond to one another's outputs. While value alignment is typically evaluated for isolated models, how value perturbations propagate through agent interactions remains poorly understood. We present ValueFlow, a perturbation-based evaluation framework for measuring and analyzing value drift in multi-agent systems. ValueFlow introduces a 56-value evaluation dataset derived from the Schwartz Value Survey and quantifies agents' value orientations during interaction using an LLM-as-a-judge protocol. Building on this measurement layer, ValueFlow decomposes value drift into agent-level response behavior and system-level structural effects, operationalized by two metrics: beta-susceptibility, which measures an agent's sensitivity to perturbed peer signals, and system susceptibility (SS), which captures how node-level perturbations affect final system outputs. Experiments across multiple model backbones, prompt personas, value dimensions, and network structures show that susceptibility varies widely across values and is strongly shaped by structural topology.",
      "authors": [
        "Jinnuo Liu",
        "Chuke Liu",
        "Hua Shen"
      ],
      "categories": [
        "cs.MA",
        "cs.CL"
      ],
      "primary_category": "cs.MA",
      "published": "2026-02-09T12:06:07Z",
      "updated": "2026-02-09T12:06:07Z",
      "pdf_url": "https://arxiv.org/pdf/2602.08567v1",
      "abs_url": "http://arxiv.org/abs/2602.08567v1",
      "summary": "ValueFlow框架评估多智能体LLM系统中价值观扰动的传播和影响。",
      "key_contributions": [
        "提出ValueFlow框架，用于评估多智能体系统中价值观漂移。",
        "构建包含56个价值观的评估数据集。",
        "定义β-susceptibility和系统敏感度（SS）指标，用于衡量个体和系统层面的敏感度。"
      ],
      "methodology": "基于价值观扰动，利用LLM作为裁判评估智能体的价值观取向，并分析个体和系统层面的敏感度。",
      "tags": [
        "多智能体系统",
        "价值观对齐",
        "价值观漂移",
        "评估框架"
      ],
      "assigned_category": "agent",
      "relevance_score": 9,
      "relevance_reason": "论文核心关注多智能体LLM系统的价值观问题，属于agent领域关键问题。",
      "analyzed_at": "2026-02-10T07:04:13.206464",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.08565v1",
      "title": "Agent-Supported Foresight for AI Systemic Risks: AI Agents for Breadth, Experts for Judgment",
      "abstract": "AI impact assessments often stress near-term risks because human judgment degrades over longer horizons, exemplifying the Collingridge dilemma: foresight is most needed when knowledge is scarcest. To address long-term systemic risks, we introduce a scalable approach that simulates in-silico agents using the strategic foresight method of the Futures Wheel. We applied it to four AI uses spanning Technology Readiness Levels (TRLs): Chatbot Companion (TRL 9, mature), AI Toy (TRL 7, medium), Griefbot (TRL 5, low), and Death App (TRL 2, conceptual). Across 30 agent runs per use, agents produced 86-110 consequences, condensed into 27-47 unique risks. To benchmark the agent outputs against human perspectives, we collected evaluations from 290 domain experts and 7 leaders, and conducted Futures Wheel sessions with 42 experts and 42 laypeople. Agents generated many systemic consequences across runs. Compared with these outputs, experts identified fewer risks, typically less systemic but judged more likely, whereas laypeople surfaced more emotionally salient concerns that were generally less systemic. We propose a hybrid foresight workflow, wherein agents broaden systemic coverage, and humans provide contextual grounding. Our dataset is available at: https://social-dynamics.net/ai-risks/foresight.",
      "authors": [
        "Leon Fröhling",
        "Alessandro Giaconia",
        "Edyta Paulina Bogucka",
        "Daniele Quercia"
      ],
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "published": "2026-02-09T12:03:49Z",
      "updated": "2026-02-09T12:03:49Z",
      "pdf_url": "https://arxiv.org/pdf/2602.08565v1",
      "abs_url": "http://arxiv.org/abs/2602.08565v1",
      "summary": "提出一种结合AI Agent和人类专家的混合方法，预测AI系统性风险。",
      "key_contributions": [
        "提出基于Agent的Futures Wheel方法",
        "比较Agent、专家和普通人的风险预测差异",
        "提出混合式AI风险预测工作流"
      ],
      "methodology": "使用AI Agent模拟Futures Wheel，生成风险，并通过专家评估进行验证和补充。",
      "tags": [
        "AI Agent",
        "风险预测",
        "Futures Wheel",
        "系统性风险"
      ],
      "assigned_category": "agent",
      "relevance_score": 9,
      "relevance_reason": "论文核心关注AI Agent在系统性风险预测中的应用，高度相关。",
      "analyzed_at": "2026-02-10T07:04:15.035654",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.08563v1",
      "title": "Stateless Yet Not Forgetful: Implicit Memory as a Hidden Channel in LLMs",
      "abstract": "Large language models (LLMs) are commonly treated as stateless: once an interaction ends, no information is assumed to persist unless it is explicitly stored and re-supplied. We challenge this assumption by introducing implicit memory-the ability of a model to carry state across otherwise independent interactions by encoding information in its own outputs and later recovering it when those outputs are reintroduced as input. This mechanism does not require any explicit memory module, yet it creates a persistent information channel across inference requests. As a concrete demonstration, we introduce a new class of temporal backdoors, which we call time bombs. Unlike conventional backdoors that activate on a single trigger input, time bombs activate only after a sequence of interactions satisfies hidden conditions accumulated via implicit memory. We show that such behavior can be induced today through straightforward prompting or fine-tuning. Beyond this case study, we analyze broader implications of implicit memory, including covert inter-agent communication, benchmark contamination, targeted manipulation, and training-data poisoning. Finally, we discuss detection challenges and outline directions for stress-testing and evaluation, with the goal of anticipating and controlling future developments. To promote future research, we release code and data at: https://github.com/microsoft/implicitMemory.",
      "authors": [
        "Ahmed Salem",
        "Andrew Paverd",
        "Sahar Abdelnabi"
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "published": "2026-02-09T12:01:32Z",
      "updated": "2026-02-09T12:01:32Z",
      "pdf_url": "https://arxiv.org/pdf/2602.08563v1",
      "abs_url": "http://arxiv.org/abs/2602.08563v1",
      "summary": "该论文揭示了LLM中一种名为“隐式记忆”的新机制，允许模型跨会话传递信息，并探讨其潜在风险。",
      "key_contributions": [
        "发现了LLM中隐式记忆的存在，无需显式记忆模块即可跨会话传递信息",
        "提出了基于隐式记忆的时间炸弹后门攻击，展示了其潜在危害",
        "分析了隐式记忆在跨Agent通信、基准测试污染等方面的广泛影响"
      ],
      "methodology": "通过prompting或微调的方式，诱导LLM产生具有隐式记忆的行为，并通过实验验证其有效性。",
      "tags": [
        "LLM",
        "隐式记忆",
        "安全",
        "后门攻击",
        "Prompt Engineering"
      ],
      "assigned_category": "memory",
      "relevance_score": 9,
      "relevance_reason": "论文核心关注LLM在没有显式记忆机制下如何传递信息，直接研究了memory相关问题。",
      "analyzed_at": "2026-02-10T07:04:17.314341",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.08519v1",
      "title": "Bridging Academia and Industry: A Comprehensive Benchmark for Attributed Graph Clustering",
      "abstract": "Attributed Graph Clustering (AGC) is a fundamental unsupervised task that integrates structural topology and node attributes to uncover latent patterns in graph-structured data. Despite its significance in industrial applications such as fraud detection and user segmentation, a significant chasm persists between academic research and real-world deployment. Current evaluation protocols suffer from the small-scale, high-homophily citation datasets, non-scalable full-batch training paradigms, and a reliance on supervised metrics that fail to reflect performance in label-scarce environments. To bridge these gaps, we present PyAGC, a comprehensive, production-ready benchmark and library designed to stress-test AGC methods across diverse scales and structural properties. We unify existing methodologies into a modular Encode-Cluster-Optimize framework and, for the first time, provide memory-efficient, mini-batch implementations for a wide array of state-of-the-art AGC algorithms. Our benchmark curates 12 diverse datasets, ranging from 2.7K to 111M nodes, specifically incorporating industrial graphs with complex tabular features and low homophily. Furthermore, we advocate for a holistic evaluation protocol that mandates unsupervised structural metrics and efficiency profiling alongside traditional supervised metrics. Battle-tested in high-stakes industrial workflows at Ant Group, this benchmark offers the community a robust, reproducible, and scalable platform to advance AGC research towards realistic deployment. The code and resources are publicly available via GitHub (https://github.com/Cloudy1225/PyAGC), PyPI (https://pypi.org/project/pyagc), and Documentation (https://pyagc.readthedocs.io).",
      "authors": [
        "Yunhui Liu",
        "Pengyu Qiu",
        "Yu Xing",
        "Yongchao Liu",
        "Peng Du",
        "Chuntao Hong",
        "Jiajun Zheng",
        "Tao Zheng",
        "Tieke He"
      ],
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "published": "2026-02-09T11:07:24Z",
      "updated": "2026-02-09T11:07:24Z",
      "pdf_url": "https://arxiv.org/pdf/2602.08519v1",
      "abs_url": "http://arxiv.org/abs/2602.08519v1",
      "summary": "提出了PyAGC，一个全面的属性图聚类基准，旨在弥合学术研究与工业应用之间的差距。",
      "key_contributions": [
        "构建了大规模、低同质性的属性图聚类基准PyAGC。",
        "统一了属性图聚类方法，提出了模块化的Encode-Cluster-Optimize框架。",
        "提供了内存高效的mini-batch属性图聚类算法实现。"
      ],
      "methodology": "提出了一个模块化的Encode-Cluster-Optimize框架，并提供了多种现有算法的mini-batch实现，在PyAGC基准上进行评估。",
      "tags": [
        "图聚类",
        "属性图",
        "基准测试",
        "机器学习",
        "工业应用"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 7,
      "relevance_reason": "涉及图结构数据的推理和模式发现，与reasoning有一定相关性。",
      "analyzed_at": "2026-02-10T07:04:21.330891",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.08503v1",
      "title": "Learning Self-Correction in Vision-Language Models via Rollout Augmentation",
      "abstract": "Self-correction is essential for solving complex reasoning problems in vision-language models (VLMs). However, existing reinforcement learning (RL) methods struggle to learn it, as effective self-correction behaviors emerge only rarely, making learning signals extremely sparse. To address this challenge, we propose correction-specific rollouts (Octopus), an RL rollout augmentation framework that synthesizes dense self-correction examples by recombining existing rollouts. This augmentation simultaneously improves sample efficiency due to rollout reuse and stabilizes RL optimization through balanced supervision. Furthermore, we introduce a response-masking strategy that decouples self-correction from direct reasoning, avoiding signal conflicts and enabling both behaviors to be learned effectively. Building on this, we introduce Octopus-8B, a reasoning VLM with controllable self-correction capability. Across 7 benchmarks, it achieves SoTA performance among open-source VLMs, outperforming the best RLVR baseline by 1.0 score while requiring only $0.72\\times$ training time per step.",
      "authors": [
        "Yi Ding",
        "Ziliang Qiu",
        "Bolian Li",
        "Ruqi Zhang"
      ],
      "categories": [
        "cs.CV",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "published": "2026-02-09T10:55:13Z",
      "updated": "2026-02-09T10:55:13Z",
      "pdf_url": "https://arxiv.org/pdf/2602.08503v1",
      "abs_url": "http://arxiv.org/abs/2602.08503v1",
      "summary": "该论文提出了一种通过rollout增强学习视觉语言模型自校正能力的方法，并在多个基准测试中取得了领先成果。",
      "key_contributions": [
        "提出了 correction-specific rollouts (Octopus) 框架，增强自校正示例",
        "引入 response-masking 策略，解耦自校正和直接推理",
        "构建了具有可控自校正能力的 reasoning VLM (Octopus-8B)"
      ],
      "methodology": "通过重组现有rollouts合成密集自校正示例，并采用response-masking策略分离自校正和推理，进行强化学习。",
      "tags": [
        "Vision-Language Model",
        "Reinforcement Learning",
        "Self-Correction",
        "Rollout Augmentation"
      ],
      "assigned_category": "multimodal",
      "relevance_score": 9,
      "relevance_reason": "论文核心关注视觉语言模型，并改进其推理能力，高度相关。",
      "analyzed_at": "2026-02-10T07:04:24.345789",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.08456v1",
      "title": "Decentralized Spatial Reuse Optimization in Wi-Fi: An Internal Regret Minimization Approach",
      "abstract": "Spatial Reuse (SR) is a cost-effective technique for improving spectral efficiency in dense IEEE 802.11 deployments by enabling simultaneous transmissions. However, the decentralized optimization of SR parameters -- transmission power and Carrier Sensing Threshold (CST) -- across different Basic Service Sets (BSSs) is challenging due to the lack of global state information. In addition, the concurrent operation of multiple agents creates a highly non-stationary environment, often resulting in suboptimal global configurations (e.g., using the maximum possible transmission power by default). To overcome these limitations, this paper introduces a decentralized learning algorithm based on regret-matching, grounded in internal regret minimization. Unlike standard decentralized ``selfish'' approaches that often converge to inefficient Nash Equilibria (NE), internal regret minimization guides competing agents toward Correlated Equilibria (CE), effectively mimicking coordination without explicit communication. Through simulation results, we showcase the superiority of our proposed approach and its ability to reach near-optimal global performance. These results confirm the not-yet-unleashed potential of scalable decentralized solutions and question the need for the heavy signaling overheads and architectural complexity associated with emerging centralized solutions like Multi-Access Point Coordination (MAPC).",
      "authors": [
        "Francesc Wilhelmi",
        "Boris Bellalta",
        "Miguel Casasnovas",
        "Aleksandra Kijanka",
        "Miguel Calvo-Fullana"
      ],
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI",
      "published": "2026-02-09T10:10:18Z",
      "updated": "2026-02-09T10:10:18Z",
      "pdf_url": "https://arxiv.org/pdf/2602.08456v1",
      "abs_url": "http://arxiv.org/abs/2602.08456v1",
      "summary": "提出基于内部后悔最小化的分布式算法，优化Wi-Fi网络中的空间复用，提升频谱效率。",
      "key_contributions": [
        "提出了一种基于内部后悔最小化的分布式学习算法。",
        "证明了该算法能够有效解决Wi-Fi网络中空间复用的优化问题。",
        "实验结果表明该算法优于传统的“自私”算法，且接近最优性能。"
      ],
      "methodology": "采用基于后悔匹配的分布式学习算法，模拟协调机制，优化传输功率和CST参数。",
      "tags": [
        "Wi-Fi",
        "空间复用",
        "分布式优化",
        "后悔匹配",
        "IEEE 802.11"
      ],
      "assigned_category": "agent",
      "relevance_score": 6,
      "relevance_reason": "解决分布式环境下多个agent的资源分配问题，与Agent领域有一定关联。",
      "analyzed_at": "2026-02-10T07:04:26.833615",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.08448v1",
      "title": "Vista: Scene-Aware Optimization for Streaming Video Question Answering under Post-Hoc Queries",
      "abstract": "Streaming video question answering (Streaming Video QA) poses distinct challenges for multimodal large language models (MLLMs), as video frames arrive sequentially and user queries can be issued at arbitrary time points. Existing solutions relying on fixed-size memory or naive compression often suffer from context loss or memory overflow, limiting their effectiveness in long-form, real-time scenarios. We present Vista, a novel framework for scene-aware streaming video QA that enables efficient and scalable reasoning over continuous video streams. The innovation of Vista can be summarized in three aspects: (1) scene-aware segmentation, where Vista dynamically clusters incoming frames into temporally and visually coherent scene units; (2) scene-aware compression, where each scene is compressed into a compact token representation and stored in GPU memory for efficient index-based retrieval, while full-resolution frames are offloaded to CPU memory; and (3) scene-aware recall, where relevant scenes are selectively recalled and reintegrated into the model input upon receiving a query, enabling both efficiency and completeness. Vista is model-agnostic and integrates seamlessly with a variety of vision-language backbones, enabling long-context reasoning without compromising latency or memory efficiency. Extensive experiments on StreamingBench demonstrate that Vista achieves state-of-the-art performance, establishing a strong baseline for real-world streaming video understanding.",
      "authors": [
        "Haocheng Lu",
        "Nan Zhang",
        "Wei Tao",
        "Xiaoyang Qu",
        "Guokuan Li",
        "Jiguang Wan",
        "Jianzong Wang"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2026-02-09T10:00:22Z",
      "updated": "2026-02-09T10:00:22Z",
      "pdf_url": "https://arxiv.org/pdf/2602.08448v1",
      "abs_url": "http://arxiv.org/abs/2602.08448v1",
      "summary": "Vista针对流式视频QA，提出场景感知的优化方案，实现高效且可扩展的推理。",
      "key_contributions": [
        "场景感知分割",
        "场景感知压缩",
        "场景感知召回"
      ],
      "methodology": "动态分割视频帧为场景单元，压缩场景为token表示，并根据查询选择性召回相关场景。",
      "tags": [
        "流式视频QA",
        "多模态学习",
        "场景感知"
      ],
      "assigned_category": "multimodal",
      "relevance_score": 9,
      "relevance_reason": "核心关注流式视频理解中的多模态学习问题，与LLM应用密切相关。",
      "analyzed_at": "2026-02-10T07:04:28.908144",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.08439v1",
      "title": "Demo-ICL: In-Context Learning for Procedural Video Knowledge Acquisition",
      "abstract": "Despite the growing video understanding capabilities of recent Multimodal Large Language Models (MLLMs), existing video benchmarks primarily assess understanding based on models' static, internal knowledge, rather than their ability to learn and adapt from dynamic, novel contexts from few examples. To bridge this gap, we present Demo-driven Video In-Context Learning, a novel task focused on learning from in-context demonstrations to answer questions about the target videos. Alongside this, we propose Demo-ICL-Bench, a challenging benchmark designed to evaluate demo-driven video in-context learning capabilities. Demo-ICL-Bench is constructed from 1200 instructional YouTube videos with associated questions, from which two types of demonstrations are derived: (i) summarizing video subtitles for text demonstration; and (ii) corresponding instructional videos as video demonstrations. To effectively tackle this new challenge, we develop Demo-ICL, an MLLM with a two-stage training strategy: video-supervised fine-tuning and information-assisted direct preference optimization, jointly enhancing the model's ability to learn from in-context examples. Extensive experiments with state-of-the-art MLLMs confirm the difficulty of Demo-ICL-Bench, demonstrate the effectiveness of Demo-ICL, and thereby unveil future research directions.",
      "authors": [
        "Yuhao Dong",
        "Shulin Tian",
        "Shuai Liu",
        "Shuangrui Ding",
        "Yuhang Zang",
        "Xiaoyi Dong",
        "Yuhang Cao",
        "Jiaqi Wang",
        "Ziwei Liu"
      ],
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2026-02-09T09:51:29Z",
      "updated": "2026-02-09T09:51:29Z",
      "pdf_url": "https://arxiv.org/pdf/2602.08439v1",
      "abs_url": "http://arxiv.org/abs/2602.08439v1",
      "summary": "提出Demo-ICL任务和基准，用于评估MLLM从视频演示中学习的能力，并提出Demo-ICL模型。",
      "key_contributions": [
        "定义了Demo-driven Video In-Context Learning任务",
        "构建了Demo-ICL-Bench基准数据集",
        "提出了Demo-ICL模型，采用两阶段训练策略"
      ],
      "methodology": "提出了Demo-ICL模型，通过视频监督微调和信息辅助直接偏好优化，增强MLLM从上下文示例中学习的能力。",
      "tags": [
        "Video Understanding",
        "In-Context Learning",
        "Multimodal Learning"
      ],
      "assigned_category": "multimodal",
      "relevance_score": 9,
      "relevance_reason": "论文核心在于多模态视频理解和上下文学习，直接相关。",
      "analyzed_at": "2026-02-10T07:04:30.853712",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.08437v1",
      "title": "Large Language Models and Impossible Language Acquisition: \"False Promise\" or an Overturn of our Current Perspective towards AI",
      "abstract": "In Chomsky's provocative critique \"The False Promise of CHATGPT,\" Large Language Models (LLMs) are characterized as mere pattern predictors that do not acquire languages via intrinsic causal and self-correction structures like humans, therefore are not able to distinguish impossible languages. It stands as a representative in a fundamental challenge to the intellectual foundations of AI, for it integrally synthesizes major issues in methodologies within LLMs and possesses an iconic a priori rationalist perspective. We examine this famous critic from both the perspective in pre-existing literature of linguistics and psychology as well as a research based on an experiment inquiring the capacity of learning both possible and impossible languages among LLMs. We constructed a set of syntactically impossible languages by applying certain transformations to English. These include reversing whole sentences, and adding negation based on word-count parity. Two rounds of controlled experiments were each conducted on GPT-2 small models and long short-term memory (LSTM) models. Statistical analysis (Welch's t-test) shows GPT2 small models underperform in learning all of the impossible languages compared to their performance on the possible language (p<.001). On the other hand, LSTM models' performance tallies with Chomsky's argument, suggesting the irreplaceable role of the evolution of transformer architecture. Based on theoretical analysis and empirical findings, we propose a new vision within Chomsky's theory towards LLMs, and a shift of theoretical paradigm outside Chomsky, from his \"rationalist-romantics\" paradigm to functionalism and empiricism in LLMs research.",
      "authors": [
        "Ziyan wang",
        "Longlong Ma"
      ],
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "published": "2026-02-09T09:50:12Z",
      "updated": "2026-02-09T09:50:12Z",
      "pdf_url": "https://arxiv.org/pdf/2602.08437v1",
      "abs_url": "http://arxiv.org/abs/2602.08437v1",
      "summary": "论文通过实验和理论分析，探讨LLM在学习不可能语言方面的能力，并对Chomsky的观点提出新的见解。",
      "key_contributions": [
        "通过实验验证GPT-2和小模型在学习不可能语言上的表现差异，揭示Transformer架构的重要性。",
        "提出在Chomsky框架内对LLM的新视角，以及从理性主义到功能主义/经验主义的理论范式转变。",
        "构建了一系列基于英语的句法上不可能的语言，为研究LLM的语言学习能力提供了实验基础。"
      ],
      "methodology": "构建不可能语言数据集，分别在GPT-2和LSTM模型上进行实验，采用Welch's t-test进行统计分析，并结合理论分析。",
      "tags": [
        "LLM",
        "Impossible Language",
        "Chomsky",
        "GPT-2",
        "LSTM"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 8,
      "relevance_reason": "论文探讨了LLM的语言理解能力，涉及LLM推理能力的关键方面。",
      "analyzed_at": "2026-02-10T07:04:33.127097",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.08401v1",
      "title": "On Protecting Agentic Systems' Intellectual Property via Watermarking",
      "abstract": "The evolution of Large Language Models (LLMs) into agentic systems that perform autonomous reasoning and tool use has created significant intellectual property (IP) value. We demonstrate that these systems are highly vulnerable to imitation attacks, where adversaries steal proprietary capabilities by training imitation models on victim outputs. Crucially, existing LLM watermarking techniques fail in this domain because real-world agentic systems often operate as grey boxes, concealing the internal reasoning traces required for verification. This paper presents AGENTWM, the first watermarking framework designed specifically for agentic models. AGENTWM exploits the semantic equivalence of action sequences, injecting watermarks by subtly biasing the distribution of functionally identical tool execution paths. This mechanism allows AGENTWM to embed verifiable signals directly into the visible action trajectory while remaining indistinguishable to users. We develop an automated pipeline to generate robust watermark schemes and a rigorous statistical hypothesis testing procedure for verification. Extensive evaluations across three complex domains demonstrate that AGENTWM achieves high detection accuracy with negligible impact on agent performance. Our results confirm that AGENTWM effectively protects agentic IP against adaptive adversaries, who cannot remove the watermarks without severely degrading the stolen model's utility.",
      "authors": [
        "Liwen Wang",
        "Zongjie Li",
        "Yuchong Xie",
        "Shuai Wang",
        "Dongdong She",
        "Wei Wang",
        "Juergen Rahmel"
      ],
      "categories": [
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.AI",
      "published": "2026-02-09T09:02:15Z",
      "updated": "2026-02-09T09:02:15Z",
      "pdf_url": "https://arxiv.org/pdf/2602.08401v1",
      "abs_url": "http://arxiv.org/abs/2602.08401v1",
      "summary": "提出AGENTWM框架，通过在Agent动作序列中嵌入水印，保护Agentic系统知识产权。",
      "key_contributions": [
        "设计了首个针对Agentic模型的水印框架AGENTWM",
        "利用动作序列的语义等价性，通过微调工具执行路径注入水印",
        "开发了自动生成鲁棒水印方案的pipeline以及验证的统计假设检验流程"
      ],
      "methodology": "通过在功能等价的动作序列中选择特定路径来嵌入水印，并设计验证流程检测水印。",
      "tags": [
        "watermarking",
        "intellectual property",
        "agentic systems"
      ],
      "assigned_category": "agent",
      "relevance_score": 9,
      "relevance_reason": "论文核心关注AI Agent的IP保护，并提出了专门的水印方案。",
      "analyzed_at": "2026-02-10T07:04:35.397971",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.08392v1",
      "title": "BiManiBench: A Hierarchical Benchmark for Evaluating Bimanual Coordination of Multimodal Large Language Models",
      "abstract": "Multimodal Large Language Models (MLLMs) have significantly advanced embodied AI, and using them to benchmark robotic intelligence has become a pivotal trend. However, existing frameworks remain predominantly confined to single-arm manipulation, failing to capture the spatio-temporal coordination required for bimanual tasks like lifting a heavy pot. To address this, we introduce BiManiBench, a hierarchical benchmark evaluating MLLMs across three tiers: fundamental spatial reasoning, high-level action planning, and low-level end-effector control. Our framework isolates unique bimanual challenges, such as arm reachability and kinematic constraints, thereby distinguishing perceptual hallucinations from planning failures. Analysis of over 30 state-of-the-art models reveals that despite high-level reasoning proficiency, MLLMs struggle with dual-arm spatial grounding and control, frequently resulting in mutual interference and sequencing errors. These findings suggest the current paradigm lacks a deep understanding of mutual kinematic constraints, highlighting the need for future research to focus on inter-arm collision-avoidance and fine-grained temporal sequencing.",
      "authors": [
        "Xin Wu",
        "Zhixuan Liang",
        "Yue Ma",
        "Mengkang Hu",
        "Zhiyuan Qin",
        "Xiu Li"
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "published": "2026-02-09T08:47:14Z",
      "updated": "2026-02-09T08:47:14Z",
      "pdf_url": "https://arxiv.org/pdf/2602.08392v1",
      "abs_url": "http://arxiv.org/abs/2602.08392v1",
      "summary": "提出BiManiBench基准测试MLLM在双臂操作中的空间推理、规划和控制能力。",
      "key_contributions": [
        "提出了BiManiBench双臂操作基准测试",
        "评估了MLLM在双臂任务中的性能",
        "揭示了MLLM在双臂空间推理和控制方面的不足"
      ],
      "methodology": "构建分层基准，包含空间推理、动作规划和末端执行器控制三个层次，评估MLLM在双臂任务中的表现。",
      "tags": [
        "MLLM",
        "Bimanual Manipulation",
        "Benchmark",
        "Robotics"
      ],
      "assigned_category": "multimodal",
      "relevance_score": 9,
      "relevance_reason": "论文核心关注MLLM在多模态双臂机器人操作中的应用和评估。",
      "analyzed_at": "2026-02-10T07:04:37.527026",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.08389v1",
      "title": "Altruism and Fair Objective in Mixed-Motive Markov games",
      "abstract": "Cooperation is fundamental for society's viability, as it enables the emergence of structure within heterogeneous groups that seek collective well-being. However, individuals are inclined to defect in order to benefit from the group's cooperation without contributing the associated costs, thus leading to unfair situations. In game theory, social dilemmas entail this dichotomy between individual interest and collective outcome. The most dominant approach to multi-agent cooperation is the utilitarian welfare which can produce efficient highly inequitable outcomes. This paper proposes a novel framework to foster fairer cooperation by replacing the standard utilitarian objective with Proportional Fairness. We introduce a fair altruistic utility for each agent, defined on the individual log-payoff space and derive the analytical conditions required to ensure cooperation in classic social dilemmas. We then extend this framework to sequential settings by defining a Fair Markov Game and deriving novel fair Actor-Critic algorithms to learn fair policies. Finally, we evaluate our method in various social dilemma environments.",
      "authors": [
        "Yao-hua Franck Xu",
        "Tayeb Lemlouma",
        "Arnaud Braud",
        "Jean-Marie Bonnin"
      ],
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.GT",
        "cs.LG"
      ],
      "primary_category": "cs.MA",
      "published": "2026-02-09T08:40:52Z",
      "updated": "2026-02-09T08:40:52Z",
      "pdf_url": "https://arxiv.org/pdf/2602.08389v1",
      "abs_url": "http://arxiv.org/abs/2602.08389v1",
      "summary": "提出一种基于比例公平的新框架，旨在马尔可夫博弈中促进更公平的合作。",
      "key_contributions": [
        "提出了基于比例公平的智能体公平利他效用",
        "推导了经典社会困境中确保合作的分析条件",
        "定义了公平马尔可夫博弈，并推导了新的公平Actor-Critic算法"
      ],
      "methodology": "将标准的功利主义目标替换为比例公平，并在个体log-payoff空间上定义公平利他效用，设计Actor-Critic算法。",
      "tags": [
        "多智能体",
        "合作博弈",
        "公平性",
        "强化学习"
      ],
      "assigned_category": "agent",
      "relevance_score": 8,
      "relevance_reason": "探讨多智能体合作中的公平性，与智能体领域紧密相关。",
      "analyzed_at": "2026-02-10T07:04:39.380023",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.08382v1",
      "title": "Dynamic Long Context Reasoning over Compressed Memory via End-to-End Reinforcement Learning",
      "abstract": "Large Language Models (LLMs) face significant challenges in long-context processing, including quadratic computational costs, information forgetting, and the context fragmentation inherent in retrieval-augmented generation (RAG). We propose a cognitively inspired framework for efficient long-context inference based on chunk-wise compression and selective memory recall, rather than processing all raw tokens. The framework segments long inputs into chunks and encodes each chunk into compressed memory representations using a learned compressor. A gating module dynamically selects relevant memory blocks, which are then iteratively processed by a reasoning module with an evolving working memory to solve downstream tasks. The compressor and reasoner are jointly optimized via end-to-end reinforcement learning, while the gating module is trained separately as a classifier. Experimental results show that the proposed method achieves competitive accuracy on multi-hop reasoning benchmarks such as RULER-HQA, extrapolates context length from 7K to 1.75M tokens, and offers a favorable accuracy-efficiency trade-off compared to strong long-context baselines. In particular, it achieves up to a 2 times reduction in peak GPU memory usage and a 6 times inference speedup over MemAgent.",
      "authors": [
        "Zhuoen Chen",
        "Dongfang Li",
        "Meishan Zhang",
        "Baotian Hu",
        "Min Zhang"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "published": "2026-02-09T08:33:11Z",
      "updated": "2026-02-09T08:33:11Z",
      "pdf_url": "https://arxiv.org/pdf/2602.08382v1",
      "abs_url": "http://arxiv.org/abs/2602.08382v1",
      "summary": "提出一种基于压缩记忆和强化学习的LLM长文本推理框架，提升效率和扩展上下文长度。",
      "key_contributions": [
        "提出了一种基于chunk-wise压缩和选择性记忆召回的长文本推理框架",
        "使用强化学习联合优化压缩器和推理器",
        "实验证明该方法在多跳推理任务上具有竞争力，并能扩展上下文长度"
      ],
      "methodology": "将长输入分割成块并压缩成记忆，使用门控模块选择相关记忆块，并通过强化学习优化压缩器和推理器。",
      "tags": [
        "长文本推理",
        "压缩记忆",
        "强化学习",
        "多跳推理"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 9,
      "relevance_reason": "论文核心关注LLM长文本推理的效率和扩展，直接解决相关问题。",
      "analyzed_at": "2026-02-10T07:04:41.180891",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.08355v1",
      "title": "E-VAds: An E-commerce Short Videos Understanding Benchmark for MLLMs",
      "abstract": "E-commerce short videos represent a high-revenue segment of the online video industry characterized by a goal-driven format and dense multi-modal signals. Current models often struggle with these videos because existing benchmarks focus primarily on general-purpose tasks and neglect the reasoning of commercial intent. In this work, we first propose a \\textbf{multi-modal information density assessment framework} to quantify the complexity of this domain. Our evaluation reveals that e-commerce content exhibits substantially higher density across visual, audio, and textual modalities compared to mainstream datasets, establishing a more challenging frontier for video understanding. To address this gap, we introduce \\textbf{E-commerce Video Ads Benchmark (E-VAds)}, which is the first benchmark specifically designed for e-commerce short video understanding. We curated 3,961 high-quality videos from Taobao covering a wide range of product categories and used a multi-agent system to generate 19,785 open-ended Q&A pairs. These questions are organized into two primary dimensions, namely Perception and Cognition and Reasoning, which consist of five distinct tasks. Finally, we develop \\textbf{E-VAds-R1}, an RL-based reasoning model featuring a multi-grained reward design called \\textbf{MG-GRPO}. This strategy provides smooth guidance for early exploration while creating a non-linear incentive for expert-level precision. Experimental results demonstrate that E-VAds-R1 achieves a 109.2% performance gain in commercial intent reasoning with only a few hundred training samples.",
      "authors": [
        "Xianjie Liu",
        "Yiman Hu",
        "Liang Wu",
        "Ping Hu",
        "Yixiong Zou",
        "Jian Xu",
        "Bo Zheng"
      ],
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2026-02-09T07:43:38Z",
      "updated": "2026-02-09T07:43:38Z",
      "pdf_url": "https://arxiv.org/pdf/2602.08355v1",
      "abs_url": "http://arxiv.org/abs/2602.08355v1",
      "summary": "提出了电商短视频理解基准E-VAds，并设计了基于RL的推理模型E-VAds-R1。",
      "key_contributions": [
        "提出了多模态信息密度评估框架，量化了电商视频的复杂性",
        "构建了电商短视频理解基准E-VAds，包含高质量视频和开放式问答对",
        "开发了基于RL的推理模型E-VAds-R1，提升了电商意图推理性能"
      ],
      "methodology": "使用多智能体系统生成问答对，设计RL模型进行推理，并采用多粒度奖励机制进行优化。",
      "tags": [
        "e-commerce",
        "short video",
        "MLLM",
        "reasoning",
        "benchmark"
      ],
      "assigned_category": "multimodal",
      "relevance_score": 9,
      "relevance_reason": "论文核心在于构建多模态的电商视频理解基准，属于多模态学习的关键问题。",
      "analyzed_at": "2026-02-10T07:04:44.765343",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.08346v1",
      "title": "What, Whether and How? Unveiling Process Reward Models for Thinking with Images Reasoning",
      "abstract": "The rapid advancement of Large Vision Language Models (LVLMs) has demonstrated excellent abilities in various visual tasks. Building upon these developments, the thinking with images paradigm has emerged, enabling models to dynamically edit and re-encode visual information at each reasoning step, mirroring human visual processing. However, this paradigm introduces significant challenges as diverse errors may occur during reasoning processes. This necessitates Process Reward Models (PRMs) for distinguishing positive and negative reasoning steps, yet existing benchmarks for PRMs are predominantly text-centric and lack comprehensive assessment under this paradigm. To address these gaps, this work introduces the first comprehensive benchmark specifically designed for evaluating PRMs under the thinking with images paradigm. Our main contributions are: (1) Through extensive analysis of reasoning trajectories and guided search experiments with PRMs, we define 7 fine-grained error types and demonstrate both the necessity for specialized PRMs and the potential for improvement. (2) We construct a comprehensive benchmark comprising 1,206 manually annotated thinking with images reasoning trajectories spanning 4 categories and 16 subcategories for fine-grained evaluation of PRMs. (3) Our experimental analysis reveals that current LVLMs fall short as effective PRMs, exhibiting limited capabilities in visual reasoning process evaluation with significant performance disparities across error types, positive evaluation bias, and sensitivity to reasoning step positions. These findings demonstrate the effectiveness of our benchmark and establish crucial foundations for advancing PRMs in LVLMs.",
      "authors": [
        "Yujin Zhou",
        "Pengcheng Wen",
        "Jiale Chen",
        "Boqin Yin",
        "Han Zhu",
        "Jiaming Ji",
        "Juntao Dai",
        "Chi-Min Chan",
        "Sirui Han"
      ],
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2026-02-09T07:31:14Z",
      "updated": "2026-02-09T07:31:14Z",
      "pdf_url": "https://arxiv.org/pdf/2602.08346v1",
      "abs_url": "http://arxiv.org/abs/2602.08346v1",
      "summary": "论文提出了一个用于评估LVLMs视觉推理过程奖励模型的综合基准测试。",
      "key_contributions": [
        "定义了7种细粒度的错误类型，揭示了专用PRM的必要性。",
        "构建了一个包含1206条人工标注推理轨迹的综合基准。",
        "分析表明现有LVLMs作为有效PRM存在不足。"
      ],
      "methodology": "通过分析推理轨迹和PRMs引导的搜索实验，构建基准测试并评估现有LVLMs的性能。",
      "tags": [
        "视觉推理",
        "奖励模型",
        "基准测试",
        "LVLMs",
        "图像理解"
      ],
      "assigned_category": "multimodal",
      "relevance_score": 9,
      "relevance_reason": "论文核心关注视觉语言模型和图像推理，属于多模态学习的关键研究方向。",
      "analyzed_at": "2026-02-10T07:04:46.918477",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.08339v1",
      "title": "CoTZero: Annotation-Free Human-Like Vision Reasoning via Hierarchical Synthetic CoT",
      "abstract": "Recent advances in vision-language models (VLMs) have markedly improved image-text alignment, yet they still fall short of human-like visual reasoning. A key limitation is that many VLMs rely on surface correlations rather than building logically coherent structured representations, which often leads to missed higher-level semantic structure and non-causal relational understanding, hindering compositional and verifiable reasoning. To address these limitations by introducing human models into the reasoning process, we propose CoTZero, an annotation-free paradigm with two components: (i) a dual-stage data synthesis approach and (ii) a cognition-aligned training method. In the first component, we draw inspiration from neurocognitive accounts of compositional productivity and global-to-local analysis. In the bottom-up stage, CoTZero extracts atomic visual primitives and incrementally composes them into diverse, structured question-reasoning forms. In the top-down stage, it enforces hierarchical reasoning by using coarse global structure to guide the interpretation of local details and causal relations. In the cognition-aligned training component, built on the synthesized CoT data, we introduce Cognitively Coherent Verifiable Rewards (CCVR) in Reinforcement Fine-Tuning (RFT) to further strengthen VLMs' hierarchical reasoning and generalization, providing stepwise feedback on reasoning coherence and factual correctness. Experiments show that CoTZero achieves an F1 score of 83.33 percent on our multi-level semantic inconsistency benchmark with lexical-perturbation negatives, across both in-domain and out-of-domain settings. Ablations confirm that each component contributes to more interpretable and human-aligned visual reasoning.",
      "authors": [
        "Chengyi Du",
        "Yazhe Niu",
        "Dazhong Shen",
        "Luxin Xu"
      ],
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "published": "2026-02-09T07:26:40Z",
      "updated": "2026-02-09T07:26:40Z",
      "pdf_url": "https://arxiv.org/pdf/2602.08339v1",
      "abs_url": "http://arxiv.org/abs/2602.08339v1",
      "summary": "CoTZero通过无标注的分层合成CoT数据，提升视觉语言模型的人类水平视觉推理能力。",
      "key_contributions": [
        "提出了无标注的CoTZero范式",
        "设计了双阶段数据合成方法，模拟人类认知过程",
        "引入了认知对齐的可验证奖励，强化模型的层次推理能力"
      ],
      "methodology": "CoTZero通过合成数据和强化学习，提升VLMs在推理一致性和事实正确性方面的表现，并采用认知对齐奖励进行训练。",
      "tags": [
        "视觉语言模型",
        "视觉推理",
        "链式思考",
        "强化学习"
      ],
      "assigned_category": "multimodal",
      "relevance_score": 9,
      "relevance_reason": "论文核心关注VLM的视觉推理能力，并提出CoT框架，属于多模态学习的关键问题。",
      "analyzed_at": "2026-02-10T07:04:49.565786",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.08336v1",
      "title": "UReason: Benchmarking the Reasoning Paradox in Unified Multimodal Models",
      "abstract": "To elicit capabilities for addressing complex and implicit visual requirements, recent unified multimodal models increasingly adopt chain-of-thought reasoning to guide image generation. However, the actual effect of reasoning on visual synthesis remains unclear. We present UReason, a diagnostic benchmark for reasoning-driven image generation that evaluates whether reasoning can be faithfully executed in pixels. UReason contains 2,000 instances across five task families: Code, Arithmetic, Spatial, Attribute, and Text reasoning. To isolate the role of reasoning traces, we introduce an evaluation framework comparing direct generation, reasoning-guided generation, and de-contextualized generation which conditions only on the refined prompt. Across eight open-source unified models, we observe a consistent Reasoning Paradox: Reasoning traces generally improve performance over direct generation, yet retaining intermediate thoughts as conditioning context often hinders visual synthesis, and conditioning only on the refined prompt yields substantial gains. Our analysis suggests that the bottleneck lies in contextual interference rather than insufficient reasoning capacity. UReason provides a principled testbed for studying reasoning in unified models and motivates future methods that effectively integrate reasoning for visual generation while mitigating interference.",
      "authors": [
        "Cheng Yang",
        "Chufan Shi",
        "Bo Shui",
        "Yaokang Wu",
        "Muzi Tao",
        "Huijuan Wang",
        "Ivan Yee Lee",
        "Yong Liu",
        "Xuezhe Ma",
        "Taylor Berg-Kirkpatrick"
      ],
      "categories": [
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "published": "2026-02-09T07:17:57Z",
      "updated": "2026-02-09T07:17:57Z",
      "pdf_url": "https://arxiv.org/pdf/2602.08336v1",
      "abs_url": "http://arxiv.org/abs/2602.08336v1",
      "summary": "论文提出了UReason基准测试，揭示了统一多模态模型中推理在视觉合成中的悖论现象。",
      "key_contributions": [
        "提出了UReason基准测试，包含2000个实例，覆盖五种推理任务。",
        "设计了一种评估框架，比较直接生成、推理引导生成和去语境化生成。",
        "揭示了推理悖论现象：推理痕迹改善性能，但保留中间思想反而阻碍视觉合成。"
      ],
      "methodology": "通过UReason基准，对比不同生成方式在多模态模型上的表现，分析推理痕迹的作用。",
      "tags": [
        "multimodal",
        "reasoning",
        "image generation",
        "benchmark"
      ],
      "assigned_category": "multimodal",
      "relevance_score": 9,
      "relevance_reason": "核心关注多模态模型中的推理能力，与multimodal领域高度相关。",
      "analyzed_at": "2026-02-10T07:04:51.504144",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    },
    {
      "arxiv_id": "2602.08329v1",
      "title": "Near-Oracle KV Selection via Pre-hoc Sparsity for Long-Context Inference",
      "abstract": "A core bottleneck in large language model (LLM) inference is the cost of attending over the ever-growing key-value (KV) cache. Although near-oracle top-k KV selection can preserve the quality of dense attention while sharply reducing computation and bandwidth, existing sparse methods generally rely on posterior heuristics, i.e., selectors conditioned on observed attention or proxy scores. Such conditioning introduces posterior bias: it tends to distort true token importance and miss salient tokens, thereby impairing long-range reasoning. To tackle this problem, we propose Pre-hoc Sparsity (PrHS), which selects KV entries before attention scoring and provides explicit accuracy control. Let the attention mass of discarded entries be delta (the dropped mass). Through a marginal-to-mutual-information analysis, we derive an upper bound on the mutual-information loss that depends only on the dropped mass. This relation explains failure modes of posterior heuristics and enables verifiable guarantees by controlling the dropped mass in advance. Within PrHS, we instantiate three orthogonal pre-hoc selectors along the axes of time, depth, and layer. Extensive experiments on LLaMA and Mistral families validate PrHS. Across GSM8K and CoQA, PrHS reduces retrieval overhead by over 90%, achieving 3x higher retrieval sparsity than HShare at matched or better accuracy. It incurs under 1% average degradation on LongBench, lowers attention FLOPs by about 15% versus prior sparse baselines, and yields a 9.9x speedup in attention-operator latency and 2.8x higher throughput on NVIDIA A100-80GB GPUs than the dense baseline.",
      "authors": [
        "Yifei Gao",
        "Lei Wang",
        "Rong-Cheng Tu",
        "Qixin Zhang",
        "Jun Cheng",
        "Dacheng Tao"
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IT"
      ],
      "primary_category": "cs.LG",
      "published": "2026-02-09T07:05:23Z",
      "updated": "2026-02-09T07:05:23Z",
      "pdf_url": "https://arxiv.org/pdf/2602.08329v1",
      "abs_url": "http://arxiv.org/abs/2602.08329v1",
      "summary": "提出Pre-hoc Sparsity方法，解决长文本推理中KV选择的后验偏差问题，提升推理效率和准确性。",
      "key_contributions": [
        "提出了Pre-hoc Sparsity (PrHS) 方法",
        "推导了互信息损失的上界，实现了显式的精度控制",
        "设计了三种正交的预先选择器：时间、深度和层"
      ],
      "methodology": "通过互信息分析，推导出丢弃token的注意力质量损失上界，并在token重要性评分前进行KV选择。",
      "tags": [
        "长文本推理",
        "稀疏注意力",
        "KV缓存",
        "语言模型优化"
      ],
      "assigned_category": "reasoning",
      "relevance_score": 9,
      "relevance_reason": "直接解决长文本推理中的关键瓶颈问题，显著提升效率并保证推理质量。",
      "analyzed_at": "2026-02-10T07:04:53.605723",
      "llm_provider": "gemini",
      "llm_model": "gemini-2.0-flash"
    }
  ],
  "fetch_time": "2026-02-10T07:04:53.605938"
}