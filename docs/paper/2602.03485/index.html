<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Self-Verification Dilemma: Experience-Driven Suppression of Overused Checking in LLM Reasoning - Paper Tracker</title>
    <meta name="description" content="Daily arXiv paper tracking with AI-powered analysis">
    <link rel="stylesheet" href="/Paper_Tracking/static/css/style.css">
    <script>window.SITE_BASE_URL = "/Paper_Tracking";</script>
    
</head>
<body>
    <nav class="navbar">
        <div class="nav-container">
            <a href="/Paper_Tracking/" class="nav-brand">
                <svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                    <path d="M4 19.5A2.5 2.5 0 0 1 6.5 17H20"></path>
                    <path d="M6.5 2H20v20H6.5A2.5 2.5 0 0 1 4 19.5v-15A2.5 2.5 0 0 1 6.5 2z"></path>
                </svg>
                <span>Paper Tracker</span>
            </a>
            <div class="nav-links">
                
                <a href="/Paper_Tracking/category/memory/" class="nav-link ">
                    LLM Memory &amp; RAG
                </a>
                
                <a href="/Paper_Tracking/category/agent/" class="nav-link ">
                    AI Agents
                </a>
                
                <a href="/Paper_Tracking/category/reasoning/" class="nav-link ">
                    LLM Reasoning
                </a>
                
            </div>
            <div class="nav-search">
                <input type="text" id="search-input" placeholder="搜索论文..." autocomplete="off">
                <div id="search-results" class="search-dropdown"></div>
            </div>
        </div>
    </nav>

    <main class="main-content">
        
<div class="container">
    <article class="paper-detail">
        <nav class="breadcrumb">
            <a href="/Paper_Tracking/">首页</a>
            <span>/</span>
            <a href="/Paper_Tracking/category/reasoning/">LLM Reasoning</a>
            <span>/</span>
            <span>2602.03485v1</span>
        </nav>

        <header class="paper-detail-header">
            <div class="paper-detail-meta">
                <span class="paper-category-badge">LLM Reasoning</span>
                <span class="paper-score score-high">
                    相关度: 9/10
                </span>
            </div>
            <h1 class="paper-detail-title">Self-Verification Dilemma: Experience-Driven Suppression of Overused Checking in LLM Reasoning</h1>
            <div class="paper-detail-authors">
                
                <span class="author">Quanyu Long</span>, 
                
                <span class="author">Kai Jie Jiang</span>, 
                
                <span class="author">Jianda Chen</span>, 
                
                <span class="author">Xu Guo</span>, 
                
                <span class="author">Leilei Gan</span>, 
                
                <span class="author">Wenya Wang</span>
                
            </div>
            <div class="paper-detail-info">
                <span>arXiv: 2602.03485v1</span>
                <span>发布: 2026-02-03</span>
                <span>更新: 2026-02-03</span>
            </div>
            <div class="paper-detail-actions">
                <a href="https://arxiv.org/pdf/2602.03485v1" class="btn btn-primary" target="_blank" rel="noopener">
                    <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <path d="M21 15v4a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2v-4"></path>
                        <polyline points="7 10 12 15 17 10"></polyline>
                        <line x1="12" y1="15" x2="12" y2="3"></line>
                    </svg>
                    下载 PDF
                </a>
                <a href="http://arxiv.org/abs/2602.03485v1" class="btn btn-outline" target="_blank" rel="noopener">
                    <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path>
                        <polyline points="15 3 21 3 21 9"></polyline>
                        <line x1="10" y1="14" x2="21" y2="3"></line>
                    </svg>
                    arXiv 页面
                </a>
            </div>
        </header>

        <section class="paper-section">
            <h2>AI 摘要</h2>
            <div class="summary-box">
                <p>论文发现LLM推理中过度自验证现象，提出经验驱动框架抑制无效自验证，减少token使用并保持甚至提升准确率。</p>
            </div>
        </section>

        <section class="paper-section">
            <h2>主要贡献</h2>
            <ul class="contributions-list">
                
                <li>发现LLM推理中过度自验证问题</li>
                
                <li>提出经验驱动的自验证抑制框架</li>
                
                <li>实验证明该方法能减少token使用并维持/提升准确率</li>
                
            </ul>
        </section>

        <section class="paper-section">
            <h2>方法论</h2>
            <p>通过检测LLM的自验证行为，检索历史经验池判断是否需要验证，如果经验表明不必要，则抑制验证。</p>
        </section>

        <section class="paper-section">
            <h2>原文摘要</h2>
            <div class="abstract-box">
                <p>Large Reasoning Models (LRMs) achieve strong performance by generating long reasoning traces with reflection. Through a large-scale empirical analysis, we find that a substantial fraction of reflective steps consist of self-verification (recheck) that repeatedly confirm intermediate results. These rechecks occur frequently across models and benchmarks, yet the vast majority are confirmatory rather than corrective, rarely identifying errors and altering reasoning outcomes. This reveals a mismatch between how often self-verification is activated and how often it is actually useful. Motivated by this, we propose a novel, experience-driven test-time framework that reduces the overused verification. Our method detects the activation of recheck behavior, consults an offline experience pool of past verification outcomes, and estimates whether a recheck is likely unnecessary via efficient retrieval. When historical experience suggests unnecessary, a suppression signal redirects the model to proceed. Across multiple model and benchmarks, our approach reduces token usage up to 20.3% while maintaining the accuracy, and in some datasets even yields accuracy improvements.</p>
            </div>
        </section>

        <section class="paper-section">
            <h2>标签</h2>
            <div class="paper-tags">
                
                <span class="tag">LLM</span>
                
                <span class="tag">Reasoning</span>
                
                <span class="tag">Self-Verification</span>
                
                <span class="tag">Efficiency</span>
                
                <span class="tag">Experience-Driven</span>
                
            </div>
        </section>

        <section class="paper-section">
            <h2>arXiv 分类</h2>
            <div class="paper-tags">
                
                <span class="tag tag-secondary">cs.CL</span>
                
                <span class="tag tag-secondary">cs.AI</span>
                
                <span class="tag tag-secondary">cs.LG</span>
                
            </div>
        </section>

        <footer class="paper-detail-footer">
            <p class="analysis-info">
                分析模型: gemini / gemini-2.0-flash
                · 分析时间: 2026-02-04 20:44
            </p>
            <p class="relevance-reason">
                <strong>相关度评分原因:</strong> 论文直接研究LLM推理中的自验证问题，是LLM推理领域的核心问题。
            </p>
        </footer>
    </article>
</div>

    </main>

    <footer class="footer">
        <div class="footer-container">
            <p>Generated at 2026-02-05 01:08 · Powered by arXiv API</p>
        </div>
    </footer>

    <script src="/Paper_Tracking/static/js/main.js"></script>
    
</body>
</html>