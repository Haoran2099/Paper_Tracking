<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Beyond Tokens: Semantic-Aware Speculative Decoding for Efficient Inference by Probing Internal States - Paper Tracker</title>
    <meta name="description" content="Daily arXiv paper tracking with AI-powered analysis">
    <link rel="stylesheet" href="/Paper_Tracking/static/css/style.css">
    <script>window.SITE_BASE_URL = "/Paper_Tracking";</script>
    
</head>
<body>
    <nav class="navbar">
        <div class="nav-container">
            <a href="/Paper_Tracking/" class="nav-brand">
                <svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                    <path d="M4 19.5A2.5 2.5 0 0 1 6.5 17H20"></path>
                    <path d="M6.5 2H20v20H6.5A2.5 2.5 0 0 1 4 19.5v-15A2.5 2.5 0 0 1 6.5 2z"></path>
                </svg>
                <span>Paper Tracker</span>
            </a>
            <div class="nav-links">
                
                <a href="/Paper_Tracking/category/memory/" class="nav-link ">
                    LLM Memory &amp; RAG
                </a>
                
                <a href="/Paper_Tracking/category/agent/" class="nav-link ">
                    AI Agents
                </a>
                
                <a href="/Paper_Tracking/category/reasoning/" class="nav-link ">
                    LLM Reasoning
                </a>
                
            </div>
            <div class="nav-search">
                <input type="text" id="search-input" placeholder="搜索论文..." autocomplete="off">
                <div id="search-results" class="search-dropdown"></div>
            </div>
        </div>
    </nav>

    <main class="main-content">
        
<div class="container">
    <article class="paper-detail">
        <nav class="breadcrumb">
            <a href="/Paper_Tracking/">首页</a>
            <span>/</span>
            <a href="/Paper_Tracking/category/reasoning/">LLM Reasoning</a>
            <span>/</span>
            <span>2602.03708v1</span>
        </nav>

        <header class="paper-detail-header">
            <div class="paper-detail-meta">
                <span class="paper-category-badge">LLM Reasoning</span>
                <span class="paper-score score-high">
                    相关度: 9/10
                </span>
            </div>
            <h1 class="paper-detail-title">Beyond Tokens: Semantic-Aware Speculative Decoding for Efficient Inference by Probing Internal States</h1>
            <div class="paper-detail-authors">
                
                <span class="author">Ximing Dong</span>, 
                
                <span class="author">Shaowei Wang</span>, 
                
                <span class="author">Dayi Lin</span>, 
                
                <span class="author">Boyuan Chen</span>, 
                
                <span class="author">Ahmed E. Hassan</span>
                
            </div>
            <div class="paper-detail-info">
                <span>arXiv: 2602.03708v1</span>
                <span>发布: 2026-02-03</span>
                <span>更新: 2026-02-03</span>
            </div>
            <div class="paper-detail-actions">
                <a href="https://arxiv.org/pdf/2602.03708v1" class="btn btn-primary" target="_blank" rel="noopener">
                    <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <path d="M21 15v4a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2v-4"></path>
                        <polyline points="7 10 12 15 17 10"></polyline>
                        <line x1="12" y1="15" x2="12" y2="3"></line>
                    </svg>
                    下载 PDF
                </a>
                <a href="http://arxiv.org/abs/2602.03708v1" class="btn btn-outline" target="_blank" rel="noopener">
                    <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path>
                        <polyline points="15 3 21 3 21 9"></polyline>
                        <line x1="10" y1="14" x2="21" y2="3"></line>
                    </svg>
                    arXiv 页面
                </a>
            </div>
        </header>

        <section class="paper-section">
            <h2>AI 摘要</h2>
            <div class="summary-box">
                <p>SemanticSpec通过语义感知的推测解码，提升LLM推理效率，尤其在长链推理中表现突出。</p>
            </div>
        </section>

        <section class="paper-section">
            <h2>主要贡献</h2>
            <ul class="contributions-list">
                
                <li>提出语义感知的推测解码框架SemanticSpec</li>
                
                <li>引入语义概率估计机制，利用内部隐状态评估语义序列的可能性</li>
                
                <li>实验证明在多个基准测试上优于传统方法</li>
                
            </ul>
        </section>

        <section class="paper-section">
            <h2>方法论</h2>
            <p>通过探查模型内部隐状态，评估生成具有特定语义序列的可能性，从而实现更高效的推测解码。</p>
        </section>

        <section class="paper-section">
            <h2>原文摘要</h2>
            <div class="abstract-box">
                <p>Large Language Models (LLMs) achieve strong performance across many tasks but suffer from high inference latency due to autoregressive decoding. The issue is exacerbated in Large Reasoning Models (LRMs), which generate lengthy chains of thought. While speculative decoding accelerates inference by drafting and verifying multiple tokens in parallel, existing methods operate at the token level and ignore semantic equivalence (i.e., different token sequences expressing the same meaning), leading to inefficient rejections. We propose SemanticSpec, a semantic-aware speculative decoding framework that verifies entire semantic sequences instead of tokens. SemanticSpec introduces a semantic probability estimation mechanism that probes the model&#39;s internal hidden states to assess the likelihood of generating sequences with specific meanings.Experiments on four benchmarks show that SemanticSpec achieves up to 2.7x speedup on DeepSeekR1-32B and 2.1x on QwQ-32B, consistently outperforming token-level and sequence-level baselines in both efficiency and effectiveness.</p>
            </div>
        </section>

        <section class="paper-section">
            <h2>标签</h2>
            <div class="paper-tags">
                
                <span class="tag">LLM</span>
                
                <span class="tag">推测解码</span>
                
                <span class="tag">语义理解</span>
                
                <span class="tag">推理效率</span>
                
                <span class="tag">内部状态</span>
                
            </div>
        </section>

        <section class="paper-section">
            <h2>arXiv 分类</h2>
            <div class="paper-tags">
                
                <span class="tag tag-secondary">cs.CL</span>
                
                <span class="tag tag-secondary">cs.PF</span>
                
            </div>
        </section>

        <footer class="paper-detail-footer">
            <p class="analysis-info">
                分析模型: gemini / gemini-2.0-flash
                · 分析时间: 2026-02-04 20:42
            </p>
            <p class="relevance-reason">
                <strong>相关度评分原因:</strong> 论文核心解决LLM推理效率问题，与推理主题高度相关。
            </p>
        </footer>
    </article>
</div>

    </main>

    <footer class="footer">
        <div class="footer-container">
            <p>Generated at 2026-02-05 01:08 · Powered by arXiv API</p>
        </div>
    </footer>

    <script src="/Paper_Tracking/static/js/main.js"></script>
    
</body>
</html>