<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Not All Negative Samples Are Equal: LLMs Learn Better from Plausible Reasoning - Paper Tracker</title>
    <meta name="description" content="Daily arXiv paper tracking with AI-powered analysis">
    <link rel="stylesheet" href="/Paper_Tracking/static/css/style.css">
    <script>window.SITE_BASE_URL = "/Paper_Tracking";</script>
    
</head>
<body>
    <nav class="navbar">
        <div class="nav-container">
            <a href="/Paper_Tracking/" class="nav-brand">
                <svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                    <path d="M4 19.5A2.5 2.5 0 0 1 6.5 17H20"></path>
                    <path d="M6.5 2H20v20H6.5A2.5 2.5 0 0 1 4 19.5v-15A2.5 2.5 0 0 1 6.5 2z"></path>
                </svg>
                <span>Paper Tracker</span>
            </a>
            <div class="nav-links">
                
                <a href="/Paper_Tracking/category/memory/" class="nav-link ">
                    LLM Memory &amp; RAG
                </a>
                
                <a href="/Paper_Tracking/category/agent/" class="nav-link ">
                    AI Agents
                </a>
                
                <a href="/Paper_Tracking/category/reasoning/" class="nav-link ">
                    LLM Reasoning
                </a>
                
            </div>
            <div class="nav-search">
                <input type="text" id="search-input" placeholder="搜索论文..." autocomplete="off">
                <div id="search-results" class="search-dropdown"></div>
            </div>
        </div>
    </nav>

    <main class="main-content">
        
<div class="container">
    <article class="paper-detail">
        <nav class="breadcrumb">
            <a href="/Paper_Tracking/">首页</a>
            <span>/</span>
            <a href="/Paper_Tracking/category/reasoning/">LLM Reasoning</a>
            <span>/</span>
            <span>2602.03516v1</span>
        </nav>

        <header class="paper-detail-header">
            <div class="paper-detail-meta">
                <span class="paper-category-badge">LLM Reasoning</span>
                <span class="paper-score score-high">
                    相关度: 9/10
                </span>
            </div>
            <h1 class="paper-detail-title">Not All Negative Samples Are Equal: LLMs Learn Better from Plausible Reasoning</h1>
            <div class="paper-detail-authors">
                
                <span class="author">Zixiang Di</span>, 
                
                <span class="author">Jinyi Han</span>, 
                
                <span class="author">Shuo Zhang</span>, 
                
                <span class="author">Ying Liao</span>, 
                
                <span class="author">Zhi Li</span>, 
                
                <span class="author">Xiaofeng Ji</span>, 
                
                <span class="author">Yongqi Wang</span>, 
                
                <span class="author">Zheming Yang</span>, 
                
                <span class="author">Ming Gao</span>, 
                
                <span class="author">Bingdong Li</span>, 
                
                <span class="author">Jie Wang</span>
                
            </div>
            <div class="paper-detail-info">
                <span>arXiv: 2602.03516v1</span>
                <span>发布: 2026-02-03</span>
                <span>更新: 2026-02-03</span>
            </div>
            <div class="paper-detail-actions">
                <a href="https://arxiv.org/pdf/2602.03516v1" class="btn btn-primary" target="_blank" rel="noopener">
                    <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <path d="M21 15v4a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2v-4"></path>
                        <polyline points="7 10 12 15 17 10"></polyline>
                        <line x1="12" y1="15" x2="12" y2="3"></line>
                    </svg>
                    下载 PDF
                </a>
                <a href="http://arxiv.org/abs/2602.03516v1" class="btn btn-outline" target="_blank" rel="noopener">
                    <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path>
                        <polyline points="15 3 21 3 21 9"></polyline>
                        <line x1="10" y1="14" x2="21" y2="3"></line>
                    </svg>
                    arXiv 页面
                </a>
            </div>
        </header>

        <section class="paper-section">
            <h2>AI 摘要</h2>
            <div class="summary-box">
                <p>提出PNS方法，通过合成高质量负样本来提升LLM的推理能力。</p>
            </div>
        </section>

        <section class="paper-section">
            <h2>主要贡献</h2>
            <ul class="contributions-list">
                
                <li>提出了Plausible Negative Samples（PNS）方法</li>
                
                <li>使用逆向强化学习生成高质量负样本</li>
                
                <li>验证了PNS作为偏好优化的数据源的有效性</li>
                
            </ul>
        </section>

        <section class="paper-section">
            <h2>方法论</h2>
            <p>使用逆向强化学习训练模型，生成格式正确但答案错误的负样本，用于训练LLM。</p>
        </section>

        <section class="paper-section">
            <h2>原文摘要</h2>
            <div class="abstract-box">
                <p>Learning from negative samples holds great promise for improving Large Language Model (LLM) reasoning capability, yet existing methods treat all incorrect responses as equally informative, overlooking the crucial role of sample quality. To address this, we propose Plausible Negative Samples (PNS), a method that synthesizes high-quality negative samples exhibiting expected format and structural coherence while ultimately yielding incorrect answers. PNS trains a dedicated model via reverse reinforcement learning (RL) guided by a composite reward combining format compliance, accuracy inversion, reward model assessment, and chain-of-thought evaluation, generating responses nearly indistinguishable from correct solutions. We further validate PNS as a plug-and-play data source for preference optimization across three backbone models on seven mathematical reasoning benchmarks. Results demonstrate that PNS consistently outperforms other negative sample synthesis methods, achieving an average improvement of 2.03% over RL-trained models.</p>
            </div>
        </section>

        <section class="paper-section">
            <h2>标签</h2>
            <div class="paper-tags">
                
                <span class="tag">LLM</span>
                
                <span class="tag">推理</span>
                
                <span class="tag">负样本</span>
                
                <span class="tag">强化学习</span>
                
            </div>
        </section>

        <section class="paper-section">
            <h2>arXiv 分类</h2>
            <div class="paper-tags">
                
                <span class="tag tag-secondary">cs.LG</span>
                
                <span class="tag tag-secondary">cs.AI</span>
                
            </div>
        </section>

        <footer class="paper-detail-footer">
            <p class="analysis-info">
                分析模型: gemini / gemini-2.0-flash
                · 分析时间: 2026-02-04 20:43
            </p>
            <p class="relevance-reason">
                <strong>相关度评分原因:</strong> 论文直接针对LLM的推理能力，并提出了新的负样本生成方法。
            </p>
        </footer>
    </article>
</div>

    </main>

    <footer class="footer">
        <div class="footer-container">
            <p>Generated at 2026-02-05 01:08 · Powered by arXiv API</p>
        </div>
    </footer>

    <script src="/Paper_Tracking/static/js/main.js"></script>
    
</body>
</html>